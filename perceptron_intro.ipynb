{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Введение в перцептрон и нейронные сети\n",
    "\n",
    "В этой тетрадке мы познакомимся с основами перцептрона и нейронных сетей, реализуем простой перцептрон и обучим его на классических задачах."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Что такое перцептрон?\n",
    "\n",
    "**Перцептрон** — это простейшая модель искусственного нейрона, предложенная Фрэнком Розенблаттом в 1957 году.\n",
    "\n",
    "### Математическая модель:\n",
    "\n",
    "Перцептрон вычисляет взвешенную сумму входов и применяет к ней функцию активации:\n",
    "\n",
    "$$y = f\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)$$\n",
    "\n",
    "где:\n",
    "- $x_i$ — входные признаки\n",
    "- $w_i$ — веса\n",
    "- $b$ — смещение (bias)\n",
    "- $f$ — функция активации (обычно ступенчатая)\n",
    "\n",
    "### Функция активации:\n",
    "\n",
    "Классическая ступенчатая функция:\n",
    "$$f(z) = \\begin{cases} 1, & \\text{если } z \\geq 0 \\\\ 0, & \\text{если } z < 0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Устанавливаем seed для воспроизводимости\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Реализация перцептрона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"Простая реализация перцептрона\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.errors_ = []\n",
    "    \n",
    "    def activation(self, z):\n",
    "        \"\"\"Ступенчатая функция активации\"\"\"\n",
    "        return np.where(z >= 0, 1, 0)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Обучение перцептрона\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Инициализация весов и смещения\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Обучение\n",
    "        for _ in range(self.n_iterations):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                # Предсказание\n",
    "                linear_output = np.dot(xi, self.weights) + self.bias\n",
    "                y_pred = self.activation(linear_output)\n",
    "                \n",
    "                # Обновление весов\n",
    "                update = self.learning_rate * (target - y_pred)\n",
    "                self.weights += update * xi\n",
    "                self.bias += update\n",
    "                \n",
    "                # Подсчет ошибок\n",
    "                errors += int(update != 0.0)\n",
    "            \n",
    "            self.errors_.append(errors)\n",
    "            \n",
    "            # Остановка, если нет ошибок\n",
    "            if errors == 0:\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Предсказание классов\"\"\"\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return self.activation(linear_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Обучение на логических функциях\n",
    "\n",
    "Проверим работу перцептрона на простых логических функциях."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### 3.1. Функция AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные для AND\n",
    "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_and = np.array([0, 0, 0, 1])\n",
    "\n",
    "# Обучение\n",
    "perceptron_and = Perceptron(learning_rate=0.1, n_iterations=10)\n",
    "perceptron_and.fit(X_and, y_and)\n",
    "\n",
    "# Предсказания\n",
    "predictions = perceptron_and.predict(X_and)\n",
    "\n",
    "print(\"Функция AND:\")\n",
    "print(f\"Входы: {X_and}\")\n",
    "print(f\"Ожидаемые выходы: {y_and}\")\n",
    "print(f\"Предсказания: {predictions}\")\n",
    "print(f\"Точность: {np.mean(predictions == y_and) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### 3.2. Функция OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные для OR\n",
    "X_or = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_or = np.array([0, 1, 1, 1])\n",
    "\n",
    "# Обучение\n",
    "perceptron_or = Perceptron(learning_rate=0.1, n_iterations=10)\n",
    "perceptron_or.fit(X_or, y_or)\n",
    "\n",
    "# Предсказания\n",
    "predictions = perceptron_or.predict(X_or)\n",
    "\n",
    "print(\"Функция OR:\")\n",
    "print(f\"Входы: {X_or}\")\n",
    "print(f\"Ожидаемые выходы: {y_or}\")\n",
    "print(f\"Предсказания: {predictions}\")\n",
    "print(f\"Точность: {np.mean(predictions == y_or) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### 3.3. Функция XOR (проблема линейной неразделимости)\n",
    "\n",
    "Классическая проблема перцептрона — невозможность решить задачу XOR, так как данные линейно неразделимы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные для XOR\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Обучение\n",
    "perceptron_xor = Perceptron(learning_rate=0.1, n_iterations=100)\n",
    "perceptron_xor.fit(X_xor, y_xor)\n",
    "\n",
    "# Предсказания\n",
    "predictions = perceptron_xor.predict(X_xor)\n",
    "\n",
    "print(\"Функция XOR:\")\n",
    "print(f\"Входы: {X_xor}\")\n",
    "print(f\"Ожидаемые выходы: {y_xor}\")\n",
    "print(f\"Предсказания: {predictions}\")\n",
    "print(f\"Точность: {np.mean(predictions == y_xor) * 100:.2f}%\")\n",
    "print(\"\\n⚠️ Перцептрон не может решить XOR! Нужна многослойная сеть.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 4. Визуализация разделяющей прямой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(perceptron, X, y, title):\n",
    "    \"\"\"Визуализация разделяющей прямой\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Точки данных\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='red', marker='o', s=100, label='Класс 0')\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', marker='s', s=100, label='Класс 1')\n",
    "    \n",
    "    # Разделяющая прямая\n",
    "    x_min, x_max = -0.5, 1.5\n",
    "    if perceptron.weights[1] != 0:\n",
    "        x_line = np.array([x_min, x_max])\n",
    "        y_line = -(perceptron.weights[0] * x_line + perceptron.bias) / perceptron.weights[1]\n",
    "        plt.plot(x_line, y_line, 'g--', linewidth=2, label='Разделяющая прямая')\n",
    "    \n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(-0.5, 1.5)\n",
    "    plt.xlabel('x₁')\n",
    "    plt.ylabel('x₂')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Визуализация для AND, OR, XOR\n",
    "plot_decision_boundary(perceptron_and, X_and, y_and, 'Перцептрон: AND')\n",
    "plot_decision_boundary(perceptron_or, X_or, y_or, 'Перцептрон: OR')\n",
    "plot_decision_boundary(perceptron_xor, X_xor, y_xor, 'Перцептрон: XOR (неудача)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 5. Многослойный перцептрон (MLP)\n",
    "\n",
    "Для решения линейно неразделимых задач (как XOR) нужна многослойная сеть с нелинейными функциями активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"Простой многослойный перцептрон с одним скрытым слоем\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        # Инициализация весов\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.5\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.5\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Сигмоидальная функция активации\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        \"\"\"Производная сигмоиды\"\"\"\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Прямое распространение\"\"\"\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        \"\"\"Обратное распространение ошибки\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Ошибка выходного слоя\n",
    "        dz2 = output - y.reshape(-1, 1)\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Ошибка скрытого слоя\n",
    "        dz1 = np.dot(dz2, self.W2.T) * self.sigmoid_derivative(self.z1)\n",
    "        dW1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Обновление весов\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "    \n",
    "    def train(self, X, y, epochs=10000):\n",
    "        \"\"\"Обучение сети\"\"\"\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            # Прямое распространение\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # Вычисление ошибки\n",
    "            loss = np.mean((output - y.reshape(-1, 1)) ** 2)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Обратное распространение\n",
    "            self.backward(X, y, output)\n",
    "            \n",
    "            if epoch % 1000 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss:.6f}')\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Предсказание классов\"\"\"\n",
    "        output = self.forward(X)\n",
    "        return (output > 0.5).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 6. Решение XOR с помощью MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание и обучение MLP для XOR\n",
    "mlp = MLP(input_size=2, hidden_size=4, output_size=1, learning_rate=0.5)\n",
    "losses = mlp.train(X_xor, y_xor, epochs=5000)\n",
    "\n",
    "# Предсказания\n",
    "predictions = mlp.predict(X_xor)\n",
    "\n",
    "print(\"\\nРезультаты MLP на XOR:\")\n",
    "print(f\"Входы: {X_xor}\")\n",
    "print(f\"Ожидаемые выходы: {y_xor}\")\n",
    "print(f\"Предсказания: {predictions}\")\n",
    "print(f\"Точность: {np.mean(predictions == y_xor) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 7. Визуализация обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# График ошибки обучения\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Эпоха')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Кривая обучения MLP на задаче XOR')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 8. Применение на реальных данных\n",
    "\n",
    "Протестируем MLP на более сложных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация линейно разделимых данных\n",
    "X_linear, y_linear = make_classification(\n",
    "    n_samples=200,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    flip_y=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Визуализация\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_linear[y_linear == 0, 0], X_linear[y_linear == 0, 1], \n",
    "            c='red', marker='o', alpha=0.6, label='Класс 0')\n",
    "plt.scatter(X_linear[y_linear == 1, 0], X_linear[y_linear == 1, 1], \n",
    "            c='blue', marker='s', alpha=0.6, label='Класс 1')\n",
    "plt.xlabel('Признак 1')\n",
    "plt.ylabel('Признак 2')\n",
    "plt.title('Линейно разделимые данные')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Генерация нелинейно разделимых данных (окружности)\n",
    "X_circles, y_circles = make_circles(n_samples=200, noise=0.1, factor=0.5, random_state=42)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_circles[y_circles == 0, 0], X_circles[y_circles == 0, 1], \n",
    "            c='red', marker='o', alpha=0.6, label='Класс 0')\n",
    "plt.scatter(X_circles[y_circles == 1, 0], X_circles[y_circles == 1, 1], \n",
    "            c='blue', marker='s', alpha=0.6, label='Класс 1')\n",
    "plt.xlabel('Признак 1')\n",
    "plt.ylabel('Признак 2')\n",
    "plt.title('Нелинейно разделимые данные (окружности)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение на линейно разделимых данных\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_linear, y_linear, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "mlp_linear = MLP(input_size=2, hidden_size=5, output_size=1, learning_rate=0.1)\n",
    "losses_linear = mlp_linear.train(X_train, y_train, epochs=1000)\n",
    "\n",
    "# Оценка\n",
    "train_pred = mlp_linear.predict(X_train)\n",
    "test_pred = mlp_linear.predict(X_test)\n",
    "\n",
    "print(f\"\\nЛинейно разделимые данные:\")\n",
    "print(f\"Точность на обучающей выборке: {np.mean(train_pred == y_train) * 100:.2f}%\")\n",
    "print(f\"Точность на тестовой выборке: {np.mean(test_pred == y_test) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение на нелинейно разделимых данных\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_circles, y_circles, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "mlp_circles = MLP(input_size=2, hidden_size=10, output_size=1, learning_rate=0.5)\n",
    "losses_circles = mlp_circles.train(X_train, y_train, epochs=5000)\n",
    "\n",
    "# Оценка\n",
    "train_pred = mlp_circles.predict(X_train)\n",
    "test_pred = mlp_circles.predict(X_test)\n",
    "\n",
    "print(f\"\\nНелинейно разделимые данные (окружности):\")\n",
    "print(f\"Точность на обучающей выборке: {np.mean(train_pred == y_train) * 100:.2f}%\")\n",
    "print(f\"Точность на тестовой выборке: {np.mean(test_pred == y_test) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 9. Ключевые выводы\n",
    "\n",
    "### Перцептрон:\n",
    "- ✅ Прост в реализации и обучении\n",
    "- ✅ Гарантированно сходится для линейно разделимых данных\n",
    "- ❌ Не может решать линейно неразделимые задачи (XOR)\n",
    "- ❌ Ограничен бинарной классификацией\n",
    "\n",
    "### Многослойный перцептрон (MLP):\n",
    "- ✅ Может решать нелинейные задачи\n",
    "- ✅ Универсальный аппроксиматор (может приблизить любую функцию)\n",
    "- ✅ Подходит для многоклассовой классификации\n",
    "- ⚠️ Требует больше вычислительных ресурсов\n",
    "- ⚠️ Может переобучаться\n",
    "- ⚠️ Требует настройки гиперпараметров\n",
    "\n",
    "### Основные концепции:\n",
    "1. **Веса и смещения** — обучаемые параметры модели\n",
    "2. **Функция активации** — добавляет нелинейность\n",
    "3. **Прямое распространение** — вычисление выхода сети\n",
    "4. **Обратное распространение** — обновление весов на основе ошибки\n",
    "5. **Градиентный спуск** — алгоритм оптимизации\n",
    "\n",
    "### Рекомендации:\n",
    "- Начинайте с простых моделей (перцептрон)\n",
    "- Переходите к MLP при линейной неразделимости\n",
    "- Нормализуйте входные данные\n",
    "- Используйте валидацию для предотвращения переобучения\n",
    "- Экспериментируйте с архитектурой и гиперпараметрами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 10. Задания для самостоятельной работы\n",
    "\n",
    "1. Реализуйте функцию активации ReLU и сравните результаты с сигмоидой\n",
    "2. Добавьте визуализацию границы решений для MLP\n",
    "3. Реализуйте регуляризацию (L2) для предотвращения переобучения\n",
    "4. Создайте MLP с несколькими скрытыми слоями\n",
    "5. Реализуйте метод Adam для оптимизации вместо простого градиентного спуска\n",
    "6. Протестируйте на датасете Iris или Wine\n",
    "7. Сравните свою реализацию с MLPClassifier из sklearn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
