{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM): Теория и практика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теория\n",
    "\n",
    "### Что такое SVM?\n",
    "\n",
    "Support Vector Machine (Метод опорных векторов) — это мощный алгоритм машинного обучения для задач классификации и регрессии. Основная идея SVM — найти оптимальную гиперплоскость, которая максимально разделяет классы.\n",
    "\n",
    "### Линейный SVM\n",
    "\n",
    "Для линейно разделимых данных SVM ищет гиперплоскость:\n",
    "$$\n",
    "w^T x + b = 0\n",
    "$$\n",
    "\n",
    "Где:\n",
    "- $w$ — вектор весов (перпендикулярен гиперплоскости)\n",
    "- $b$ — смещение (bias)\n",
    "- $x$ — вектор признаков\n",
    "\n",
    "### Margin (Зазор)\n",
    "\n",
    "Расстояние от гиперплоскости до ближайших точек каждого класса называется **margin**. SVM максимизирует этот зазор.\n",
    "\n",
    "Margin вычисляется как:\n",
    "$$\n",
    "\\text{margin} = \\frac{2}{\\|w\\|}\n",
    "$$\n",
    "\n",
    "### Целевая функция\n",
    "\n",
    "SVM решает задачу оптимизации:\n",
    "$$\n",
    "\\min_{w,b} \\frac{1}{2}\\|w\\|^2\n",
    "$$\n",
    "\n",
    "При ограничениях:\n",
    "$$\n",
    "y_i(w^T x_i + b) \\geq 1, \\quad \\forall i\n",
    "$$\n",
    "\n",
    "Где $y_i \\in \\{-1, +1\\}$ — метки классов.\n",
    "\n",
    "### Soft Margin SVM\n",
    "\n",
    "Для данных, которые не являются линейно разделимыми, используется **soft margin** с введением переменных ослабления $\\xi_i$:\n",
    "\n",
    "$$\n",
    "\\min_{w,b,\\xi} \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^{n}\\xi_i\n",
    "$$\n",
    "\n",
    "При ограничениях:\n",
    "$$\n",
    "y_i(w^T x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n",
    "$$\n",
    "\n",
    "Где:\n",
    "- $C$ — параметр регуляризации (баланс между максимизацией margin и минимизацией ошибок)\n",
    "- $\\xi_i$ — величина нарушения для точки $i$\n",
    "\n",
    "### Kernel Trick (Ядро)\n",
    "\n",
    "Для нелинейно разделимых данных SVM использует **ядра** (kernels), которые отображают данные в пространство более высокой размерности:\n",
    "\n",
    "1. **Линейное ядро**: $K(x_i, x_j) = x_i^T x_j$\n",
    "2. **Полиномиальное ядро**: $K(x_i, x_j) = (x_i^T x_j + c)^d$\n",
    "3. **RBF (Radial Basis Function)**: $K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2)$\n",
    "4. **Sigmoid**: $K(x_i, x_j) = \\tanh(\\alpha x_i^T x_j + c)$\n",
    "\n",
    "### Опорные векторы\n",
    "\n",
    "**Опорные векторы** (support vectors) — это точки обучающей выборки, которые лежат на границах margin или нарушают его. Только эти точки влияют на положение разделяющей гиперплоскости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика: Линейный SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Генерация линейно разделимых данных\n",
    "np.random.seed(42)\n",
    "X, y = datasets.make_classification(n_samples=200, n_features=2, n_redundant=0, \n",
    "                                      n_informative=2, n_clusters_per_class=1, \n",
    "                                      class_sep=2, random_state=42)\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Стандартизация данных\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Обучение линейного SVM\n",
    "svm_linear = SVC(kernel='linear', C=1.0)\n",
    "svm_linear.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Предсказания\n",
    "y_pred = svm_linear.predict(X_test_scaled)\n",
    "\n",
    "# Метрики\n",
    "print(\"Линейный SVM\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация разделяющей гиперплоскости и опорных векторов\n",
    "def plot_svm_decision_boundary(X, y, model, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Создание сетки для визуализации\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Предсказания на сетке\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Отображение границы решения\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolor='k', s=50)\n",
    "    \n",
    "    # Опорные векторы\n",
    "    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "                s=200, linewidth=2, facecolors='none', edgecolors='green', label='Support Vectors')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_svm_decision_boundary(X_train_scaled, y_train, svm_linear, 'Линейный SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика: SVM с RBF ядром"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация нелинейно разделимых данных\n",
    "X_nonlinear, y_nonlinear = datasets.make_moons(n_samples=200, noise=0.15, random_state=42)\n",
    "\n",
    "# Разделение данных\n",
    "X_train_nl, X_test_nl, y_train_nl, y_test_nl = train_test_split(\n",
    "    X_nonlinear, y_nonlinear, test_size=0.3, random_state=42)\n",
    "\n",
    "# Стандартизация\n",
    "X_train_nl_scaled = scaler.fit_transform(X_train_nl)\n",
    "X_test_nl_scaled = scaler.transform(X_test_nl)\n",
    "\n",
    "# Обучение SVM с RBF ядром\n",
    "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm_rbf.fit(X_train_nl_scaled, y_train_nl)\n",
    "\n",
    "# Предсказания\n",
    "y_pred_nl = svm_rbf.predict(X_test_nl_scaled)\n",
    "\n",
    "# Метрики\n",
    "print(\"SVM с RBF ядром\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_nl, y_pred_nl))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_nl, y_pred_nl))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_nl, y_pred_nl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svm_decision_boundary(X_train_nl_scaled, y_train_nl, svm_rbf, 'SVM с RBF ядром (нелинейные данные)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение различных ядер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравнение разных ядер на нелинейных данных\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, kernel in enumerate(kernels):\n",
    "    # Обучение модели\n",
    "    if kernel == 'poly':\n",
    "        model = SVC(kernel=kernel, degree=3, C=1.0)\n",
    "    else:\n",
    "        model = SVC(kernel=kernel, C=1.0)\n",
    "    \n",
    "    model.fit(X_train_nl_scaled, y_train_nl)\n",
    "    \n",
    "    # Создание сетки\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_train_nl_scaled[:, 0].min() - 1, X_train_nl_scaled[:, 0].max() + 1\n",
    "    y_min, y_max = X_train_nl_scaled[:, 1].min() - 1, X_train_nl_scaled[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Визуализация\n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    axes[idx].scatter(X_train_nl_scaled[:, 0], X_train_nl_scaled[:, 1], \n",
    "                      c=y_train_nl, cmap='coolwarm', edgecolor='k', s=50)\n",
    "    axes[idx].scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "                      s=200, linewidth=2, facecolors='none', edgecolors='green')\n",
    "    \n",
    "    # Метрики\n",
    "    y_pred_kernel = model.predict(X_test_nl_scaled)\n",
    "    acc = accuracy_score(y_test_nl, y_pred_kernel)\n",
    "    \n",
    "    axes[idx].set_title(f'{kernel.upper()} kernel (Accuracy: {acc:.3f})')\n",
    "    axes[idx].set_xlabel('Feature 1')\n",
    "    axes[idx].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Влияние параметра C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравнение разных значений параметра C\n",
    "C_values = [0.1, 1, 10, 100]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, C in enumerate(C_values):\n",
    "    model = SVC(kernel='rbf', C=C, gamma='scale')\n",
    "    model.fit(X_train_nl_scaled, y_train_nl)\n",
    "    \n",
    "    # Создание сетки\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_train_nl_scaled[:, 0].min() - 1, X_train_nl_scaled[:, 0].max() + 1\n",
    "    y_min, y_max = X_train_nl_scaled[:, 1].min() - 1, X_train_nl_scaled[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Визуализация\n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    axes[idx].scatter(X_train_nl_scaled[:, 0], X_train_nl_scaled[:, 1], \n",
    "                      c=y_train_nl, cmap='coolwarm', edgecolor='k', s=50)\n",
    "    axes[idx].scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "                      s=200, linewidth=2, facecolors='none', edgecolors='green')\n",
    "    \n",
    "    y_pred_c = model.predict(X_test_nl_scaled)\n",
    "    acc = accuracy_score(y_test_nl, y_pred_c)\n",
    "    n_support = len(model.support_vectors_)\n",
    "    \n",
    "    axes[idx].set_title(f'C = {C} (Acc: {acc:.3f}, Support Vectors: {n_support})')\n",
    "    axes[idx].set_xlabel('Feature 1')\n",
    "    axes[idx].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nВлияние параметра C:\")\n",
    "print(\"- Малое C: больше margin, больше опорных векторов, больше ошибок (underfitting)\")\n",
    "print(\"- Большое C: меньше margin, меньше опорных векторов, меньше ошибок (overfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Влияние параметра gamma (для RBF ядра)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравнение разных значений gamma\n",
    "gamma_values = [0.01, 0.1, 1, 10]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, gamma in enumerate(gamma_values):\n",
    "    model = SVC(kernel='rbf', C=1.0, gamma=gamma)\n",
    "    model.fit(X_train_nl_scaled, y_train_nl)\n",
    "    \n",
    "    # Создание сетки\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_train_nl_scaled[:, 0].min() - 1, X_train_nl_scaled[:, 0].max() + 1\n",
    "    y_min, y_max = X_train_nl_scaled[:, 1].min() - 1, X_train_nl_scaled[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Визуализация\n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    axes[idx].scatter(X_train_nl_scaled[:, 0], X_train_nl_scaled[:, 1], \n",
    "                      c=y_train_nl, cmap='coolwarm', edgecolor='k', s=50)\n",
    "    axes[idx].scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "                      s=200, linewidth=2, facecolors='none', edgecolors='green')\n",
    "    \n",
    "    y_pred_gamma = model.predict(X_test_nl_scaled)\n",
    "    acc = accuracy_score(y_test_nl, y_pred_gamma)\n",
    "    \n",
    "    axes[idx].set_title(f'gamma = {gamma} (Accuracy: {acc:.3f})')\n",
    "    axes[idx].set_xlabel('Feature 1')\n",
    "    axes[idx].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nВлияние параметра gamma:\")\n",
    "print(\"- Малое gamma: широкая область влияния, более гладкая граница (underfitting)\")\n",
    "print(\"- Большое gamma: узкая область влияния, сложная граница (overfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическое применение: классификация на реальных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка датасета Iris\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "# Используем только два класса для бинарной классификации\n",
    "X_iris_binary = X_iris[y_iris != 2]\n",
    "y_iris_binary = y_iris[y_iris != 2]\n",
    "\n",
    "# Разделение данных\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris_binary, y_iris_binary, test_size=0.3, random_state=42)\n",
    "\n",
    "# Стандартизация\n",
    "scaler_iris = StandardScaler()\n",
    "X_train_iris_scaled = scaler_iris.fit_transform(X_train_iris)\n",
    "X_test_iris_scaled = scaler_iris.transform(X_test_iris)\n",
    "\n",
    "# Обучение SVM\n",
    "svm_iris = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm_iris.fit(X_train_iris_scaled, y_train_iris)\n",
    "\n",
    "# Предсказания\n",
    "y_pred_iris = svm_iris.predict(X_test_iris_scaled)\n",
    "\n",
    "# Метрики\n",
    "print(\"SVM на датасете Iris (бинарная классификация)\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_iris, y_pred_iris))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_iris, y_pred_iris, target_names=iris.target_names[:2]))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_iris, y_pred_iris))\n",
    "print(\"\\nКоличество опорных векторов:\", len(svm_iris.support_vectors_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Резюме\n",
    "\n",
    "### Преимущества SVM:\n",
    "1. Эффективен в пространствах высокой размерности\n",
    "2. Работает хорошо даже когда признаков больше, чем объектов\n",
    "3. Использует только опорные векторы (экономия памяти)\n",
    "4. Универсален благодаря различным ядрам\n",
    "\n",
    "### Недостатки SVM:\n",
    "1. Медленное обучение на больших датасетах\n",
    "2. Чувствителен к выбору ядра и параметров\n",
    "3. Не предоставляет напрямую оценки вероятности\n",
    "4. Сложно интерпретировать модель\n",
    "\n",
    "### Ключевые параметры:\n",
    "- **C**: контролирует trade-off между максимизацией margin и минимизацией ошибок\n",
    "- **gamma** (для RBF): определяет, насколько далеко влияет один обучающий пример\n",
    "- **kernel**: тип ядра (linear, poly, rbf, sigmoid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
