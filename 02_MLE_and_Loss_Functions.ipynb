{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE и функции потерь в регрессии\n",
    "\n",
    "## Введение\n",
    "\n",
    "Maximum Likelihood Estimation (MLE) является теоретической основой для многих функций потерь в машинном обучении. В этом ноутбуке мы покажем, как **минимизация функций потерь эквивалентна максимизации правдоподобия** при определенных предположениях о распределении данных.\n",
    "\n",
    "### Ключевая идея:\n",
    "\n",
    "**Минимизация loss функции = Максимизация likelihood функции**\n",
    "\n",
    "Разные предположения о распределении ошибок/целевой переменной приводят к разным loss функциям."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T08:02:12.577207Z",
     "start_time": "2026-01-24T08:02:11.720013Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "\n",
    "np.random.seed(42)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Линейная регрессия: от MLE к MSE\n",
    "\n",
    "### Теория\n",
    "\n",
    "**Предположение**: Ошибки распределены нормально\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "Это означает:\n",
    "$$y_i | x_i \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_i, \\sigma^2)$$\n",
    "\n",
    "**Функция правдоподобия** (вероятность наблюдать данные при заданных параметрах):\n",
    "\n",
    "$$L(\\beta_0, \\beta_1, \\sigma^2 | X, y) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - (\\beta_0 + \\beta_1 x_i))^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "**Логарифм правдоподобия** (проще для оптимизации):\n",
    "\n",
    "$$\\log L = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i - (\\beta_0 + \\beta_1 x_i))^2$$\n",
    "\n",
    "**Максимизация** $\\log L$ по $\\beta_0, \\beta_1$ **эквивалентна минимизации**:\n",
    "\n",
    "$$\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 \\quad \\Rightarrow \\quad \\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "### Вывод: MSE loss = MLE при нормально распределенных ошибках!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практический пример: Предсказание концентрации белка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем данные: зависимость концентрации белка от времени\n",
    "n = 100\n",
    "X = np.random.uniform(0, 10, n)  # время (часы)\n",
    "true_beta0 = 2.0   # начальная концентрация\n",
    "true_beta1 = 3.0   # скорость роста\n",
    "true_sigma = 2.0   # шум измерений\n",
    "\n",
    "# y = истинная зависимость + гауссовский шум\n",
    "y = true_beta0 + true_beta1 * X + np.random.normal(0, true_sigma, n)\n",
    "\n",
    "print(f\"Истинные параметры: β₀={true_beta0}, β₁={true_beta1}, σ={true_sigma}\")\n",
    "print(f\"Размер выборки: n={n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод 1: Максимизация Likelihood (MLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood_linear(params, X, y):\n",
    "    \"\"\"\n",
    "    Отрицательный логарифм правдоподобия для линейной регрессии\n",
    "    с предположением о нормально распределенных ошибках\n",
    "    \"\"\"\n",
    "    beta0, beta1, sigma = params\n",
    "    \n",
    "    if sigma <= 0:\n",
    "        return np.inf\n",
    "    \n",
    "    # Предсказания модели\n",
    "    predictions = beta0 + beta1 * X\n",
    "    residuals = y - predictions\n",
    "    \n",
    "    # Negative log-likelihood\n",
    "    n = len(y)\n",
    "    nll = n/2 * np.log(2 * np.pi * sigma**2) + np.sum(residuals**2) / (2 * sigma**2)\n",
    "    \n",
    "    return nll\n",
    "\n",
    "# Оптимизация: находим параметры, максимизирующие likelihood\n",
    "initial_params = [0, 0, 1]\n",
    "result_mle = minimize(\n",
    "    negative_log_likelihood_linear, \n",
    "    initial_params, \n",
    "    args=(X, y), \n",
    "    method='L-BFGS-B',\n",
    "    bounds=[(-np.inf, np.inf), (-np.inf, np.inf), (0.01, np.inf)]\n",
    ")\n",
    "\n",
    "beta0_mle, beta1_mle, sigma_mle = result_mle.x\n",
    "\n",
    "print(\"\\n=== MLE подход ===\")\n",
    "print(f\"MLE оценки: β₀={beta0_mle:.3f}, β₁={beta1_mle:.3f}, σ={sigma_mle:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод 2: Минимизация MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(params, X, y):\n",
    "    \"\"\"\n",
    "    Mean Squared Error loss функция\n",
    "    \"\"\"\n",
    "    beta0, beta1 = params\n",
    "    predictions = beta0 + beta1 * X\n",
    "    mse = np.mean((y - predictions)**2)\n",
    "    return mse\n",
    "\n",
    "# Оптимизация: находим параметры, минимизирующие MSE\n",
    "initial_params_mse = [0, 0]\n",
    "result_mse = minimize(\n",
    "    mse_loss, \n",
    "    initial_params_mse, \n",
    "    args=(X, y), \n",
    "    method='L-BFGS-B'\n",
    ")\n",
    "\n",
    "beta0_mse, beta1_mse = result_mse.x\n",
    "\n",
    "print(\"\\n=== MSE подход ===\")\n",
    "print(f\"MSE оценки: β₀={beta0_mse:.3f}, β₁={beta1_mse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод 3: sklearn LinearRegression (также минимизирует MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn использует аналитическое решение (метод наименьших квадратов)\n",
    "model_sklearn = LinearRegression()\n",
    "model_sklearn.fit(X.reshape(-1, 1), y)\n",
    "\n",
    "beta0_sklearn = model_sklearn.intercept_\n",
    "beta1_sklearn = model_sklearn.coef_[0]\n",
    "\n",
    "print(\"\\n=== sklearn LinearRegression ===\")\n",
    "print(f\"sklearn оценки: β₀={beta0_sklearn:.3f}, β₁={beta1_sklearn:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"СРАВНЕНИЕ МЕТОДОВ\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Истинные параметры:    β₀={true_beta0:.3f}, β₁={true_beta1:.3f}\")\n",
    "print(f\"MLE (max likelihood):  β₀={beta0_mle:.3f}, β₁={beta1_mle:.3f}\")\n",
    "print(f\"MSE (min loss):        β₀={beta0_mse:.3f}, β₁={beta1_mse:.3f}\")\n",
    "print(f\"sklearn:               β₀={beta0_sklearn:.3f}, β₁={beta1_sklearn:.3f}\")\n",
    "print(\"\\n✓ Все три метода дают практически одинаковые результаты!\")\n",
    "print(\"  Это подтверждает: минимизация MSE = максимизация likelihood\")\n",
    "\n",
    "# Вычисляем MSE для MLE решения\n",
    "y_pred_mle = beta0_mle + beta1_mle * X\n",
    "mse_value = np.mean((y - y_pred_mle)**2)\n",
    "print(f\"\\nMSE для MLE решения: {mse_value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# График 1: Данные и предсказания\n",
    "axes[0].scatter(X, y, alpha=0.5, s=30, label='Данные')\n",
    "X_line = np.linspace(0, 10, 100)\n",
    "axes[0].plot(X_line, true_beta0 + true_beta1 * X_line, 'g--', lw=2, label='Истинная зависимость')\n",
    "axes[0].plot(X_line, beta0_mle + beta1_mle * X_line, 'r-', lw=2, label='MLE/MSE оценка')\n",
    "axes[0].set_xlabel('Время (часы)', fontsize=11)\n",
    "axes[0].set_ylabel('Концентрация белка', fontsize=11)\n",
    "axes[0].set_title('Линейная регрессия через MLE/MSE', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# График 2: Поверхность log-likelihood\n",
    "beta0_range = np.linspace(1, 4, 40)\n",
    "beta1_range = np.linspace(2, 4, 40)\n",
    "B0, B1 = np.meshgrid(beta0_range, beta1_range)\n",
    "\n",
    "LL = np.zeros_like(B0)\n",
    "for i in range(len(beta0_range)):\n",
    "    for j in range(len(beta1_range)):\n",
    "        LL[j, i] = -negative_log_likelihood_linear([B0[j, i], B1[j, i], sigma_mle], X, y)\n",
    "\n",
    "contour = axes[1].contourf(B0, B1, LL, levels=30, cmap='viridis')\n",
    "axes[1].plot(beta0_mle, beta1_mle, 'r*', markersize=20, label='MLE оценка', markeredgecolor='white', markeredgewidth=1.5)\n",
    "axes[1].plot(true_beta0, true_beta1, 'wo', markersize=12, label='Истинные значения', markeredgecolor='red', markeredgewidth=1.5)\n",
    "axes[1].set_xlabel('β₀ (intercept)', fontsize=11)\n",
    "axes[1].set_ylabel('β₁ (slope)', fontsize=11)\n",
    "axes[1].set_title('Поверхность log-likelihood', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "plt.colorbar(contour, ax=axes[1], label='Log-likelihood')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Логистическая регрессия: от MLE к Binary Cross-Entropy\n",
    "\n",
    "### Теория\n",
    "\n",
    "**Предположение**: Целевая переменная распределена по Бернулли (бинарная классификация)\n",
    "\n",
    "$$y_i \\sim \\text{Bernoulli}(p_i)$$\n",
    "\n",
    "где вероятность определяется сигмоидной функцией:\n",
    "\n",
    "$$p_i = \\sigma(\\beta_0 + \\beta_1 x_i) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_i)}}$$\n",
    "\n",
    "**Функция правдоподобия** (для бинарных исходов $y_i \\in \\{0, 1\\}$):\n",
    "\n",
    "$$L(\\beta_0, \\beta_1 | X, y) = \\prod_{i=1}^{n} p_i^{y_i}(1-p_i)^{1-y_i}$$\n",
    "\n",
    "**Логарифм правдоподобия**:\n",
    "\n",
    "$$\\log L = \\sum_{i=1}^{n} \\left[y_i \\log(p_i) + (1-y_i)\\log(1-p_i)\\right]$$\n",
    "\n",
    "**Максимизация** $\\log L$ **эквивалентна минимизации Binary Cross-Entropy (BCE)**:\n",
    "\n",
    "$$\\text{BCE} = -\\frac{1}{n}\\sum_{i=1}^{n}\\left[y_i \\log(\\hat{p}_i) + (1-y_i)\\log(1-\\hat{p}_i)\\right]$$\n",
    "\n",
    "### Вывод: BCE loss = MLE при распределении Бернулли!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практический пример: Классификация больных/здоровых на основе биомаркера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем данные: бинарная классификация\n",
    "n = 200\n",
    "X_log = np.random.uniform(-5, 5, n)  # уровень биомаркера\n",
    "true_beta0_log = -1.0  # порог\n",
    "true_beta1_log = 0.8   # чувствительность к биомаркеру\n",
    "\n",
    "# Истинная вероятность заболевания\n",
    "true_probs = 1 / (1 + np.exp(-(true_beta0_log + true_beta1_log * X_log)))\n",
    "\n",
    "# Генерируем бинарные исходы (0 = здоров, 1 = болен)\n",
    "y_log = np.random.binomial(1, true_probs)\n",
    "\n",
    "print(f\"Истинные параметры: β₀={true_beta0_log}, β₁={true_beta1_log}\")\n",
    "print(f\"Размер выборки: n={n}\")\n",
    "print(f\"Баланс классов: {np.sum(y_log==0)} здоровых, {np.sum(y_log==1)} больных\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Сигмоидная функция с защитой от переполнения\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод 1: Максимизация Likelihood (MLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood_logistic(params, X, y):\n",
    "    \"\"\"\n",
    "    Отрицательный логарифм правдоподобия для логистической регрессии\n",
    "    с предположением о распределении Бернулли\n",
    "    \"\"\"\n",
    "    beta0, beta1 = params\n",
    "    \n",
    "    # Вычисляем вероятности\n",
    "    z = beta0 + beta1 * X\n",
    "    probs = sigmoid(z)\n",
    "    \n",
    "    # Защита от log(0)\n",
    "    epsilon = 1e-15\n",
    "    probs = np.clip(probs, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # Negative log-likelihood\n",
    "    nll = -np.sum(y * np.log(probs) + (1 - y) * np.log(1 - probs))\n",
    "    \n",
    "    return nll\n",
    "\n",
    "# Оптимизация: находим параметры, максимизирующие likelihood\n",
    "initial_params_log = [0, 0]\n",
    "result_mle_log = minimize(\n",
    "    negative_log_likelihood_logistic, \n",
    "    initial_params_log, \n",
    "    args=(X_log, y_log), \n",
    "    method='L-BFGS-B'\n",
    ")\n",
    "\n",
    "beta0_mle_log, beta1_mle_log = result_mle_log.x\n",
    "\n",
    "print(\"\\n=== MLE подход ===\")\n",
    "print(f\"MLE оценки: β₀={beta0_mle_log:.3f}, β₁={beta1_mle_log:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод 2: Минимизация Binary Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(params, X, y):\n",
    "    \"\"\"\n",
    "    Binary Cross-Entropy loss функция\n",
    "    \"\"\"\n",
    "    beta0, beta1 = params\n",
    "    \n",
    "    # Вычисляем вероятности\n",
    "    z = beta0 + beta1 * X\n",
    "    probs = sigmoid(z)\n",
    "    \n",
    "    # Защита от log(0)\n",
    "    epsilon = 1e-15\n",
    "    probs = np.clip(probs, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # Binary Cross-Entropy\n",
    "    bce = -np.mean(y * np.log(probs) + (1 - y) * np.log(1 - probs))\n",
    "    \n",
    "    return bce\n",
    "\n",
    "# Оптимизация: находим параметры, минимизирующие BCE\n",
    "result_bce_log = minimize(\n",
    "    binary_cross_entropy_loss, \n",
    "    initial_params_log, \n",
    "    args=(X_log, y_log), \n",
    "    method='L-BFGS-B'\n",
    ")\n",
    "\n",
    "beta0_bce_log, beta1_bce_log = result_bce_log.x\n",
    "\n",
    "print(\"\\n=== BCE подход ===\")\n",
    "print(f\"BCE оценки: β₀={beta0_bce_log:.3f}, β₁={beta1_bce_log:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод 3: sklearn LogisticRegression (также максимизирует likelihood/минимизирует BCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn использует численную оптимизацию\n",
    "model_log_sklearn = LogisticRegression(penalty=None, solver='lbfgs')\n",
    "model_log_sklearn.fit(X_log.reshape(-1, 1), y_log)\n",
    "\n",
    "beta0_sklearn_log = model_log_sklearn.intercept_[0]\n",
    "beta1_sklearn_log = model_log_sklearn.coef_[0][0]\n",
    "\n",
    "print(\"\\n=== sklearn LogisticRegression ===\")\n",
    "print(f\"sklearn оценки: β₀={beta0_sklearn_log:.3f}, β₁={beta1_sklearn_log:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"СРАВНЕНИЕ МЕТОДОВ\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Истинные параметры:    β₀={true_beta0_log:.3f}, β₁={true_beta1_log:.3f}\")\n",
    "print(f\"MLE (max likelihood):  β₀={beta0_mle_log:.3f}, β₁={beta1_mle_log:.3f}\")\n",
    "print(f\"BCE (min loss):        β₀={beta0_bce_log:.3f}, β₁={beta1_bce_log:.3f}\")\n",
    "print(f\"sklearn:               β₀={beta0_sklearn_log:.3f}, β₁={beta1_sklearn_log:.3f}\")\n",
    "print(\"\\n✓ Все три метода дают практически одинаковые результаты!\")\n",
    "print(\"  Это подтверждает: минимизация BCE = максимизация likelihood\")\n",
    "\n",
    "# Вычисляем BCE для MLE решения\n",
    "probs_mle = sigmoid(beta0_mle_log + beta1_mle_log * X_log)\n",
    "bce_value = -np.mean(y_log * np.log(probs_mle + 1e-15) + (1 - y_log) * np.log(1 - probs_mle + 1e-15))\n",
    "print(f\"\\nBCE для MLE решения: {bce_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# График 1: Данные и вероятности\n",
    "axes[0].scatter(X_log[y_log==0], y_log[y_log==0], alpha=0.5, s=40, label='Класс 0 (здоров)', color='blue', marker='o')\n",
    "axes[0].scatter(X_log[y_log==1], y_log[y_log==1], alpha=0.5, s=40, label='Класс 1 (болен)', color='red', marker='^')\n",
    "\n",
    "X_line_log = np.linspace(-5, 5, 200)\n",
    "true_probs_line = sigmoid(true_beta0_log + true_beta1_log * X_line_log)\n",
    "mle_probs_line = sigmoid(beta0_mle_log + beta1_mle_log * X_line_log)\n",
    "\n",
    "axes[0].plot(X_line_log, true_probs_line, 'g--', lw=2.5, label='Истинная вероятность')\n",
    "axes[0].plot(X_line_log, mle_probs_line, 'orange', lw=2.5, label='MLE/BCE оценка')\n",
    "axes[0].axhline(0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "axes[0].set_xlabel('Уровень биомаркера', fontsize=11)\n",
    "axes[0].set_ylabel('P(болен | биомаркер)', fontsize=11)\n",
    "axes[0].set_title('Логистическая регрессия через MLE/BCE', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(loc='best')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(-0.1, 1.1)\n",
    "\n",
    "# График 2: Поверхность log-likelihood\n",
    "beta0_range_log = np.linspace(-2, 0, 40)\n",
    "beta1_range_log = np.linspace(0.4, 1.2, 40)\n",
    "B0_log, B1_log = np.meshgrid(beta0_range_log, beta1_range_log)\n",
    "\n",
    "LL_log = np.zeros_like(B0_log)\n",
    "for i in range(len(beta0_range_log)):\n",
    "    for j in range(len(beta1_range_log)):\n",
    "        LL_log[j, i] = -negative_log_likelihood_logistic([B0_log[j, i], B1_log[j, i]], X_log, y_log)\n",
    "\n",
    "contour_log = axes[1].contourf(B0_log, B1_log, LL_log, levels=30, cmap='viridis')\n",
    "axes[1].plot(beta0_mle_log, beta1_mle_log, 'r*', markersize=20, label='MLE оценка', markeredgecolor='white', markeredgewidth=1.5)\n",
    "axes[1].plot(true_beta0_log, true_beta1_log, 'wo', markersize=12, label='Истинные значения', markeredgecolor='red', markeredgewidth=1.5)\n",
    "axes[1].set_xlabel('β₀ (intercept)', fontsize=11)\n",
    "axes[1].set_ylabel('β₁ (slope)', fontsize=11)\n",
    "axes[1].set_title('Поверхность log-likelihood', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "plt.colorbar(contour_log, ax=axes[1], label='Log-likelihood')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Резюме\n",
    "\n",
    "### Ключевые выводы:\n",
    "\n",
    "| Тип регрессии | Предположение о распределении | Функция правдоподобия | Loss функция | Связь |\n",
    "|--------------|------------------------------|----------------------|--------------|-------|\n",
    "| **Линейная** | $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ | Произведение нормальных плотностей | **MSE** (Mean Squared Error) | max(likelihood) = min(MSE) |\n",
    "| **Логистическая** | $y \\sim \\text{Bernoulli}(p)$ | Произведение вероятностей Бернулли | **BCE** (Binary Cross-Entropy) | max(likelihood) = min(BCE) |\n",
    "\n",
    "### Общий принцип:\n",
    "\n",
    "1. **Выбираем распределение** для ошибок/целевой переменной\n",
    "2. **Записываем likelihood функцию** на основе этого распределения\n",
    "3. **Максимизируем log-likelihood** (или минимизируем negative log-likelihood)\n",
    "4. Получаем **loss функцию** для машинного обучения!\n",
    "\n",
    "### Практическое значение:\n",
    "\n",
    "- MLE дает **теоретическое обоснование** для выбора loss функций\n",
    "- Разные предположения о данных → разные loss функции\n",
    "- Понимание связи помогает выбирать правильные метрики и модели\n",
    "- В биоинформатике это важно для моделирования экспрессии генов, классификации болезней и т.д.\n",
    "\n",
    "### Другие примеры:\n",
    "\n",
    "- **Пуассоновская регрессия** (для count data): предположение $y \\sim \\text{Poisson}(\\lambda)$ → Poisson loss\n",
    "- **Мультиклассовая классификация**: предположение $y \\sim \\text{Categorical}(p_1, ..., p_k)$ → Categorical Cross-Entropy\n",
    "- **Робастная регрессия**: предположение о распределении Лапласа → MAE (Mean Absolute Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Задания для самостоятельной работы\n",
    "\n",
    "1. **Сравните поведение MSE и MAE**: Создайте датасет с выбросами и сравните, как работают регрессии с MSE (предположение о нормальном распределении) и MAE (предположение о распределении Лапласа)\n",
    "\n",
    "2. **Мультиклассовая классификация**: Расширьте пример логистической регрессии на случай 3+ классов. Покажите связь между максимизацией likelihood и минимизацией Categorical Cross-Entropy\n",
    "\n",
    "3. **Пуассоновская регрессия**: Реализуйте регрессию для count data (например, количество мутаций). Используйте Пуассоновское распределение и выведите соответствующую loss функцию\n",
    "\n",
    "4. **Эффект размера выборки**: Исследуйте, как размер выборки влияет на точность MLE оценок. Постройте графики для n = 20, 50, 100, 500, 1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
