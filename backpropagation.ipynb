{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Обратное распространение ошибки (Backpropagation)\n",
    "\n",
    "В этой тетрадке мы детально разберем:\n",
    "- Математику обратного распространения\n",
    "- Вычислительный граф и цепное правило\n",
    "- Пошаговую реализацию с нуля\n",
    "- Численную проверку градиентов\n",
    "- Визуализацию процесса обучения\n",
    "- Практические примеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Настройка графиков\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Введение: Зачем нужно обратное распространение?\n",
    "\n",
    "### Задача обучения нейронной сети:\n",
    "\n",
    "Дано:\n",
    "- Обучающие данные: $(x^{(i)}, y^{(i)})$, где $i = 1, ..., m$\n",
    "- Нейронная сеть с параметрами (весами и смещениями)\n",
    "- Функция потерь $L(\\hat{y}, y)$\n",
    "\n",
    "Найти:\n",
    "- Оптимальные значения параметров, минимизирующие функцию потерь\n",
    "\n",
    "### Метод градиентного спуска:\n",
    "\n",
    "$$w := w - \\alpha \\frac{\\partial L}{\\partial w}$$\n",
    "\n",
    "где $\\alpha$ — learning rate (скорость обучения).\n",
    "\n",
    "**Проблема:** Как вычислить $\\frac{\\partial L}{\\partial w}$ для всех весов в сети?\n",
    "\n",
    "**Решение:** Алгоритм обратного распространения ошибки (backpropagation)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Математические основы: Цепное правило\n",
    "\n",
    "### Цепное правило для композиции функций:\n",
    "\n",
    "Если $y = f(u)$ и $u = g(x)$, то:\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$$\n",
    "\n",
    "### Для многомерного случая:\n",
    "\n",
    "Если $z = f(x, y)$, $x = g(t)$, $y = h(t)$, то:\n",
    "$$\\frac{dz}{dt} = \\frac{\\partial z}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial z}{\\partial y} \\frac{dy}{dt}$$\n",
    "\n",
    "### Пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_rule_example():\n",
    "    \"\"\"\n",
    "    Пример: f(x) = (3x + 2)^2\n",
    "    \n",
    "    Представим как композицию:\n",
    "    u = 3x + 2\n",
    "    f = u^2\n",
    "    \n",
    "    Тогда: df/dx = df/du * du/dx = 2u * 3 = 6u = 6(3x + 2)\n",
    "    \"\"\"\n",
    "    \n",
    "    x = 2.0\n",
    "    \n",
    "    # Прямое распространение\n",
    "    u = 3 * x + 2  # u = 8\n",
    "    f = u ** 2      # f = 64\n",
    "    \n",
    "    print(\"Прямое распространение:\")\n",
    "    print(f\"x = {x}\")\n",
    "    print(f\"u = 3x + 2 = {u}\")\n",
    "    print(f\"f = u^2 = {f}\")\n",
    "    \n",
    "    # Обратное распространение\n",
    "    df_du = 2 * u   # df/du = 2u = 16\n",
    "    du_dx = 3       # du/dx = 3\n",
    "    df_dx = df_du * du_dx  # df/dx = 48\n",
    "    \n",
    "    print(\"\\nОбратное распространение (цепное правило):\")\n",
    "    print(f\"df/du = 2u = {df_du}\")\n",
    "    print(f\"du/dx = 3 = {du_dx}\")\n",
    "    print(f\"df/dx = df/du * du/dx = {df_dx}\")\n",
    "    \n",
    "    # Проверка численным методом\n",
    "    epsilon = 1e-7\n",
    "    f_plus = (3 * (x + epsilon) + 2) ** 2\n",
    "    f_minus = (3 * (x - epsilon) + 2) ** 2\n",
    "    df_dx_numerical = (f_plus - f_minus) / (2 * epsilon)\n",
    "    \n",
    "    print(\"\\nЧисленная проверка:\")\n",
    "    print(f\"df/dx (численно) = {df_dx_numerical:.6f}\")\n",
    "    print(f\"df/dx (аналитически) = {df_dx}\")\n",
    "    print(f\"Разница: {abs(df_dx - df_dx_numerical):.10f}\")\n",
    "\n",
    "chain_rule_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Вычислительный граф\n",
    "\n",
    "Вычислительный граф — это направленный граф, где:\n",
    "- **Узлы** — операции или переменные\n",
    "- **Рёбра** — поток данных\n",
    "\n",
    "### Пример вычислительного графа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_computational_graph():\n",
    "    \"\"\"Визуализация вычислительного графа для f = (x*w + b)^2\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Координаты узлов\n",
    "    nodes = {\n",
    "        'x': (1, 3),\n",
    "        'w': (1, 1),\n",
    "        'b': (3, 1),\n",
    "        'mul': (2, 2),\n",
    "        'add': (4, 2),\n",
    "        'square': (6, 2),\n",
    "        'L': (8, 2)\n",
    "    }\n",
    "    \n",
    "    # Прямое распространение (синие стрелки)\n",
    "    forward_edges = [\n",
    "        ('x', 'mul', 'x=2'),\n",
    "        ('w', 'mul', 'w=3'),\n",
    "        ('mul', 'add', 'x*w=6'),\n",
    "        ('b', 'add', 'b=1'),\n",
    "        ('add', 'square', 'x*w+b=7'),\n",
    "        ('square', 'L', '(x*w+b)²=49')\n",
    "    ]\n",
    "    \n",
    "    # Обратное распространение (красные стрелки)\n",
    "    backward_edges = [\n",
    "        ('L', 'square', 'dL/d(•)=1'),\n",
    "        ('square', 'add', 'dL/d(•)=14'),\n",
    "        ('add', 'mul', 'dL/d(•)=14'),\n",
    "        ('add', 'b', 'dL/db=14'),\n",
    "        ('mul', 'w', 'dL/dw=28'),\n",
    "        ('mul', 'x', 'dL/dx=42')\n",
    "    ]\n",
    "    \n",
    "    # Рисуем узлы\n",
    "    for name, (x, y) in nodes.items():\n",
    "        if name in ['x', 'w', 'b']:\n",
    "            color = 'lightblue'\n",
    "            label = f'{name}'\n",
    "        elif name == 'L':\n",
    "            color = 'lightcoral'\n",
    "            label = 'Loss'\n",
    "        else:\n",
    "            color = 'lightgreen'\n",
    "            if name == 'mul':\n",
    "                label = '×'\n",
    "            elif name == 'add':\n",
    "                label = '+'\n",
    "            else:\n",
    "                label = '²'\n",
    "        \n",
    "        circle = plt.Circle((x, y), 0.3, color=color, ec='black', linewidth=2, zorder=3)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(x, y, label, ha='center', va='center', fontsize=14, fontweight='bold', zorder=4)\n",
    "    \n",
    "    # Рисуем прямые рёбра\n",
    "    for src, dst, label in forward_edges:\n",
    "        x1, y1 = nodes[src]\n",
    "        x2, y2 = nodes[dst]\n",
    "        \n",
    "        # Стрелка\n",
    "        dx = x2 - x1\n",
    "        dy = y2 - y1\n",
    "        length = np.sqrt(dx**2 + dy**2)\n",
    "        dx_norm = dx / length * 0.3\n",
    "        dy_norm = dy / length * 0.3\n",
    "        \n",
    "        ax.arrow(x1 + dx_norm, y1 + dy_norm, \n",
    "                dx - 2*dx_norm, dy - 2*dy_norm,\n",
    "                head_width=0.15, head_length=0.1, \n",
    "                fc='blue', ec='blue', linewidth=2, zorder=1)\n",
    "        \n",
    "        # Метка\n",
    "        ax.text((x1 + x2) / 2, (y1 + y2) / 2 + 0.3, label, \n",
    "               ha='center', fontsize=9, color='blue', zorder=2)\n",
    "    \n",
    "    # Рисуем обратные рёбра (пунктиром)\n",
    "    for src, dst, label in backward_edges:\n",
    "        x1, y1 = nodes[src]\n",
    "        x2, y2 = nodes[dst]\n",
    "        \n",
    "        # Стрелка (смещена вниз)\n",
    "        offset = -0.2\n",
    "        dx = x2 - x1\n",
    "        dy = y2 - y1\n",
    "        length = np.sqrt(dx**2 + dy**2)\n",
    "        dx_norm = dx / length * 0.3\n",
    "        dy_norm = dy / length * 0.3\n",
    "        \n",
    "        ax.arrow(x1 + dx_norm, y1 + dy_norm + offset, \n",
    "                dx - 2*dx_norm, dy - 2*dy_norm,\n",
    "                head_width=0.15, head_length=0.1,\n",
    "                fc='red', ec='red', linewidth=2, \n",
    "                linestyle='--', alpha=0.7, zorder=1)\n",
    "        \n",
    "        # Метка\n",
    "        ax.text((x1 + x2) / 2, (y1 + y2) / 2 - 0.4, label,\n",
    "               ha='center', fontsize=9, color='red', zorder=2)\n",
    "    \n",
    "    # Легенда\n",
    "    ax.text(1, 4.2, 'Вычислительный граф', fontsize=16, fontweight='bold')\n",
    "    ax.text(1, 3.8, '→ Прямое распространение (forward)', color='blue', fontsize=11)\n",
    "    ax.text(1, 3.5, '⇢ Обратное распространение (backward)', color='red', fontsize=11)\n",
    "    \n",
    "    ax.set_xlim(0, 9)\n",
    "    ax.set_ylim(0, 4.5)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_computational_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 4. Обратное распространение в простой сети\n",
    "\n",
    "Рассмотрим простую сеть с одним скрытым слоем:\n",
    "\n",
    "```\n",
    "Input (x) → Hidden (h) → Output (ŷ) → Loss (L)\n",
    "```\n",
    "\n",
    "### Архитектура:\n",
    "\n",
    "**Слой 1 (вход → скрытый):**\n",
    "$$z^{[1]} = W^{[1]} x + b^{[1]}$$\n",
    "$$a^{[1]} = \\sigma(z^{[1]})$$\n",
    "\n",
    "**Слой 2 (скрытый → выход):**\n",
    "$$z^{[2]} = W^{[2]} a^{[1]} + b^{[2]}$$\n",
    "$$a^{[2]} = \\sigma(z^{[2]}) = \\hat{y}$$\n",
    "\n",
    "**Функция потерь (MSE):**\n",
    "$$L = \\frac{1}{2}(\\hat{y} - y)^2$$\n",
    "\n",
    "### Прямое распространение (Forward Pass):\n",
    "\n",
    "1. Вычислить $z^{[1]}, a^{[1]}$\n",
    "2. Вычислить $z^{[2]}, a^{[2]}$\n",
    "3. Вычислить $L$\n",
    "\n",
    "### Обратное распространение (Backward Pass):\n",
    "\n",
    "**Выходной слой:**\n",
    "$$\\frac{\\partial L}{\\partial a^{[2]}} = \\hat{y} - y$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z^{[2]}} = \\frac{\\partial L}{\\partial a^{[2]}} \\cdot \\sigma'(z^{[2]})$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W^{[2]}} = \\frac{\\partial L}{\\partial z^{[2]}} \\cdot (a^{[1]})^T$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b^{[2]}} = \\frac{\\partial L}{\\partial z^{[2]}}$$\n",
    "\n",
    "**Скрытый слой:**\n",
    "$$\\frac{\\partial L}{\\partial a^{[1]}} = (W^{[2]})^T \\cdot \\frac{\\partial L}{\\partial z^{[2]}}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z^{[1]}} = \\frac{\\partial L}{\\partial a^{[1]}} \\cdot \\sigma'(z^{[1]})$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W^{[1]}} = \\frac{\\partial L}{\\partial z^{[1]}} \\cdot x^T$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b^{[1]}} = \\frac{\\partial L}{\\partial z^{[1]}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 5. Пошаговая реализация с детальными выводами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkVerbose:\n",
    "    \"\"\"Нейронная сеть с подробным выводом вычислений\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Инициализация весов\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.5\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.5\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        print(\"Инициализация сети:\")\n",
    "        print(f\"W1 shape: {self.W1.shape}, b1 shape: {self.b1.shape}\")\n",
    "        print(f\"W2 shape: {self.W2.shape}, b2 shape: {self.b2.shape}\")\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def forward_verbose(self, X, y):\n",
    "        \"\"\"Прямое распространение с подробным выводом\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ПРЯМОЕ РАСПРОСТРАНЕНИЕ (FORWARD PASS)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Слой 1\n",
    "        print(\"\\n--- Слой 1: Вход → Скрытый ---\")\n",
    "        print(f\"Вход X shape: {X.shape}\")\n",
    "        \n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        print(f\"z1 = X @ W1 + b1\")\n",
    "        print(f\"z1 shape: {self.z1.shape}\")\n",
    "        print(f\"z1[0, :3] = {self.z1[0, :3]}\")\n",
    "        \n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        print(f\"\\na1 = sigmoid(z1)\")\n",
    "        print(f\"a1[0, :3] = {self.a1[0, :3]}\")\n",
    "        \n",
    "        # Слой 2\n",
    "        print(\"\\n--- Слой 2: Скрытый → Выход ---\")\n",
    "        \n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        print(f\"z2 = a1 @ W2 + b2\")\n",
    "        print(f\"z2 shape: {self.z2.shape}\")\n",
    "        print(f\"z2[0] = {self.z2[0]}\")\n",
    "        \n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        print(f\"\\na2 = sigmoid(z2) = ŷ (предсказание)\")\n",
    "        print(f\"a2[0] = {self.a2[0]}\")\n",
    "        \n",
    "        # Loss\n",
    "        print(\"\\n--- Вычисление функции потерь ---\")\n",
    "        self.loss = np.mean((self.a2 - y) ** 2)\n",
    "        print(f\"Loss = MSE = mean((ŷ - y)²)\")\n",
    "        print(f\"Loss = {self.loss:.6f}\")\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def backward_verbose(self, X, y):\n",
    "        \"\"\"Обратное распространение с подробным выводом\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ОБРАТНОЕ РАСПРОСТРАНЕНИЕ (BACKWARD PASS)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Градиенты выходного слоя\n",
    "        print(\"\\n--- Слой 2: Градиенты выходного слоя ---\")\n",
    "        \n",
    "        # dL/da2\n",
    "        dL_da2 = 2 * (self.a2 - y) / m\n",
    "        print(f\"dL/da2 = 2(ŷ - y) / m\")\n",
    "        print(f\"dL/da2[0] = {dL_da2[0]}\")\n",
    "        \n",
    "        # dL/dz2 = dL/da2 * da2/dz2\n",
    "        da2_dz2 = self.sigmoid_derivative(self.z2)\n",
    "        dL_dz2 = dL_da2 * da2_dz2\n",
    "        print(f\"\\ndL/dz2 = dL/da2 ⊙ sigmoid'(z2)  (⊙ = поэлементное умножение)\")\n",
    "        print(f\"dL/dz2[0] = {dL_dz2[0]}\")\n",
    "        \n",
    "        # dL/dW2 = a1^T @ dL/dz2\n",
    "        dL_dW2 = np.dot(self.a1.T, dL_dz2)\n",
    "        print(f\"\\ndL/dW2 = a1.T @ dL/dz2\")\n",
    "        print(f\"dL/dW2 shape: {dL_dW2.shape}\")\n",
    "        print(f\"dL/dW2[:3, 0] = {dL_dW2[:3, 0]}\")\n",
    "        \n",
    "        # dL/db2\n",
    "        dL_db2 = np.sum(dL_dz2, axis=0, keepdims=True)\n",
    "        print(f\"\\ndL/db2 = sum(dL/dz2, axis=0)\")\n",
    "        print(f\"dL/db2 = {dL_db2}\")\n",
    "        \n",
    "        # Градиенты скрытого слоя\n",
    "        print(\"\\n--- Слой 1: Градиенты скрытого слоя ---\")\n",
    "        \n",
    "        # dL/da1 = dL/dz2 @ W2^T\n",
    "        dL_da1 = np.dot(dL_dz2, self.W2.T)\n",
    "        print(f\"dL/da1 = dL/dz2 @ W2.T  (обратное распространение через W2)\")\n",
    "        print(f\"dL/da1 shape: {dL_da1.shape}\")\n",
    "        print(f\"dL/da1[0, :3] = {dL_da1[0, :3]}\")\n",
    "        \n",
    "        # dL/dz1 = dL/da1 * da1/dz1\n",
    "        da1_dz1 = self.sigmoid_derivative(self.z1)\n",
    "        dL_dz1 = dL_da1 * da1_dz1\n",
    "        print(f\"\\ndL/dz1 = dL/da1 ⊙ sigmoid'(z1)\")\n",
    "        print(f\"dL/dz1[0, :3] = {dL_dz1[0, :3]}\")\n",
    "        \n",
    "        # dL/dW1 = X^T @ dL/dz1\n",
    "        dL_dW1 = np.dot(X.T, dL_dz1)\n",
    "        print(f\"\\ndL/dW1 = X.T @ dL/dz1\")\n",
    "        print(f\"dL/dW1 shape: {dL_dW1.shape}\")\n",
    "        print(f\"dL/dW1[:3, :3] = \\n{dL_dW1[:3, :3]}\")\n",
    "        \n",
    "        # dL/db1\n",
    "        dL_db1 = np.sum(dL_dz1, axis=0, keepdims=True)\n",
    "        print(f\"\\ndL/db1 = sum(dL/dz1, axis=0)\")\n",
    "        print(f\"dL/db1[:3] = {dL_db1[0, :3]}\")\n",
    "        \n",
    "        return dL_dW1, dL_db1, dL_dW2, dL_db2\n",
    "    \n",
    "    def update_weights(self, dW1, db1, dW2, db2, learning_rate):\n",
    "        \"\"\"Обновление весов методом градиентного спуска\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ОБНОВЛЕНИЕ ВЕСОВ (GRADIENT DESCENT)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nLearning rate α = {learning_rate}\")\n",
    "        \n",
    "        print(\"\\nПеред обновлением:\")\n",
    "        print(f\"W1[0, 0] = {self.W1[0, 0]:.6f}\")\n",
    "        print(f\"W2[0, 0] = {self.W2[0, 0]:.6f}\")\n",
    "        \n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        \n",
    "        print(\"\\nПосле обновления:\")\n",
    "        print(f\"W1[0, 0] = {self.W1[0, 0]:.6f}  (изменение: {-learning_rate * dW1[0, 0]:.6f})\")\n",
    "        print(f\"W2[0, 0] = {self.W2[0, 0]:.6f}  (изменение: {-learning_rate * dW2[0, 0]:.6f})\")\n",
    "\n",
    "\n",
    "# Демонстрация на простом примере\n",
    "print(\"\\n\" + \"#\"*60)\n",
    "print(\"# ДЕМОНСТРАЦИЯ ОБРАТНОГО РАСПРОСТРАНЕНИЯ\")\n",
    "print(\"#\"*60)\n",
    "\n",
    "# Простые данные\n",
    "X = np.array([[0.5, 0.3]])\n",
    "y = np.array([[1.0]])\n",
    "\n",
    "print(f\"\\nВходные данные: X = {X}\")\n",
    "print(f\"Целевое значение: y = {y}\")\n",
    "\n",
    "# Создание сети\n",
    "nn = NeuralNetworkVerbose(input_size=2, hidden_size=3, output_size=1)\n",
    "\n",
    "# Прямое распространение\n",
    "predictions = nn.forward_verbose(X, y)\n",
    "\n",
    "# Обратное распространение\n",
    "dW1, db1, dW2, db2 = nn.backward_verbose(X, y)\n",
    "\n",
    "# Обновление весов\n",
    "nn.update_weights(dW1, db1, dW2, db2, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 6. Численная проверка градиентов (Gradient Checking)\n",
    "\n",
    "Для проверки правильности реализации обратного распространения используем численное дифференцирование:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} \\approx \\frac{L(w + \\epsilon) - L(w - \\epsilon)}{2\\epsilon}$$\n",
    "\n",
    "где $\\epsilon$ — малое число (например, $10^{-7}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(nn, X, y, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Численная проверка градиентов\n",
    "    \n",
    "    Сравнивает аналитические градиенты (из backprop) с численными\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ЧИСЛЕННАЯ ПРОВЕРКА ГРАДИЕНТОВ (GRADIENT CHECKING)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Получаем аналитические градиенты\n",
    "    nn.forward_verbose = lambda X, y: nn.sigmoid(np.dot(nn.sigmoid(np.dot(X, nn.W1) + nn.b1), nn.W2) + nn.b2)\n",
    "    \n",
    "    # Вычисляем loss\n",
    "    def compute_loss(nn, X, y):\n",
    "        z1 = np.dot(X, nn.W1) + nn.b1\n",
    "        a1 = nn.sigmoid(z1)\n",
    "        z2 = np.dot(a1, nn.W2) + nn.b2\n",
    "        a2 = nn.sigmoid(z2)\n",
    "        return np.mean((a2 - y) ** 2)\n",
    "    \n",
    "    # Аналитические градиенты через backprop\n",
    "    z1 = np.dot(X, nn.W1) + nn.b1\n",
    "    a1 = nn.sigmoid(z1)\n",
    "    z2 = np.dot(a1, nn.W2) + nn.b2\n",
    "    a2 = nn.sigmoid(z2)\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    dL_da2 = 2 * (a2 - y) / m\n",
    "    dL_dz2 = dL_da2 * nn.sigmoid_derivative(z2)\n",
    "    dL_dW2_analytical = np.dot(a1.T, dL_dz2)\n",
    "    \n",
    "    dL_da1 = np.dot(dL_dz2, nn.W2.T)\n",
    "    dL_dz1 = dL_da1 * nn.sigmoid_derivative(z1)\n",
    "    dL_dW1_analytical = np.dot(X.T, dL_dz1)\n",
    "    \n",
    "    # Численные градиенты для W1[0, 0]\n",
    "    print(\"\\nПроверка градиента для W1[0, 0]:\")\n",
    "    \n",
    "    original_value = nn.W1[0, 0]\n",
    "    \n",
    "    # L(w + epsilon)\n",
    "    nn.W1[0, 0] = original_value + epsilon\n",
    "    loss_plus = compute_loss(nn, X, y)\n",
    "    \n",
    "    # L(w - epsilon)\n",
    "    nn.W1[0, 0] = original_value - epsilon\n",
    "    loss_minus = compute_loss(nn, X, y)\n",
    "    \n",
    "    # Восстанавливаем\n",
    "    nn.W1[0, 0] = original_value\n",
    "    \n",
    "    # Численный градиент\n",
    "    dL_dW1_numerical = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "    dL_dW1_analytical_value = dL_dW1_analytical[0, 0]\n",
    "    \n",
    "    print(f\"Аналитический градиент: {dL_dW1_analytical_value:.10f}\")\n",
    "    print(f\"Численный градиент:     {dL_dW1_numerical:.10f}\")\n",
    "    print(f\"Разница:                {abs(dL_dW1_analytical_value - dL_dW1_numerical):.2e}\")\n",
    "    \n",
    "    # Проверка градиента для W2[0, 0]\n",
    "    print(\"\\nПроверка градиента для W2[0, 0]:\")\n",
    "    \n",
    "    original_value = nn.W2[0, 0]\n",
    "    \n",
    "    nn.W2[0, 0] = original_value + epsilon\n",
    "    loss_plus = compute_loss(nn, X, y)\n",
    "    \n",
    "    nn.W2[0, 0] = original_value - epsilon\n",
    "    loss_minus = compute_loss(nn, X, y)\n",
    "    \n",
    "    nn.W2[0, 0] = original_value\n",
    "    \n",
    "    dL_dW2_numerical = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "    dL_dW2_analytical_value = dL_dW2_analytical[0, 0]\n",
    "    \n",
    "    print(f\"Аналитический градиент: {dL_dW2_analytical_value:.10f}\")\n",
    "    print(f\"Численный градиент:     {dL_dW2_numerical:.10f}\")\n",
    "    print(f\"Разница:                {abs(dL_dW2_analytical_value - dL_dW2_numerical):.2e}\")\n",
    "    \n",
    "    print(\"\\n✓ Если разница < 1e-7, то градиенты вычислены правильно!\")\n",
    "\n",
    "# Проверка\n",
    "gradient_check(nn, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 7. Полная реализация с визуализацией обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkBackprop:\n",
    "    \"\"\"Полная реализация нейронной сети с backpropagation\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes) - 1\n",
    "        \n",
    "        # Инициализация весов (He инициализация)\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])\n",
    "            b = np.zeros((1, layer_sizes[i+1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "        \n",
    "        # История обучения\n",
    "        self.losses = []\n",
    "        self.gradient_norms = []\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Прямое распространение\"\"\"\n",
    "        self.activations = [X]\n",
    "        self.zs = []\n",
    "        \n",
    "        A = X\n",
    "        for i in range(self.num_layers):\n",
    "            Z = np.dot(A, self.weights[i]) + self.biases[i]\n",
    "            self.zs.append(Z)\n",
    "            \n",
    "            # Используем ReLU для скрытых слоев, sigmoid для выходного\n",
    "            if i == self.num_layers - 1:\n",
    "                A = self.sigmoid(Z)\n",
    "            else:\n",
    "                A = self.relu(Z)\n",
    "            \n",
    "            self.activations.append(A)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        \"\"\"Обратное распространение\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Инициализация градиентов\n",
    "        dW = [np.zeros_like(w) for w in self.weights]\n",
    "        db = [np.zeros_like(b) for b in self.biases]\n",
    "        \n",
    "        # Градиент выходного слоя (для MSE)\n",
    "        dA = 2 * (self.activations[-1] - y) / m\n",
    "        \n",
    "        # Обратное распространение через слои\n",
    "        for i in reversed(range(self.num_layers)):\n",
    "            # Градиент по z\n",
    "            if i == self.num_layers - 1:\n",
    "                dZ = dA * self.sigmoid_derivative(self.zs[i])\n",
    "            else:\n",
    "                dZ = dA * self.relu_derivative(self.zs[i])\n",
    "            \n",
    "            # Градиенты по весам и смещениям\n",
    "            dW[i] = np.dot(self.activations[i].T, dZ)\n",
    "            db[i] = np.sum(dZ, axis=0, keepdims=True)\n",
    "            \n",
    "            # Градиент для предыдущего слоя\n",
    "            if i > 0:\n",
    "                dA = np.dot(dZ, self.weights[i].T)\n",
    "        \n",
    "        # Сохраняем норму градиентов для визуализации\n",
    "        grad_norm = sum(np.sum(g**2) for g in dW)\n",
    "        self.gradient_norms.append(grad_norm)\n",
    "        \n",
    "        return dW, db\n",
    "    \n",
    "    def update_weights(self, dW, db, learning_rate):\n",
    "        \"\"\"Обновление весов\"\"\"\n",
    "        for i in range(self.num_layers):\n",
    "            self.weights[i] -= learning_rate * dW[i]\n",
    "            self.biases[i] -= learning_rate * db[i]\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        \"\"\"Вычисление MSE loss\"\"\"\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate, verbose=True):\n",
    "        \"\"\"Обучение сети\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y_pred, y)\n",
    "            self.losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            dW, db = self.backward(X, y)\n",
    "            \n",
    "            # Update weights\n",
    "            self.update_weights(dW, db, learning_rate)\n",
    "            \n",
    "            if verbose and epoch % 100 == 0:\n",
    "                accuracy = np.mean((y_pred > 0.5).astype(int) == y)\n",
    "                print(f'Epoch {epoch}: Loss = {loss:.6f}, Accuracy = {accuracy:.4f}')\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Предсказание\"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        return (y_pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 8. Обучение на реальных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация данных (полумесяцы)\n",
    "X, y = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Разделение на train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Размер обучающей выборки: {X_train.shape}\")\n",
    "print(f\"Размер тестовой выборки: {X_test.shape}\")\n",
    "\n",
    "# Визуализация данных\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[y_train.flatten() == 0, 0], X_train[y_train.flatten() == 0, 1],\n",
    "           c='red', marker='o', s=50, alpha=0.7, edgecolors='k', label='Класс 0')\n",
    "plt.scatter(X_train[y_train.flatten() == 1, 0], X_train[y_train.flatten() == 1, 1],\n",
    "           c='blue', marker='s', s=50, alpha=0.7, edgecolors='k', label='Класс 1')\n",
    "plt.xlabel('Признак 1')\n",
    "plt.ylabel('Признак 2')\n",
    "plt.title('Обучающие данные (полумесяцы)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание и обучение сети\n",
    "print(\"\\nОбучение нейронной сети...\\n\")\n",
    "\n",
    "nn = NeuralNetworkBackprop(layer_sizes=[2, 16, 8, 1])\n",
    "nn.train(X_train, y_train, epochs=1000, learning_rate=0.1, verbose=True)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "y_pred_test = nn.predict(X_test)\n",
    "test_accuracy = np.mean(y_pred_test == y_test)\n",
    "print(f\"\\nТочность на тестовой выборке: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация процесса обучения\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# График функции потерь\n",
    "axes[0].plot(nn.losses, linewidth=2)\n",
    "axes[0].set_xlabel('Эпоха')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Кривая обучения: функция потерь')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# График нормы градиентов\n",
    "axes[1].plot(nn.gradient_norms, linewidth=2, color='orange')\n",
    "axes[1].set_xlabel('Эпоха')\n",
    "axes[1].set_ylabel('Норма градиента')\n",
    "axes[1].set_title('Эволюция нормы градиентов')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация границы решений\n",
    "def plot_decision_boundary(model, X, y, title):\n",
    "    \"\"\"Визуализация границы решений\"\"\"\n",
    "    \n",
    "    h = 0.02  # Шаг сетки\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Предсказания для всех точек сетки\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Визуализация\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu')\n",
    "    plt.contour(xx, yy, Z, colors='black', linewidths=0.5, levels=[0.5])\n",
    "    \n",
    "    # Точки данных\n",
    "    plt.scatter(X[y.flatten() == 0, 0], X[y.flatten() == 0, 1],\n",
    "               c='red', marker='o', s=50, alpha=0.7, edgecolors='k', label='Класс 0')\n",
    "    plt.scatter(X[y.flatten() == 1, 0], X[y.flatten() == 1, 1],\n",
    "               c='blue', marker='s', s=50, alpha=0.7, edgecolors='k', label='Класс 1')\n",
    "    \n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xlabel('Признак 1')\n",
    "    plt.ylabel('Признак 2')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(nn, X_train, y_train, \n",
    "                      'Граница решений после обучения')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 9. Визуализация активаций по слоям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_activations(model, X, sample_idx=0):\n",
    "    \"\"\"Визуализация активаций для одного примера\"\"\"\n",
    "    \n",
    "    # Прямое распространение\n",
    "    model.forward(X)\n",
    "    \n",
    "    # Визуализация\n",
    "    fig, axes = plt.subplots(1, len(model.activations), figsize=(15, 3))\n",
    "    \n",
    "    for i, (ax, activation) in enumerate(zip(axes, model.activations)):\n",
    "        values = activation[sample_idx].reshape(-1, 1)\n",
    "        \n",
    "        im = ax.imshow(values.T, cmap='viridis', aspect='auto')\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlabel('Нейрон')\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.set_title(f'Вход\\n(размер: {values.shape[0]})')\n",
    "        elif i == len(model.activations) - 1:\n",
    "            ax.set_title(f'Выход\\n(размер: {values.shape[0]})')\n",
    "        else:\n",
    "            ax.set_title(f'Слой {i}\\n(размер: {values.shape[0]})')\n",
    "        \n",
    "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    \n",
    "    plt.suptitle(f'Активации для примера {sample_idx}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Визуализация для нескольких примеров\n",
    "for idx in [0, 5, 10]:\n",
    "    visualize_activations(nn, X_train, sample_idx=idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 10. Сравнение с разными функциями активации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация более сложных данных (концентрические окружности)\n",
    "X_circles, y_circles = make_circles(n_samples=300, noise=0.1, factor=0.5, random_state=42)\n",
    "y_circles = y_circles.reshape(-1, 1)\n",
    "\n",
    "# Визуализация\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_circles[y_circles.flatten() == 0, 0], X_circles[y_circles.flatten() == 0, 1],\n",
    "           c='red', marker='o', s=50, alpha=0.7, edgecolors='k', label='Класс 0')\n",
    "plt.scatter(X_circles[y_circles.flatten() == 1, 0], X_circles[y_circles.flatten() == 1, 1],\n",
    "           c='blue', marker='s', s=50, alpha=0.7, edgecolors='k', label='Класс 1')\n",
    "plt.xlabel('Признак 1')\n",
    "plt.ylabel('Признак 2')\n",
    "plt.title('Данные: концентрические окружности')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение сети на новых данных\n",
    "print(\"Обучение на концентрических окружностях...\\n\")\n",
    "\n",
    "nn_circles = NeuralNetworkBackprop(layer_sizes=[2, 32, 16, 8, 1])\n",
    "nn_circles.train(X_circles, y_circles, epochs=2000, learning_rate=0.1, verbose=True)\n",
    "\n",
    "# Оценка\n",
    "y_pred_circles = nn_circles.predict(X_circles)\n",
    "accuracy = np.mean(y_pred_circles == y_circles)\n",
    "print(f\"\\nТочность: {accuracy:.4f}\")\n",
    "\n",
    "# Визуализация границы решений\n",
    "plot_decision_boundary(nn_circles, X_circles, y_circles,\n",
    "                      'Граница решений для концентрических окружностей')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 11. Ключевые выводы\n",
    "\n",
    "### Алгоритм обратного распространения:\n",
    "\n",
    "**Шаг 1: Прямое распространение (Forward Pass)**\n",
    "- Вычисляем активации всех слоёв\n",
    "- Сохраняем промежуточные значения $z^{[l]}$ и $a^{[l]}$\n",
    "- Вычисляем функцию потерь $L$\n",
    "\n",
    "**Шаг 2: Обратное распространение (Backward Pass)**\n",
    "- Начинаем с выходного слоя: вычисляем $\\frac{\\partial L}{\\partial z^{[L]}}$\n",
    "- Двигаемся назад через слои, используя цепное правило\n",
    "- Для каждого слоя вычисляем: $\\frac{\\partial L}{\\partial W^{[l]}}$, $\\frac{\\partial L}{\\partial b^{[l]}}$\n",
    "\n",
    "**Шаг 3: Обновление весов (Gradient Descent)**\n",
    "$$W^{[l]} := W^{[l]} - \\alpha \\frac{\\partial L}{\\partial W^{[l]}}$$\n",
    "$$b^{[l]} := b^{[l]} - \\alpha \\frac{\\partial L}{\\partial b^{[l]}}$$\n",
    "\n",
    "### Преимущества backpropagation:\n",
    "\n",
    "| Свойство | Описание |\n",
    "|----------|----------|\n",
    "| **Эффективность** | Вычисляет все градиенты за один проход (O(n)) |\n",
    "| **Точность** | Аналитические градиенты точнее численных |\n",
    "| **Универсальность** | Работает для любой дифференцируемой архитектуры |\n",
    "| **Модульность** | Легко добавлять новые слои и операции |\n",
    "\n",
    "### Важные моменты:\n",
    "\n",
    "1. **Цепное правило** — основа backpropagation\n",
    "2. **Вычислительный граф** — удобное представление вычислений\n",
    "3. **Кэширование** — сохраняем промежуточные значения из forward pass\n",
    "4. **Gradient checking** — проверяем правильность реализации\n",
    "5. **Векторизация** — используем матричные операции для эффективности\n",
    "\n",
    "### Типичные ошибки:\n",
    "\n",
    "1. ❌ Неправильные размерности матриц\n",
    "2. ❌ Забыли сохранить промежуточные значения\n",
    "3. ❌ Неправильный порядок умножения матриц\n",
    "4. ❌ Забыли применить производную функции активации\n",
    "5. ❌ Неправильное усреднение по батчу\n",
    "\n",
    "### Оптимизации:\n",
    "\n",
    "- **Mini-batch SGD** — компромисс между скоростью и стабильностью\n",
    "- **Momentum** — сглаживание градиентов\n",
    "- **Adam** — адаптивная скорость обучения\n",
    "- **Learning rate scheduling** — уменьшение LR со временем\n",
    "\n",
    "### Практические советы:\n",
    "\n",
    "1. Всегда проверяйте градиенты численно (gradient checking)\n",
    "2. Визуализируйте кривую обучения\n",
    "3. Мониторьте норму градиентов\n",
    "4. Используйте правильную инициализацию весов\n",
    "5. Нормализуйте входные данные\n",
    "6. Начинайте с малой сети и увеличивайте при необходимости"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 12. Математическая сводка\n",
    "\n",
    "### Прямое распространение:\n",
    "\n",
    "Для слоя $l$:\n",
    "$$z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$$\n",
    "$$a^{[l]} = g^{[l]}(z^{[l]})$$\n",
    "\n",
    "где $g^{[l]}$ — функция активации слоя $l$.\n",
    "\n",
    "### Обратное распространение:\n",
    "\n",
    "**Выходной слой** (для MSE loss):\n",
    "$$\\frac{\\partial L}{\\partial z^{[L]}} = (a^{[L]} - y) \\odot g'(z^{[L]})$$\n",
    "\n",
    "**Скрытые слои** (для $l = L-1, ..., 1$):\n",
    "$$\\frac{\\partial L}{\\partial z^{[l]}} = \\left((W^{[l+1]})^T \\frac{\\partial L}{\\partial z^{[l+1]}}\\right) \\odot g'(z^{[l]})$$\n",
    "\n",
    "**Градиенты параметров:**\n",
    "$$\\frac{\\partial L}{\\partial W^{[l]}} = \\frac{1}{m} \\frac{\\partial L}{\\partial z^{[l]}} (a^{[l-1]})^T$$\n",
    "$$\\frac{\\partial L}{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial z^{[l]}_i}$$\n",
    "\n",
    "где:\n",
    "- $\\odot$ — поэлементное произведение (Hadamard product)\n",
    "- $m$ — размер батча\n",
    "- $g'$ — производная функции активации\n",
    "\n",
    "### Производные функций активации:\n",
    "\n",
    "**Sigmoid:**\n",
    "$$\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$$\n",
    "\n",
    "**Tanh:**\n",
    "$$\\tanh'(z) = 1 - \\tanh^2(z)$$\n",
    "\n",
    "**ReLU:**\n",
    "$$\\text{ReLU}'(z) = \\begin{cases} 1, & z > 0 \\\\ 0, & z \\leq 0 \\end{cases}$$\n",
    "\n",
    "### Функции потерь:\n",
    "\n",
    "**MSE (регрессия):**\n",
    "$$L = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2$$\n",
    "$$\\frac{\\partial L}{\\partial \\hat{y}} = \\frac{1}{m}(\\hat{y} - y)$$\n",
    "\n",
    "**Binary Cross-Entropy (бинарная классификация):**\n",
    "$$L = -\\frac{1}{m} \\sum_{i=1}^{m} [y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$$\n",
    "$$\\frac{\\partial L}{\\partial \\hat{y}} = \\frac{1}{m}\\left(\\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## 13. Задания для самостоятельной работы\n",
    "\n",
    "1. **Реализация:**\n",
    "   - Добавьте поддержку различных функций потерь (BCE, categorical cross-entropy)\n",
    "   - Реализуйте mini-batch gradient descent\n",
    "   - Добавьте momentum и Adam optimizer\n",
    "\n",
    "2. **Градиенты:**\n",
    "   - Реализуйте gradient checking для всех весов сети\n",
    "   - Визуализируйте распределение градиентов по слоям\n",
    "   - Исследуйте проблему затухающих/взрывающихся градиентов\n",
    "\n",
    "3. **Архитектура:**\n",
    "   - Добавьте L2-регуляризацию в backpropagation\n",
    "   - Реализуйте Dropout в forward и backward pass\n",
    "   - Попробуйте разные функции активации\n",
    "\n",
    "4. **Эксперименты:**\n",
    "   - Сравните скорость сходимости с разными learning rates\n",
    "   - Исследуйте влияние глубины сети на обучение\n",
    "   - Протестируйте на датасете MNIST\n",
    "\n",
    "5. **Визуализация:**\n",
    "   - Анимируйте процесс обучения (как меняется граница решений)\n",
    "   - Визуализируйте веса сети\n",
    "   - Постройте heatmap градиентов\n",
    "\n",
    "6. **Продвинутое:**\n",
    "   - Реализуйте автоматическое дифференцирование\n",
    "   - Создайте вычислительный граф с динамическими операциями\n",
    "   - Сравните производительность с PyTorch/TensorFlow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
