{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в RNN и LSTM\n",
    "\n",
    "В этом notebook мы познакомимся с рекуррентными нейронными сетями (RNN) и Long Short-Term Memory (LSTM) сетями, которые широко применяются для анализа последовательностей в биологии: ДНК, РНК, белковых последовательностей, временных рядов экспрессии генов и др."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Установка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Что такое RNN?\n",
    "\n",
    "**Рекуррентная нейронная сеть (RNN)** — это тип нейронной сети, специально разработанный для работы с последовательностями данных. В отличие от обычных нейронных сетей, RNN имеет \"память\" — она может учитывать предыдущие элементы последовательности при обработке текущего элемента.\n",
    "\n",
    "### Применение в биологии:\n",
    "- Предсказание структуры белков\n",
    "- Анализ последовательностей ДНК/РНК\n",
    "- Предсказание сайтов связывания транскрипционных факторов\n",
    "- Анализ временных рядов экспрессии генов\n",
    "- Классификация последовательностей (например, определение семейств белков)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Простой пример: Vanilla RNN\n",
    "\n",
    "Создадим простую RNN для классификации последовательностей ДНК."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Простая RNN модель\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # RNN слой\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Полносвязный слой для выхода\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Инициализация скрытого состояния\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Прямой проход через RNN\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # Берем выход последнего временного шага\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Пример данных: кодирование ДНК последовательностей\n",
    "\n",
    "Создадим синтетические данные для демонстрации. В реальных задачах вы будете использовать реальные последовательности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot кодирование нуклеотидов\n",
    "nucleotide_dict = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "\n",
    "def encode_sequence(seq):\n",
    "    \"\"\"Преобразует ДНК последовательность в one-hot encoding\"\"\"\n",
    "    encoding = torch.zeros(len(seq), 4)\n",
    "    for i, nucleotide in enumerate(seq):\n",
    "        if nucleotide in nucleotide_dict:\n",
    "            encoding[i, nucleotide_dict[nucleotide]] = 1\n",
    "    return encoding\n",
    "\n",
    "# Пример\n",
    "example_seq = \"ATCGATCG\"\n",
    "encoded = encode_sequence(example_seq)\n",
    "print(f\"Последовательность: {example_seq}\")\n",
    "print(f\"Размерность encoded: {encoded.shape}\")\n",
    "print(f\"\\nПервые 3 нуклеотида (one-hot):\\n{encoded[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Создание синтетического датасета\n",
    "\n",
    "Создадим задачу бинарной классификации: последовательности, начинающиеся с \"ATG\" (старт-кодон), будут иметь метку 1, остальные — 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_dna_sequence(length=50, start_with_atg=False):\n",
    "    \"\"\"Генерирует случайную ДНК последовательность\"\"\"\n",
    "    nucleotides = ['A', 'C', 'G', 'T']\n",
    "    if start_with_atg:\n",
    "        seq = 'ATG' + ''.join(random.choices(nucleotides, k=length-3))\n",
    "    else:\n",
    "        # Убедимся, что не начинается с ATG\n",
    "        seq = ''.join(random.choices(nucleotides, k=length))\n",
    "        while seq.startswith('ATG'):\n",
    "            seq = ''.join(random.choices(nucleotides, k=length))\n",
    "    return seq\n",
    "\n",
    "# Создаем датасет\n",
    "def create_dataset(n_samples=1000, seq_length=50):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for _ in range(n_samples // 2):\n",
    "        # Положительные примеры (с ATG)\n",
    "        seq = generate_dna_sequence(seq_length, start_with_atg=True)\n",
    "        sequences.append(encode_sequence(seq))\n",
    "        labels.append(1)\n",
    "        \n",
    "        # Отрицательные примеры (без ATG)\n",
    "        seq = generate_dna_sequence(seq_length, start_with_atg=False)\n",
    "        sequences.append(encode_sequence(seq))\n",
    "        labels.append(0)\n",
    "    \n",
    "    return torch.stack(sequences), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Создаем тренировочные и тестовые данные\n",
    "X_train, y_train = create_dataset(n_samples=800, seq_length=50)\n",
    "X_test, y_test = create_dataset(n_samples=200, seq_length=50)\n",
    "\n",
    "print(f\"Размер тренировочных данных: {X_train.shape}\")\n",
    "print(f\"Размер меток: {y_train.shape}\")\n",
    "print(f\"Распределение классов (train): {torch.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Обучение простой RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гиперпараметры\n",
    "input_size = 4  # 4 нуклеотида\n",
    "hidden_size = 32\n",
    "output_size = 2  # бинарная классификация\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "# Создаем модель\n",
    "model_rnn = SimpleRNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Обучение\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_rnn.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for sequences, labels in train_loader:\n",
    "        # Прямой проход\n",
    "        outputs = model_rnn(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Обратный проход\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "# График loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss (RNN)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Оценка модели RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка на тестовых данных\n",
    "model_rnn.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model_rnn(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f'Accuracy RNN на тестовых данных: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Матрица ошибок\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test.numpy(), predicted.numpy())\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix (RNN)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test.numpy(), predicted.numpy(), target_names=['No ATG', 'Has ATG']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Проблема исчезающего градиента в RNN\n",
    "\n",
    "Простые RNN страдают от **проблемы исчезающего/взрывающегося градиента** при работе с длинными последовательностями. Это затрудняет обучение сети запоминать информацию на больших расстояниях.\n",
    "\n",
    "**Решение: LSTM (Long Short-Term Memory)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. LSTM архитектура\n",
    "\n",
    "**LSTM** — это специальный тип RNN, который решает проблему долгосрочных зависимостей с помощью:\n",
    "- **Ячейки состояния (cell state)** — \"память\" сети\n",
    "- **Вентилей (gates)**:\n",
    "  - *Forget gate* — решает, что забыть из предыдущего состояния\n",
    "  - *Input gate* — решает, какую новую информацию сохранить\n",
    "  - *Output gate* — решает, что выдать на выход\n",
    "\n",
    "Эти механизмы позволяют LSTM эффективно запоминать важную информацию и забывать неважную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM модель\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM слой\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Полносвязный слой\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Инициализация скрытого состояния и cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Прямой проход через LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Берем выход последнего временного шага\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Обучение LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем LSTM модель\n",
    "model_lstm = LSTMModel(input_size, hidden_size, output_size, num_layers=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Обучение\n",
    "train_losses_lstm = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_lstm.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for sequences, labels in train_loader:\n",
    "        outputs = model_lstm(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses_lstm.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Сравнение loss между RNN и LSTM\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(train_losses, label='RNN', linewidth=2)\n",
    "plt.plot(train_losses_lstm, label='LSTM', linewidth=2)\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Оценка LSTM модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка LSTM\n",
    "model_lstm.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model_lstm(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f'Accuracy LSTM на тестовых данных: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Матрица ошибок\n",
    "cm = confusion_matrix(y_test.numpy(), predicted.numpy())\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title('Confusion Matrix (LSTM)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test.numpy(), predicted.numpy(), target_names=['No ATG', 'Has ATG']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Bidirectional LSTM (BiLSTM)\n",
    "\n",
    "**Двунаправленная LSTM** обрабатывает последовательность в обоих направлениях (слева направо и справа налево), что позволяет учитывать контекст с обеих сторон. Это особенно полезно для задач, где важна информация из будущих элементов последовательности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                           batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Выходной слой (hidden_size * 2 из-за bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # num_directions = 2 для bidirectional\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Создаем и обучаем BiLSTM\n",
    "model_bilstm = BiLSTMModel(input_size, hidden_size, output_size, num_layers=2)\n",
    "optimizer = optim.Adam(model_bilstm.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses_bilstm = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_bilstm.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for sequences, labels in train_loader:\n",
    "        outputs = model_bilstm(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses_bilstm.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Оценка BiLSTM\n",
    "model_bilstm.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model_bilstm(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f'\\nAccuracy BiLSTM на тестовых данных: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Сравнение всех моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравнение loss всех моделей\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(train_losses, label='RNN', linewidth=2)\n",
    "plt.plot(train_losses_lstm, label='LSTM', linewidth=2)\n",
    "plt.plot(train_losses_bilstm, label='BiLSTM', linewidth=2)\n",
    "plt.title('Training Loss Comparison: RNN vs LSTM vs BiLSTM')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Сравнение accuracy\n",
    "models = ['RNN', 'LSTM', 'BiLSTM']\n",
    "accuracies = []\n",
    "\n",
    "for model in [model_rnn, model_lstm, model_bilstm]:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        acc = (predicted == y_test).sum().item() / len(y_test)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, accuracies, color=['skyblue', 'lightgreen', 'coral'])\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.text(i, acc + 0.02, f'{acc*100:.2f}%', ha='center', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Практический пример: предсказание для новой последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(model, sequence_str):\n",
    "    \"\"\"Предсказывает класс для заданной последовательности\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Кодируем последовательность\n",
    "    encoded = encode_sequence(sequence_str).unsqueeze(0)  # добавляем batch dimension\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(encoded)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    \n",
    "    return predicted.item(), probabilities[0].numpy()\n",
    "\n",
    "# Тестируем на новых последовательностях\n",
    "test_sequences = [\n",
    "    \"ATGCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCG\",  # Начинается с ATG\n",
    "    \"CGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCG\",  # Не начинается с ATG\n",
    "    \"ATGAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",  # ATG + много A\n",
    "]\n",
    "\n",
    "print(\"Предсказания BiLSTM модели:\\n\")\n",
    "for seq in test_sequences:\n",
    "    pred, probs = predict_sequence(model_bilstm, seq)\n",
    "    print(f\"Последовательность: {seq[:20]}...\")\n",
    "    print(f\"  Предсказание: {'Has ATG' if pred == 1 else 'No ATG'}\")\n",
    "    print(f\"  Вероятности: No ATG={probs[0]:.4f}, Has ATG={probs[1]:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Визуализация внимания (attention) — бонус\n",
    "\n",
    "Хотя базовые LSTM не имеют встроенного механизма attention, мы можем посмотреть на активации скрытых состояний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель с доступом к скрытым состояниям\n",
    "class LSTMWithHidden(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMWithHidden, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Получаем все скрытые состояния\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        predictions = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return predictions, out\n",
    "\n",
    "# Визуализируем активации\n",
    "model_viz = LSTMWithHidden(input_size, hidden_size, output_size)\n",
    "test_seq = encode_sequence(\"ATGCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCG\").unsqueeze(0)\n",
    "\n",
    "model_viz.eval()\n",
    "with torch.no_grad():\n",
    "    _, hidden_states = model_viz(test_seq)\n",
    "\n",
    "# Визуализируем нормы скрытых состояний\n",
    "hidden_norms = torch.norm(hidden_states[0], dim=1).numpy()\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(hidden_norms, linewidth=2)\n",
    "plt.title('Hidden State Activations по позициям в последовательности')\n",
    "plt.xlabel('Позиция в последовательности')\n",
    "plt.ylabel('Норма скрытого состояния')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Heatmap активаций\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.imshow(hidden_states[0].numpy().T, aspect='auto', cmap='viridis')\n",
    "plt.colorbar(label='Activation')\n",
    "plt.title('Heatmap скрытых состояний LSTM')\n",
    "plt.xlabel('Позиция в последовательности')\n",
    "plt.ylabel('Нейрон скрытого слоя')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Дополнительные улучшения и техники\n",
    "\n",
    "### Регуляризация:\n",
    "- **Dropout** — случайное отключение нейронов для предотвращения переобучения\n",
    "- **Weight decay** (L2 регуляризация)\n",
    "- **Gradient clipping** — обрезка градиентов для стабильности обучения\n",
    "\n",
    "### Другие архитектуры:\n",
    "- **GRU** (Gated Recurrent Unit) — упрощенная версия LSTM\n",
    "- **Attention механизмы** — фокусировка на важных частях последовательности\n",
    "- **Transformer** — современная архитектура на основе self-attention (BERT, GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример LSTM с dropout\n",
    "class LSTMWithDropout(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout=0.3):\n",
    "        super(LSTMWithDropout, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "print(\"Модель с dropout создана\")\n",
    "model_dropout = LSTMWithDropout(input_size, hidden_size, output_size)\n",
    "print(model_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Реальные применения в биологии\n",
    "\n",
    "### Задачи, где RNN/LSTM особенно эффективны:\n",
    "\n",
    "1. **Предсказание вторичной структуры белков** — последовательность аминокислот → вторичная структура (спираль, лист, петля)\n",
    "\n",
    "2. **Аннотация генома** — идентификация функциональных элементов в последовательностях ДНК\n",
    "\n",
    "3. **Предсказание сайтов связывания** — где транскрипционные факторы связываются с ДНК\n",
    "\n",
    "4. **Анализ экспрессии генов** — временные ряды уровней экспрессии\n",
    "\n",
    "5. **Классификация белковых последовательностей** — определение семейств и функций\n",
    "\n",
    "6. **Предсказание модификаций РНК** — m6A, псевдоуридин и т.д.\n",
    "\n",
    "7. **Дизайн лекарств** — генерация молекул с желаемыми свойствами (используя SMILES строки)\n",
    "\n",
    "### Популярные библиотеки и инструменты:\n",
    "- **BioPython** — работа с биологическими последовательностями\n",
    "- **DeepChem** — машинное обучение для химии и биологии\n",
    "- **ProtTrans** — трансформеры для анализа белков\n",
    "- **ESM** (Evolutionary Scale Modeling) — предобученные модели для белков от Meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Задания для самостоятельной работы\n",
    "\n",
    "1. **Измените архитектуру**: попробуйте разные значения `hidden_size`, `num_layers`, добавьте dropout\n",
    "\n",
    "2. **Усложните задачу**: вместо бинарной классификации создайте задачу с несколькими классами (например, классификация типов промоторов)\n",
    "\n",
    "3. **Используйте реальные данные**: загрузите настоящие последовательности ДНК/белков из баз данных (GenBank, UniProt)\n",
    "\n",
    "4. **Реализуйте GRU**: создайте модель на основе GRU и сравните с LSTM\n",
    "\n",
    "5. **Добавьте attention**: реализуйте простой механизм attention для визуализации важных позиций\n",
    "\n",
    "6. **Transfer learning**: используйте предобученную модель (например, из ProtTrans) и дообучите на своих данных\n",
    "\n",
    "7. **Sequence-to-sequence**: реализуйте модель для предсказания структуры по последовательности (многоклассовая классификация для каждой позиции)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Полезные ресурсы\n",
    "\n",
    "### Документация:\n",
    "- [PyTorch RNN Tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)\n",
    "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "### Статьи:\n",
    "- Hochreiter & Schmidhuber (1997) \"Long Short-Term Memory\"\n",
    "- Cho et al. (2014) \"Learning Phrase Representations using RNN Encoder-Decoder\"\n",
    "\n",
    "### Биоинформатика:\n",
    "- Alipanahi et al. (2015) \"Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning\"\n",
    "- Rives et al. (2021) \"Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заключение\n",
    "\n",
    "В этом notebook мы изучили:\n",
    "- Основы RNN и их применение к биологическим последовательностям\n",
    "- Архитектуру LSTM и решение проблемы исчезающего градиента\n",
    "- Bidirectional LSTM для учета двунаправленного контекста\n",
    "- Практическую реализацию на PyTorch\n",
    "- Техники улучшения моделей (dropout, regularization)\n",
    "- Реальные применения в биоинформатике\n",
    "\n",
    "**Следующие шаги:**\n",
    "- Попробуйте применить эти методы к вашим собственным данным\n",
    "- Изучите современные архитектуры (Transformers, Attention)\n",
    "- Используйте предобученные модели для transfer learning\n",
    "- Экспериментируйте с разными типами последовательностей (ДНК, РНК, белки)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
