{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предобработка данных и Feature Engineering\n",
    "\n",
    "В этой тетрадке рассмотрим:\n",
    "1. Работу с пропущенными значениями\n",
    "2. Обработку выбросов\n",
    "3. Удаление шумов\n",
    "4. Масштабирование признаков\n",
    "5. Кодирование категориальных переменных\n",
    "6. Создание новых признаков (Feature Engineering)\n",
    "7. Отбор признаков (Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_boston, load_iris, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import stats\n",
    "\n",
    "# Настройки отображения\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Работа с пропущенными значениями\n",
    "\n",
    "### Типы пропусков:\n",
    "- **MCAR** (Missing Completely At Random): пропуски случайны и не зависят от данных\n",
    "- **MAR** (Missing At Random): пропуски зависят от наблюдаемых данных\n",
    "- **MNAR** (Missing Not At Random): пропуски зависят от ненаблюдаемых данных\n",
    "\n",
    "### Методы обработки:\n",
    "1. Удаление строк/столбцов с пропусками\n",
    "2. Заполнение константой (0, -1, \"Unknown\")\n",
    "3. Заполнение статистикой (mean, median, mode)\n",
    "4. Заполнение с помощью модели (KNN, регрессия)\n",
    "5. Индикатор пропусков (создание дополнительного признака)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание датасета с пропусками\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'feature_1': np.random.randn(100),\n",
    "    'feature_2': np.random.randn(100),\n",
    "    'feature_3': np.random.randn(100),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 100)\n",
    "})\n",
    "\n",
    "# Внесение пропусков\n",
    "missing_indices_1 = np.random.choice(df.index, 15, replace=False)\n",
    "missing_indices_2 = np.random.choice(df.index, 20, replace=False)\n",
    "missing_indices_cat = np.random.choice(df.index, 10, replace=False)\n",
    "\n",
    "df.loc[missing_indices_1, 'feature_1'] = np.nan\n",
    "df.loc[missing_indices_2, 'feature_2'] = np.nan\n",
    "df.loc[missing_indices_cat, 'category'] = np.nan\n",
    "\n",
    "print(\"Исходные данные с пропусками:\")\n",
    "print(df.head(10))\n",
    "print(\"\\nИнформация о пропусках:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nПроцент пропусков: {df.isnull().sum().sum() / (df.shape[0] * df.shape[1]) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация пропусков\n",
    "import missingno as msno\n",
    "\n",
    "# Если библиотека не установлена, можно использовать альтернативный метод\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
    "plt.title('Визуализация пропущенных значений')\n",
    "plt.xlabel('Признаки')\n",
    "plt.ylabel('Наблюдения')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод 1: Удаление строк с пропусками\n",
    "df_dropped = df.dropna()\n",
    "print(f\"После удаления строк: {df_dropped.shape[0]} строк (было {df.shape[0]})\")\n",
    "\n",
    "# Метод 2: Заполнение средним/медианой/модой\n",
    "df_filled_mean = df.copy()\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "df_filled_mean[['feature_1', 'feature_2', 'feature_3']] = imputer_mean.fit_transform(\n",
    "    df_filled_mean[['feature_1', 'feature_2', 'feature_3']])\n",
    "\n",
    "imputer_mode = SimpleImputer(strategy='most_frequent')\n",
    "df_filled_mean[['category']] = imputer_mode.fit_transform(df_filled_mean[['category']])\n",
    "\n",
    "print(f\"\\nПосле заполнения средним/модой: {df_filled_mean.isnull().sum().sum()} пропусков\")\n",
    "\n",
    "# Метод 3: KNN Imputer\n",
    "df_knn = df.copy()\n",
    "# Преобразуем категориальную переменную для KNN\n",
    "df_knn['category_encoded'] = LabelEncoder().fit_transform(df_knn['category'].fillna('Missing'))\n",
    "df_knn_temp = df_knn[['feature_1', 'feature_2', 'feature_3', 'category_encoded']]\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df_knn_filled = pd.DataFrame(\n",
    "    knn_imputer.fit_transform(df_knn_temp),\n",
    "    columns=df_knn_temp.columns\n",
    ")\n",
    "\n",
    "print(f\"После KNN Imputer: {df_knn_filled.isnull().sum().sum()} пропусков\")\n",
    "\n",
    "# Метод 4: Индикатор пропусков\n",
    "df_indicator = df.copy()\n",
    "df_indicator['feature_1_missing'] = df_indicator['feature_1'].isnull().astype(int)\n",
    "df_indicator['feature_2_missing'] = df_indicator['feature_2'].isnull().astype(int)\n",
    "df_indicator = df_indicator.fillna(df_indicator.mean(numeric_only=True))\n",
    "\n",
    "print(f\"\\nС индикаторами пропусков: {df_indicator.shape[1]} признаков (было {df.shape[1]})\")\n",
    "print(df_indicator.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Обработка выбросов (Outliers)\n",
    "\n",
    "### Методы обнаружения:\n",
    "1. **IQR (Interquartile Range)**: значения вне [Q1 - 1.5×IQR, Q3 + 1.5×IQR]\n",
    "2. **Z-score**: |z| > 3 (более 3 стандартных отклонений)\n",
    "3. **Isolation Forest**: алгоритм для обнаружения аномалий\n",
    "4. **DBSCAN**: кластеризация с выделением шума\n",
    "\n",
    "### Методы обработки:\n",
    "1. Удаление выбросов\n",
    "2. Ограничение (capping/winsorization)\n",
    "3. Трансформация (log, sqrt)\n",
    "4. Отдельная обработка как специальная категория"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание данных с выбросами\n",
    "np.random.seed(42)\n",
    "normal_data = np.random.randn(200)\n",
    "outliers = np.random.uniform(5, 10, 20)\n",
    "data_with_outliers = np.concatenate([normal_data, outliers])\n",
    "df_outliers = pd.DataFrame({'value': data_with_outliers})\n",
    "\n",
    "# Визуализация\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Гистограмма\n",
    "axes[0].hist(df_outliers['value'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Гистограмма данных с выбросами')\n",
    "axes[0].set_xlabel('Значение')\n",
    "axes[0].set_ylabel('Частота')\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(df_outliers['value'])\n",
    "axes[1].set_title('Box Plot')\n",
    "axes[1].set_ylabel('Значение')\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(df_outliers['value'], dist=\"norm\", plot=axes[2])\n",
    "axes[2].set_title('Q-Q Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Статистика данных:\")\n",
    "print(df_outliers['value'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод 1: IQR метод\n",
    "Q1 = df_outliers['value'].quantile(0.25)\n",
    "Q3 = df_outliers['value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers_iqr = df_outliers[(df_outliers['value'] < lower_bound) | (df_outliers['value'] > upper_bound)]\n",
    "print(f\"IQR метод: обнаружено {len(outliers_iqr)} выбросов\")\n",
    "print(f\"Границы: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "\n",
    "# Удаление выбросов\n",
    "df_no_outliers_iqr = df_outliers[(df_outliers['value'] >= lower_bound) & (df_outliers['value'] <= upper_bound)]\n",
    "\n",
    "# Ограничение выбросов (capping)\n",
    "df_capped = df_outliers.copy()\n",
    "df_capped['value'] = df_capped['value'].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "print(f\"После удаления: {len(df_no_outliers_iqr)} наблюдений\")\n",
    "print(f\"После ограничения: {len(df_capped)} наблюдений\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод 2: Z-score\n",
    "z_scores = np.abs(stats.zscore(df_outliers['value']))\n",
    "threshold = 3\n",
    "outliers_zscore = df_outliers[z_scores > threshold]\n",
    "print(f\"Z-score метод (|z| > {threshold}): обнаружено {len(outliers_zscore)} выбросов\")\n",
    "\n",
    "df_no_outliers_zscore = df_outliers[z_scores <= threshold]\n",
    "print(f\"После удаления: {len(df_no_outliers_zscore)} наблюдений\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод 3: Isolation Forest\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "outliers_if = iso_forest.fit_predict(df_outliers[['value']])\n",
    "df_outliers['outlier_if'] = outliers_if\n",
    "\n",
    "print(f\"Isolation Forest: обнаружено {sum(outliers_if == -1)} выбросов\")\n",
    "\n",
    "df_no_outliers_if = df_outliers[df_outliers['outlier_if'] == 1]\n",
    "print(f\"После удаления: {len(df_no_outliers_if)} наблюдений\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравнение методов визуально\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Исходные данные\n",
    "axes[0, 0].hist(df_outliers['value'], bins=30, edgecolor='black', alpha=0.7, color='red')\n",
    "axes[0, 0].axvline(lower_bound, color='green', linestyle='--', label='IQR bounds')\n",
    "axes[0, 0].axvline(upper_bound, color='green', linestyle='--')\n",
    "axes[0, 0].set_title(f'Исходные данные (n={len(df_outliers)})')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# После IQR\n",
    "axes[0, 1].hist(df_no_outliers_iqr['value'], bins=30, edgecolor='black', alpha=0.7, color='blue')\n",
    "axes[0, 1].set_title(f'После IQR (n={len(df_no_outliers_iqr)})')\n",
    "\n",
    "# После Z-score\n",
    "axes[1, 0].hist(df_no_outliers_zscore['value'], bins=30, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1, 0].set_title(f'После Z-score (n={len(df_no_outliers_zscore)})')\n",
    "\n",
    "# После Isolation Forest\n",
    "axes[1, 1].hist(df_no_outliers_if['value'], bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1, 1].set_title(f'После Isolation Forest (n={len(df_no_outliers_if)})')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('Значение')\n",
    "    ax.set_ylabel('Частота')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Удаление шумов и сглаживание\n",
    "\n",
    "### Методы:\n",
    "1. **Скользящее среднее** (Moving Average)\n",
    "2. **Медианный фильтр**\n",
    "3. **Гауссовское сглаживание**\n",
    "4. **Фильтр Савицкого-Голея**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d, median_filter\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# Создание зашумленных данных\n",
    "np.random.seed(42)\n",
    "x = np.linspace(0, 10, 200)\n",
    "y_true = np.sin(x) + 0.5 * np.sin(3 * x)\n",
    "noise = np.random.normal(0, 0.3, len(x))\n",
    "y_noisy = y_true + noise\n",
    "\n",
    "# Применение различных методов\n",
    "# Скользящее среднее\n",
    "window_size = 10\n",
    "y_moving_avg = pd.Series(y_noisy).rolling(window=window_size, center=True).mean()\n",
    "\n",
    "# Медианный фильтр\n",
    "y_median = median_filter(y_noisy, size=10)\n",
    "\n",
    "# Гауссовское сглаживание\n",
    "y_gaussian = gaussian_filter1d(y_noisy, sigma=2)\n",
    "\n",
    "# Фильтр Савицкого-Голея\n",
    "y_savgol = savgol_filter(y_noisy, window_length=21, polyorder=3)\n",
    "\n",
    "# Визуализация\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "axes[0, 0].plot(x, y_true, 'g-', linewidth=2, label='Истинный сигнал')\n",
    "axes[0, 0].plot(x, y_noisy, 'r.', alpha=0.3, label='Зашумленный сигнал')\n",
    "axes[0, 0].set_title('Исходные данные')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(x, y_true, 'g-', linewidth=2, label='Истинный')\n",
    "axes[0, 1].plot(x, y_moving_avg, 'b-', linewidth=2, label='Скользящее среднее')\n",
    "axes[0, 1].set_title('Скользящее среднее')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 2].plot(x, y_true, 'g-', linewidth=2, label='Истинный')\n",
    "axes[0, 2].plot(x, y_median, 'm-', linewidth=2, label='Медианный фильтр')\n",
    "axes[0, 2].set_title('Медианный фильтр')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(x, y_true, 'g-', linewidth=2, label='Истинный')\n",
    "axes[1, 0].plot(x, y_gaussian, 'c-', linewidth=2, label='Гауссовское')\n",
    "axes[1, 0].set_title('Гауссовское сглаживание')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(x, y_true, 'g-', linewidth=2, label='Истинный')\n",
    "axes[1, 1].plot(x, y_savgol, 'orange', linewidth=2, label='Савицкий-Голей')\n",
    "axes[1, 1].set_title('Фильтр Савицкого-Голея')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Сравнение ошибок\n",
    "methods = ['Moving Avg', 'Median', 'Gaussian', 'Savgol']\n",
    "y_filtered = [y_moving_avg.fillna(method='bfill').fillna(method='ffill'), \n",
    "              y_median, y_gaussian, y_savgol]\n",
    "mse_errors = [np.mean((y_true - y_f)**2) for y_f in y_filtered]\n",
    "\n",
    "axes[1, 2].bar(methods, mse_errors, color=['blue', 'magenta', 'cyan', 'orange'])\n",
    "axes[1, 2].set_title('Сравнение MSE ошибок')\n",
    "axes[1, 2].set_ylabel('MSE')\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"MSE для каждого метода:\")\n",
    "for method, mse in zip(methods, mse_errors):\n",
    "    print(f\"{method}: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Масштабирование признаков\n",
    "\n",
    "### Зачем нужно масштабирование?\n",
    "- Алгоритмы на основе расстояний (KNN, SVM, K-means) чувствительны к масштабу\n",
    "- Градиентный спуск сходится быстрее\n",
    "- Регуляризация работает корректно\n",
    "\n",
    "### Методы:\n",
    "1. **StandardScaler**: $(x - \\mu) / \\sigma$ (mean=0, std=1)\n",
    "2. **MinMaxScaler**: $(x - x_{min}) / (x_{max} - x_{min})$ (диапазон [0, 1])\n",
    "3. **RobustScaler**: использует медиану и IQR (устойчив к выбросам)\n",
    "4. **MaxAbsScaler**: $x / |x_{max}|$ (диапазон [-1, 1])\n",
    "5. **Normalizer**: нормализация по строкам (||x|| = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler, Normalizer\n",
    "\n",
    "# Создание данных с разными масштабами\n",
    "np.random.seed(42)\n",
    "df_scale = pd.DataFrame({\n",
    "    'feature_small': np.random.uniform(0, 1, 100),\n",
    "    'feature_medium': np.random.uniform(0, 100, 100),\n",
    "    'feature_large': np.random.uniform(0, 10000, 100)\n",
    "})\n",
    "\n",
    "print(\"Исходные данные:\")\n",
    "print(df_scale.describe())\n",
    "\n",
    "# Применение разных методов масштабирования\n",
    "scalers = {\n",
    "    'Standard': StandardScaler(),\n",
    "    'MinMax': MinMaxScaler(),\n",
    "    'Robust': RobustScaler(),\n",
    "    'MaxAbs': MaxAbsScaler()\n",
    "}\n",
    "\n",
    "scaled_data = {}\n",
    "for name, scaler in scalers.items():\n",
    "    scaled_data[name] = pd.DataFrame(\n",
    "        scaler.fit_transform(df_scale),\n",
    "        columns=df_scale.columns\n",
    "    )\n",
    "\n",
    "# Визуализация\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Исходные данные\n",
    "df_scale.boxplot(ax=axes[0])\n",
    "axes[0].set_title('Исходные данные')\n",
    "axes[0].set_ylabel('Значение')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Масштабированные данные\n",
    "for idx, (name, data) in enumerate(scaled_data.items(), 1):\n",
    "    data.boxplot(ax=axes[idx])\n",
    "    axes[idx].set_title(f'{name} Scaler')\n",
    "    axes[idx].set_ylabel('Значение')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    print(f\"\\n{name} Scaler:\")\n",
    "    print(data.describe())\n",
    "\n",
    "# Сравнение распределений для одного признака\n",
    "axes[5].hist(df_scale['feature_medium'], bins=20, alpha=0.5, label='Original', edgecolor='black')\n",
    "axes[5].hist(scaled_data['Standard']['feature_medium'], bins=20, alpha=0.5, label='Standard', edgecolor='black')\n",
    "axes[5].hist(scaled_data['MinMax']['feature_medium'], bins=20, alpha=0.5, label='MinMax', edgecolor='black')\n",
    "axes[5].set_title('Сравнение распределений (feature_medium)')\n",
    "axes[5].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Кодирование категориальных переменных\n",
    "\n",
    "### Методы:\n",
    "1. **Label Encoding**: присвоение числовых меток (0, 1, 2, ...)\n",
    "2. **One-Hot Encoding**: бинарные столбцы для каждой категории\n",
    "3. **Target Encoding**: замена на среднее значение целевой переменной\n",
    "4. **Frequency Encoding**: замена на частоту появления\n",
    "5. **Binary Encoding**: бинарное представление меток\n",
    "6. **Ordinal Encoding**: для упорядоченных категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Создание данных с категориальными переменными\n",
    "np.random.seed(42)\n",
    "df_cat = pd.DataFrame({\n",
    "    'color': np.random.choice(['red', 'blue', 'green', 'yellow'], 100),\n",
    "    'size': np.random.choice(['S', 'M', 'L', 'XL'], 100),\n",
    "    'quality': np.random.choice(['low', 'medium', 'high'], 100),\n",
    "    'target': np.random.randint(0, 2, 100)\n",
    "})\n",
    "\n",
    "print(\"Исходные данные:\")\n",
    "print(df_cat.head(10))\n",
    "print(\"\\nКоличество уникальных значений:\")\n",
    "print(df_cat.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод 1: Label Encoding\n",
    "df_label = df_cat.copy()\n",
    "le = LabelEncoder()\n",
    "df_label['color_encoded'] = le.fit_transform(df_label['color'])\n",
    "\n",
    "print(\"Label Encoding:\")\n",
    "print(df_label[['color', 'color_encoded']].head())\n",
    "print(\"\\nМаппинг:\", dict(zip(le.classes_, le.transform(le.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод 2: One-Hot Encoding\n",
    "df_onehot = pd.get_dummies(df_cat, columns=['color'], prefix='color')\n",
    "\n",
    "print(\"One-Hot Encoding:\")\n",
    "print(df_onehot.head())\n",
    "print(f\"\\nКоличество признаков: {df_onehot.shape[1]} (было {df_cat.shape[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод 3: Target Encoding\n",
    "df_target = df_cat.copy()\n",
    "target_means = df_target.groupby('color')['target'].mean()\n",
    "df_target['color_target_encoded'] = df_target['color'].map(target_means)\n",
    "\n",
    "print(\"Target Encoding:\")\n",
    "print(df_target[['color', 'color_target_encoded', 'target']].head(10))\n",
    "print(\"\\nМаппинг (среднее значение target):\")\n",
    "print(target_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод 4: Frequency Encoding\n",
    "df_freq = df_cat.copy()\n",
    "freq = df_freq['color'].value_counts(normalize=True)\n",
    "df_freq['color_freq_encoded'] = df_freq['color'].map(freq)\n",
    "\n",
    "print(\"Frequency Encoding:\")\n",
    "print(df_freq[['color', 'color_freq_encoded']].head(10))\n",
    "print(\"\\nЧастоты:\")\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод 5: Ordinal Encoding (для упорядоченных категорий)\n",
    "df_ordinal = df_cat.copy()\n",
    "\n",
    "# Определяем порядок\n",
    "size_order = ['S', 'M', 'L', 'XL']\n",
    "quality_order = ['low', 'medium', 'high']\n",
    "\n",
    "# Создаем маппинг\n",
    "size_mapping = {size: idx for idx, size in enumerate(size_order)}\n",
    "quality_mapping = {quality: idx for idx, quality in enumerate(quality_order)}\n",
    "\n",
    "df_ordinal['size_encoded'] = df_ordinal['size'].map(size_mapping)\n",
    "df_ordinal['quality_encoded'] = df_ordinal['quality'].map(quality_mapping)\n",
    "\n",
    "print(\"Ordinal Encoding:\")\n",
    "print(df_ordinal[['size', 'size_encoded', 'quality', 'quality_encoded']].head())\n",
    "print(\"\\nSize mapping:\", size_mapping)\n",
    "print(\"Quality mapping:\", quality_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering - создание новых признаков\n",
    "\n",
    "### Типы создаваемых признаков:\n",
    "1. **Полиномиальные признаки**: $x_1^2, x_1 \\cdot x_2$, etc.\n",
    "2. **Математические трансформации**: log, sqrt, exp\n",
    "3. **Агрегации**: sum, mean, max, min\n",
    "4. **Временные признаки**: год, месяц, день недели\n",
    "5. **Статистические признаки**: rolling mean, std\n",
    "6. **Доменные признаки**: специфичные для задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Создание исходных данных\n",
    "np.random.seed(42)\n",
    "df_fe = pd.DataFrame({\n",
    "    'x1': np.random.randn(100),\n",
    "    'x2': np.random.randn(100),\n",
    "    'x3': np.random.uniform(1, 10, 100)\n",
    "})\n",
    "\n",
    "print(\"Исходные признаки:\")\n",
    "print(df_fe.head())\n",
    "print(f\"Количество признаков: {df_fe.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Полиномиальные признаки\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(df_fe)\n",
    "poly_features = poly.get_feature_names_out(df_fe.columns)\n",
    "\n",
    "df_poly = pd.DataFrame(X_poly, columns=poly_features)\n",
    "print(\"Полиномиальные признаки (degree=2):\")\n",
    "print(df_poly.head())\n",
    "print(f\"\\nКоличество признаков: {df_poly.shape[1]} (было {df_fe.shape[1]})\")\n",
    "print(\"Новые признаки:\", list(poly_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Математические трансформации\n",
    "df_math = df_fe.copy()\n",
    "\n",
    "# Логарифм (только для положительных значений)\n",
    "df_math['x3_log'] = np.log(df_math['x3'])\n",
    "\n",
    "# Квадратный корень (для положительных значений)\n",
    "df_math['x3_sqrt'] = np.sqrt(df_math['x3'])\n",
    "\n",
    "# Квадрат\n",
    "df_math['x1_squared'] = df_math['x1'] ** 2\n",
    "\n",
    "# Экспонента\n",
    "df_math['x1_exp'] = np.exp(df_math['x1'])\n",
    "\n",
    "# Взаимодействия\n",
    "df_math['x1_x2_interaction'] = df_math['x1'] * df_math['x2']\n",
    "df_math['x1_x2_ratio'] = df_math['x1'] / (df_math['x2'] + 1e-5)  # избегаем деления на 0\n",
    "\n",
    "print(\"Математические трансформации:\")\n",
    "print(df_math.head())\n",
    "print(f\"\\nКоличество признаков: {df_math.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Агрегированные признаки\n",
    "df_agg = df_fe.copy()\n",
    "\n",
    "# Сумма всех признаков\n",
    "df_agg['sum_all'] = df_agg.sum(axis=1)\n",
    "\n",
    "# Среднее значение\n",
    "df_agg['mean_all'] = df_agg.mean(axis=1)\n",
    "\n",
    "# Максимум и минимум\n",
    "df_agg['max_all'] = df_agg.max(axis=1)\n",
    "df_agg['min_all'] = df_agg.min(axis=1)\n",
    "\n",
    "# Разница между max и min\n",
    "df_agg['range_all'] = df_agg['max_all'] - df_agg['min_all']\n",
    "\n",
    "# Стандартное отклонение\n",
    "df_agg['std_all'] = df_agg[['x1', 'x2', 'x3']].std(axis=1)\n",
    "\n",
    "print(\"Агрегированные признаки:\")\n",
    "print(df_agg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Временные признаки\n",
    "dates = pd.date_range('2024-01-01', periods=100, freq='D')\n",
    "df_time = pd.DataFrame({'date': dates})\n",
    "\n",
    "# Извлечение компонентов даты\n",
    "df_time['year'] = df_time['date'].dt.year\n",
    "df_time['month'] = df_time['date'].dt.month\n",
    "df_time['day'] = df_time['date'].dt.day\n",
    "df_time['day_of_week'] = df_time['date'].dt.dayofweek\n",
    "df_time['week_of_year'] = df_time['date'].dt.isocalendar().week\n",
    "df_time['is_weekend'] = df_time['day_of_week'].isin([5, 6]).astype(int)\n",
    "df_time['quarter'] = df_time['date'].dt.quarter\n",
    "\n",
    "# Циклические признаки (для периодических величин)\n",
    "df_time['month_sin'] = np.sin(2 * np.pi * df_time['month'] / 12)\n",
    "df_time['month_cos'] = np.cos(2 * np.pi * df_time['month'] / 12)\n",
    "df_time['day_sin'] = np.sin(2 * np.pi * df_time['day_of_week'] / 7)\n",
    "df_time['day_cos'] = np.cos(2 * np.pi * df_time['day_of_week'] / 7)\n",
    "\n",
    "print(\"Временные признаки:\")\n",
    "print(df_time.head(10))\n",
    "\n",
    "# Визуализация циклических признаков\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax[0].scatter(df_time['month_sin'], df_time['month_cos'], c=df_time['month'], cmap='hsv')\n",
    "ax[0].set_xlabel('month_sin')\n",
    "ax[0].set_ylabel('month_cos')\n",
    "ax[0].set_title('Циклическое представление месяцев')\n",
    "ax[0].grid(True, alpha=0.3)\n",
    "\n",
    "ax[1].scatter(df_time['day_sin'], df_time['day_cos'], c=df_time['day_of_week'], cmap='viridis')\n",
    "ax[1].set_xlabel('day_sin')\n",
    "ax[1].set_ylabel('day_cos')\n",
    "ax[1].set_title('Циклическое представление дней недели')\n",
    "ax[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Selection - отбор признаков\n",
    "\n",
    "### Зачем нужен отбор признаков?\n",
    "- Уменьшение переобучения\n",
    "- Улучшение производительности модели\n",
    "- Снижение вычислительной сложности\n",
    "- Улучшение интерпретируемости\n",
    "\n",
    "### Методы отбора:\n",
    "1. **Filter Methods**: статистические тесты (корреляция, chi-square, mutual information)\n",
    "2. **Wrapper Methods**: RFE, Forward/Backward selection\n",
    "3. **Embedded Methods**: Lasso, Random Forest feature importance\n",
    "4. **Variance Threshold**: удаление признаков с низкой дисперсией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, SelectFromModel, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Создание датасета для демонстрации\n",
    "X, y = make_classification(n_samples=1000, n_features=30, n_informative=15, \n",
    "                           n_redundant=10, n_repeated=5, random_state=42)\n",
    "feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "df_fs = pd.DataFrame(X, columns=feature_names)\n",
    "df_fs['target'] = y\n",
    "\n",
    "print(f\"Исходный датасет: {X.shape[0]} образцов, {X.shape[1]} признаков\")\n",
    "print(f\"Распределение классов: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Variance Threshold - удаление признаков с низкой дисперсией\n",
    "selector_var = VarianceThreshold(threshold=0.1)\n",
    "X_var = selector_var.fit_transform(X)\n",
    "\n",
    "print(f\"Variance Threshold:\")\n",
    "print(f\"Признаков после фильтрации: {X_var.shape[1]} (удалено {X.shape[1] - X_var.shape[1]})\")\n",
    "\n",
    "# Визуализация дисперсии признаков\n",
    "variances = X.var(axis=0)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(len(variances)), variances)\n",
    "plt.axhline(y=0.1, color='r', linestyle='--', label='Threshold')\n",
    "plt.xlabel('Индекс признака')\n",
    "plt.ylabel('Дисперсия')\n",
    "plt.title('Дисперсия признаков')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Корреляционный анализ\n",
    "# Корреляция между признаками\n",
    "correlation_matrix = df_fs.drop('target', axis=1).corr()\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, annot=False, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Матрица корреляций признаков')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Поиск высококоррелированных пар\n",
    "threshold_corr = 0.8\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold_corr:\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "print(f\"\\nВысококоррелированные пары (|r| > {threshold_corr}):\")\n",
    "for feat1, feat2, corr in high_corr_pairs[:10]:\n",
    "    print(f\"{feat1} - {feat2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Univariate Feature Selection - SelectKBest\n",
    "# F-test (ANOVA)\n",
    "k_best = 15\n",
    "selector_ftest = SelectKBest(score_func=f_classif, k=k_best)\n",
    "X_ftest = selector_ftest.fit_transform(X, y)\n",
    "\n",
    "# Получение оценок\n",
    "ftest_scores = selector_ftest.scores_\n",
    "ftest_features = selector_ftest.get_support(indices=True)\n",
    "\n",
    "print(f\"SelectKBest (F-test):\")\n",
    "print(f\"Выбрано {k_best} лучших признаков\")\n",
    "print(f\"Индексы: {ftest_features}\")\n",
    "\n",
    "# Визуализация оценок\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(len(ftest_scores)), ftest_scores)\n",
    "plt.xlabel('Индекс признака')\n",
    "plt.ylabel('F-score')\n",
    "plt.title('F-test scores для признаков')\n",
    "\n",
    "# Mutual Information\n",
    "mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(len(mi_scores)), mi_scores)\n",
    "plt.xlabel('Индекс признака')\n",
    "plt.ylabel('MI score')\n",
    "plt.title('Mutual Information scores')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Recursive Feature Elimination (RFE)\n",
    "estimator = LogisticRegression(max_iter=1000, random_state=42)\n",
    "selector_rfe = RFE(estimator, n_features_to_select=15, step=1)\n",
    "X_rfe = selector_rfe.fit_transform(X, y)\n",
    "\n",
    "print(f\"RFE (Recursive Feature Elimination):\")\n",
    "print(f\"Выбрано признаков: {X_rfe.shape[1]}\")\n",
    "print(f\"Ранги признаков:\")\n",
    "for idx, (rank, selected) in enumerate(zip(selector_rfe.ranking_, selector_rfe.support_)):\n",
    "    if selected:\n",
    "        print(f\"Feature {idx}: rank {rank} (selected)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Feature Importance из Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Получение важности признаков\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature Importance (Random Forest):\")\n",
    "print(\"\\nТоп-15 самых важных признаков:\")\n",
    "for i in range(15):\n",
    "    print(f\"{i+1}. feature_{indices[i]}: {importances[indices[i]]:.4f}\")\n",
    "\n",
    "# Визуализация\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(X.shape[1]), importances[indices])\n",
    "plt.xlabel('Индекс признака (отсортировано)')\n",
    "plt.ylabel('Важность')\n",
    "plt.title('Feature Importance из Random Forest')\n",
    "plt.show()\n",
    "\n",
    "# Отбор признаков по порогу важности\n",
    "selector_rf = SelectFromModel(rf, threshold='median', prefit=True)\n",
    "X_rf = selector_rf.transform(X)\n",
    "print(f\"\\nПризнаков после SelectFromModel: {X_rf.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. L1-регуляризация (Lasso) для отбора признаков\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.01, random_state=42)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Признаки с ненулевыми коэффициентами\n",
    "lasso_coef = lasso.coef_\n",
    "selected_features_lasso = np.where(lasso_coef != 0)[0]\n",
    "\n",
    "print(f\"L1-регуляризация (Lasso):\")\n",
    "print(f\"Выбрано признаков: {len(selected_features_lasso)}\")\n",
    "print(f\"Индексы: {selected_features_lasso}\")\n",
    "\n",
    "# Визуализация коэффициентов\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(range(len(lasso_coef)), np.abs(lasso_coef))\n",
    "plt.xlabel('Индекс признака')\n",
    "plt.ylabel('|Коэффициент|')\n",
    "plt.title('Абсолютные значения коэффициентов Lasso')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравнение всех методов\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "methods = {\n",
    "    'Все признаки': X,\n",
    "    'Variance Threshold': X_var,\n",
    "    'SelectKBest (F-test)': X_ftest,\n",
    "    'RFE': X_rfe,\n",
    "    'Random Forest': X_rf,\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, X_selected in methods.items():\n",
    "    # Кросс-валидация\n",
    "    clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    scores = cross_val_score(clf, X_selected, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    results.append({\n",
    "        'Method': name,\n",
    "        'Features': X_selected.shape[1],\n",
    "        'Mean Accuracy': scores.mean(),\n",
    "        'Std': scores.std()\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nСравнение методов отбора признаков:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Визуализация\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# График точности\n",
    "axes[0].barh(results_df['Method'], results_df['Mean Accuracy'], \n",
    "             xerr=results_df['Std'], capsize=5, color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Mean Accuracy (CV=5)')\n",
    "axes[0].set_title('Сравнение точности моделей')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# График количества признаков vs точность\n",
    "axes[1].scatter(results_df['Features'], results_df['Mean Accuracy'], s=200, alpha=0.6)\n",
    "for idx, row in results_df.iterrows():\n",
    "    axes[1].annotate(row['Method'], \n",
    "                     (row['Features'], row['Mean Accuracy']),\n",
    "                     fontsize=8, ha='right')\n",
    "axes[1].set_xlabel('Количество признаков')\n",
    "axes[1].set_ylabel('Mean Accuracy')\n",
    "axes[1].set_title('Количество признаков vs Точность')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Резюме\n",
    "\n",
    "### Основные этапы предобработки:\n",
    "\n",
    "1. **Работа с пропусками**:\n",
    "   - Анализ паттернов пропусков (MCAR, MAR, MNAR)\n",
    "   - Выбор стратегии: удаление, заполнение или создание индикаторов\n",
    "   - KNN Imputer для сложных зависимостей\n",
    "\n",
    "2. **Обработка выбросов**:\n",
    "   - IQR для симметричных распределений\n",
    "   - Z-score для нормальных распределений\n",
    "   - Isolation Forest для многомерных данных\n",
    "   - Выбор между удалением и ограничением\n",
    "\n",
    "3. **Удаление шумов**:\n",
    "   - Скользящее среднее для временных рядов\n",
    "   - Медианный фильтр для сохранения резких переходов\n",
    "   - Гауссовское сглаживание для общего случая\n",
    "   - Савицкий-Голей для научных данных\n",
    "\n",
    "4. **Масштабирование**:\n",
    "   - StandardScaler для нормального распределения\n",
    "   - MinMaxScaler для ограниченного диапазона\n",
    "   - RobustScaler при наличии выбросов\n",
    "\n",
    "5. **Кодирование категорий**:\n",
    "   - One-Hot для номинальных признаков\n",
    "   - Ordinal для упорядоченных категорий\n",
    "   - Target Encoding с осторожностью (risk of overfitting)\n",
    "\n",
    "6. **Feature Engineering**:\n",
    "   - Полиномиальные признаки для нелинейностей\n",
    "   - Математические трансформации для нормализации\n",
    "   - Агрегации для создания обобщенных признаков\n",
    "   - Циклические признаки для периодических данных\n",
    "\n",
    "7. **Feature Selection**:\n",
    "   - Filter methods для быстрой первичной фильтрации\n",
    "   - Wrapper methods для оптимального набора\n",
    "   - Embedded methods для интеграции с обучением\n",
    "   - Баланс между количеством признаков и качеством модели\n",
    "\n",
    "### Рекомендации:\n",
    "- Всегда начинайте с EDA (разведочного анализа)\n",
    "- Документируйте все преобразования\n",
    "- Используйте pipeline для воспроизводимости\n",
    "- Применяйте одинаковые преобразования к train/test\n",
    "- Валидируйте результаты на тестовой выборке"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
