{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Глубокие нейронные сети\n",
    "\n",
    "В этой тетрадке мы рассмотрим:\n",
    "- Архитектуру глубоких нейронных сетей\n",
    "- Проблему затухающего и взрывающегося градиента\n",
    "- Методы борьбы с переобучением\n",
    "- Современные архитектуры (ResNet)\n",
    "- Практические примеры и визуализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Архитектура глубоких нейронных сетей\n",
    "\n",
    "### Что такое глубокая нейронная сеть?\n",
    "\n",
    "**Глубокая нейронная сеть (DNN)** — это нейронная сеть с несколькими скрытыми слоями.\n",
    "\n",
    "```\n",
    "Входной слой → Скрытый слой 1 → Скрытый слой 2 → ... → Скрытый слой N → Выходной слой\n",
    "```\n",
    "\n",
    "### Почему глубина важна?\n",
    "\n",
    "- **Иерархическое представление**: Каждый слой извлекает признаки разного уровня абстракции\n",
    "- **Композициональность**: Глубокие сети могут представлять сложные функции через композицию простых\n",
    "- **Эффективность**: Глубокие сети могут быть экспоненциально эффективнее мелких для некоторых задач\n",
    "\n",
    "### Основные компоненты:\n",
    "\n",
    "1. **Слои (Layers)**: Fully connected, convolutional, recurrent, etc.\n",
    "2. **Функции активации**: ReLU, Sigmoid, Tanh, Leaky ReLU, ELU, etc.\n",
    "3. **Инициализация весов**: Xavier, He, etc.\n",
    "4. **Оптимизаторы**: SGD, Adam, RMSprop, etc.\n",
    "5. **Регуляризация**: Dropout, L2, Batch Normalization, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Функции активации\n",
    "\n",
    "Функции активации добавляют нелинейность в сеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activations:\n",
    "    \"\"\"Различные функции активации и их производные\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        s = Activations.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(z):\n",
    "        return 1 - np.tanh(z) ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def leaky_relu(z, alpha=0.01):\n",
    "        return np.where(z > 0, z, alpha * z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def leaky_relu_derivative(z, alpha=0.01):\n",
    "        return np.where(z > 0, 1, alpha)\n",
    "    \n",
    "    @staticmethod\n",
    "    def elu(z, alpha=1.0):\n",
    "        return np.where(z > 0, z, alpha * (np.exp(z) - 1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def elu_derivative(z, alpha=1.0):\n",
    "        return np.where(z > 0, 1, alpha * np.exp(z))\n",
    "\n",
    "\n",
    "# Визуализация функций активации\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Функции активации и их производные', fontsize=16)\n",
    "\n",
    "# Sigmoid\n",
    "axes[0, 0].plot(x, Activations.sigmoid(x), 'b-', linewidth=2)\n",
    "axes[0, 0].set_title('Sigmoid')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_ylabel('Активация')\n",
    "axes[1, 0].plot(x, Activations.sigmoid_derivative(x), 'r-', linewidth=2)\n",
    "axes[1, 0].set_title('Производная Sigmoid')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_ylabel('Градиент')\n",
    "\n",
    "# Tanh\n",
    "axes[0, 1].plot(x, Activations.tanh(x), 'b-', linewidth=2)\n",
    "axes[0, 1].set_title('Tanh')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].plot(x, Activations.tanh_derivative(x), 'r-', linewidth=2)\n",
    "axes[1, 1].set_title('Производная Tanh')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# ReLU\n",
    "axes[0, 2].plot(x, Activations.relu(x), 'b-', linewidth=2)\n",
    "axes[0, 2].set_title('ReLU')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "axes[1, 2].plot(x, Activations.relu_derivative(x), 'r-', linewidth=2)\n",
    "axes[1, 2].set_title('Производная ReLU')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Сравнение продвинутых активаций\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, Activations.relu(x), label='ReLU', linewidth=2)\n",
    "plt.plot(x, Activations.leaky_relu(x), label='Leaky ReLU', linewidth=2)\n",
    "plt.plot(x, Activations.elu(x), label='ELU', linewidth=2)\n",
    "plt.title('Сравнение ReLU-подобных функций')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, Activations.relu_derivative(x), label='ReLU', linewidth=2)\n",
    "plt.plot(x, Activations.leaky_relu_derivative(x), label='Leaky ReLU', linewidth=2)\n",
    "plt.plot(x, Activations.elu_derivative(x), label='ELU', linewidth=2)\n",
    "plt.title('Производные ReLU-подобных функций')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"f'(x)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Сравнение функций активации:\n",
    "\n",
    "| Функция | Диапазон | Преимущества | Недостатки |\n",
    "|---------|----------|--------------|------------|\n",
    "| **Sigmoid** | (0, 1) | Гладкая, вероятностная интерпретация | Затухающий градиент, не центрирована |\n",
    "| **Tanh** | (-1, 1) | Центрирована, гладкая | Затухающий градиент |\n",
    "| **ReLU** | [0, ∞) | Быстрая, нет затухания для x>0 | \"Dying ReLU\" для x<0 |\n",
    "| **Leaky ReLU** | (-∞, ∞) | Решает проблему \"dying ReLU\" | Требует настройки alpha |\n",
    "| **ELU** | (-α, ∞) | Гладкая, робастная | Более медленная |\n",
    "\n",
    "**Рекомендации:**\n",
    "- Начинайте с **ReLU** — работает в большинстве случаев\n",
    "- Для рекуррентных сетей попробуйте **Tanh**\n",
    "- Если есть проблема \"dying ReLU\", используйте **Leaky ReLU** или **ELU**\n",
    "- Для выходного слоя: **Sigmoid** (бинарная классификация), **Softmax** (многоклассовая)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. Проблема затухающего градиента\n",
    "\n",
    "### Что это?\n",
    "\n",
    "В глубоких сетях градиенты могут становиться экспоненциально малыми при обратном распространении через множество слоев.\n",
    "\n",
    "### Математика:\n",
    "\n",
    "При обратном распространении градиент умножается на производные функций активации:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial a_n} \\cdot \\frac{\\partial a_n}{\\partial a_{n-1}} \\cdot ... \\cdot \\frac{\\partial a_2}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial w_1}$$\n",
    "\n",
    "Для sigmoid: $\\sigma'(x) \\leq 0.25$, поэтому градиент уменьшается в $0.25^n$ раз для n слоев!\n",
    "\n",
    "### Демонстрация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_vanishing_gradient():\n",
    "    \"\"\"Демонстрация затухающего градиента\"\"\"\n",
    "    \n",
    "    n_layers = 20\n",
    "    input_val = np.linspace(-3, 3, 100)\n",
    "    \n",
    "    # Для Sigmoid\n",
    "    sigmoid_gradients = []\n",
    "    for _ in range(n_layers):\n",
    "        grad = Activations.sigmoid_derivative(input_val)\n",
    "        sigmoid_gradients.append(np.mean(grad))\n",
    "        input_val = Activations.sigmoid(input_val)\n",
    "    \n",
    "    # Для Tanh\n",
    "    input_val = np.linspace(-3, 3, 100)\n",
    "    tanh_gradients = []\n",
    "    for _ in range(n_layers):\n",
    "        grad = Activations.tanh_derivative(input_val)\n",
    "        tanh_gradients.append(np.mean(grad))\n",
    "        input_val = Activations.tanh(input_val)\n",
    "    \n",
    "    # Для ReLU\n",
    "    input_val = np.linspace(-3, 3, 100)\n",
    "    relu_gradients = []\n",
    "    for _ in range(n_layers):\n",
    "        grad = Activations.relu_derivative(input_val)\n",
    "        relu_gradients.append(np.mean(grad))\n",
    "        input_val = Activations.relu(input_val)\n",
    "    \n",
    "    # Визуализация\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    layers = np.arange(1, n_layers + 1)\n",
    "    plt.plot(layers, sigmoid_gradients, 'o-', label='Sigmoid', linewidth=2)\n",
    "    plt.plot(layers, tanh_gradients, 's-', label='Tanh', linewidth=2)\n",
    "    plt.plot(layers, relu_gradients, '^-', label='ReLU', linewidth=2)\n",
    "    plt.xlabel('Номер слоя')\n",
    "    plt.ylabel('Средний градиент')\n",
    "    plt.title('Затухание градиента по слоям')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.semilogy(layers, sigmoid_gradients, 'o-', label='Sigmoid', linewidth=2)\n",
    "    plt.semilogy(layers, tanh_gradients, 's-', label='Tanh', linewidth=2)\n",
    "    plt.semilogy(layers, relu_gradients, '^-', label='ReLU', linewidth=2)\n",
    "    plt.xlabel('Номер слоя')\n",
    "    plt.ylabel('Средний градиент (log scale)')\n",
    "    plt.title('Затухание градиента (логарифмическая шкала)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Градиенты после 20 слоев:\")\n",
    "    print(f\"Sigmoid: {sigmoid_gradients[-1]:.10f}\")\n",
    "    print(f\"Tanh: {tanh_gradients[-1]:.10f}\")\n",
    "    print(f\"ReLU: {relu_gradients[-1]:.6f}\")\n",
    "\n",
    "demonstrate_vanishing_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Проблема взрывающегося градиента\n",
    "\n",
    "### Что это?\n",
    "\n",
    "Противоположная проблема: градиенты становятся экспоненциально большими.\n",
    "\n",
    "### Причины:\n",
    "- Плохая инициализация весов (слишком большие значения)\n",
    "- Высокая скорость обучения\n",
    "- Неправильная архитектура\n",
    "\n",
    "### Симптомы:\n",
    "- Loss = NaN или Inf\n",
    "- Веса становятся очень большими\n",
    "- Нестабильное обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_exploding_gradient():\n",
    "    \"\"\"Демонстрация взрывающегося градиента\"\"\"\n",
    "    \n",
    "    n_layers = 10\n",
    "    input_size = 100\n",
    "    \n",
    "    # Плохая инициализация (большие веса)\n",
    "    bad_gradients = []\n",
    "    gradient = np.ones(input_size)\n",
    "    for i in range(n_layers):\n",
    "        weight = np.random.randn(input_size, input_size) * 2.0  # Большая дисперсия\n",
    "        gradient = gradient @ weight\n",
    "        bad_gradients.append(np.linalg.norm(gradient))\n",
    "    \n",
    "    # Правильная инициализация (Xavier)\n",
    "    good_gradients = []\n",
    "    gradient = np.ones(input_size)\n",
    "    for i in range(n_layers):\n",
    "        weight = np.random.randn(input_size, input_size) * np.sqrt(2.0 / input_size)\n",
    "        gradient = gradient @ weight\n",
    "        good_gradients.append(np.linalg.norm(gradient))\n",
    "    \n",
    "    # Визуализация\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    layers = np.arange(1, n_layers + 1)\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(layers, bad_gradients, 'ro-', label='Плохая инициализация', linewidth=2)\n",
    "    plt.plot(layers, good_gradients, 'go-', label='Xavier инициализация', linewidth=2)\n",
    "    plt.xlabel('Номер слоя')\n",
    "    plt.ylabel('Норма градиента')\n",
    "    plt.title('Взрывающийся градиент')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.semilogy(layers, bad_gradients, 'ro-', label='Плохая инициализация', linewidth=2)\n",
    "    plt.semilogy(layers, good_gradients, 'go-', label='Xavier инициализация', linewidth=2)\n",
    "    plt.xlabel('Номер слоя')\n",
    "    plt.ylabel('Норма градиента (log scale)')\n",
    "    plt.title('Взрывающийся градиент (логарифмическая шкала)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Норма градиента после 10 слоев:\")\n",
    "    print(f\"Плохая инициализация: {bad_gradients[-1]:.2e}\")\n",
    "    print(f\"Xavier инициализация: {good_gradients[-1]:.2e}\")\n",
    "\n",
    "demonstrate_exploding_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 5. Методы решения проблем градиентов\n",
    "\n",
    "### 5.1. Правильная инициализация весов\n",
    "\n",
    "**Xavier (Glorot) инициализация:**\n",
    "$$W \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{in} + n_{out}}}\\right)$$\n",
    "\n",
    "**He инициализация (для ReLU):**\n",
    "$$W \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{in}}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightInitializer:\n",
    "    \"\"\"Различные методы инициализации весов\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def zeros(shape):\n",
    "        return np.zeros(shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def random(shape, scale=0.01):\n",
    "        return np.random.randn(*shape) * scale\n",
    "    \n",
    "    @staticmethod\n",
    "    def xavier(shape):\n",
    "        \"\"\"Xavier (Glorot) инициализация\"\"\"\n",
    "        fan_in, fan_out = shape[0], shape[1]\n",
    "        limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "        return np.random.uniform(-limit, limit, shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def he(shape):\n",
    "        \"\"\"He инициализация (для ReLU)\"\"\"\n",
    "        fan_in = shape[0]\n",
    "        std = np.sqrt(2.0 / fan_in)\n",
    "        return np.random.randn(*shape) * std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### 5.2. Gradient Clipping\n",
    "\n",
    "Ограничение нормы градиента:\n",
    "$$\\text{if } ||g|| > \\text{threshold}: \\quad g \\leftarrow \\frac{g}{||g||} \\cdot \\text{threshold}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_clipping(gradients, max_norm=1.0):\n",
    "    \"\"\"Обрезка градиентов по норме\"\"\"\n",
    "    total_norm = np.sqrt(sum(np.sum(g**2) for g in gradients))\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    if clip_coef < 1:\n",
    "        return [g * clip_coef for g in gradients]\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### 5.3. Batch Normalization\n",
    "\n",
    "Нормализация входов каждого слоя:\n",
    "$$\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n",
    "$$y = \\gamma \\hat{x} + \\beta$$\n",
    "\n",
    "**Преимущества:**\n",
    "- Ускоряет обучение\n",
    "- Позволяет использовать большие скорости обучения\n",
    "- Уменьшает чувствительность к инициализации\n",
    "- Действует как регуляризатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    \"\"\"Batch Normalization слой\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, epsilon=1e-5, momentum=0.9):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.gamma = np.ones((1, num_features))\n",
    "        self.beta = np.zeros((1, num_features))\n",
    "        self.running_mean = np.zeros((1, num_features))\n",
    "        self.running_var = np.ones((1, num_features))\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            mean = np.mean(X, axis=0, keepdims=True)\n",
    "            var = np.var(X, axis=0, keepdims=True)\n",
    "            \n",
    "            # Обновление скользящих средних\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
    "            \n",
    "            # Нормализация\n",
    "            X_norm = (X - mean) / np.sqrt(var + self.epsilon)\n",
    "        else:\n",
    "            X_norm = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "        \n",
    "        # Масштабирование и сдвиг\n",
    "        out = self.gamma * X_norm + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 6. Методы борьбы с переобучением\n",
    "\n",
    "### Что такое переобучение?\n",
    "\n",
    "Модель отлично работает на обучающих данных, но плохо на тестовых.\n",
    "\n",
    "**Причины:**\n",
    "- Слишком сложная модель\n",
    "- Мало данных\n",
    "- Слишком долгое обучение\n",
    "\n",
    "### 6.1. L2-регуляризация (Ridge)\n",
    "\n",
    "Добавляем штраф за большие веса:\n",
    "$$L_{\\text{total}} = L_{\\text{data}} + \\lambda \\sum_i w_i^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_regularization(weights, lambda_reg):\n",
    "    \"\"\"L2 регуляризация\"\"\"\n",
    "    return lambda_reg * np.sum(weights ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### 6.2. Dropout\n",
    "\n",
    "Случайное отключение нейронов во время обучения.\n",
    "\n",
    "**Как работает:**\n",
    "1. Во время обучения: каждый нейрон \"выключается\" с вероятностью p\n",
    "2. Во время теста: все нейроны активны, выходы масштабируются\n",
    "\n",
    "**Преимущества:**\n",
    "- Предотвращает ко-адаптацию признаков\n",
    "- Эквивалентно ансамблю множества сетей\n",
    "- Очень эффективен против переобучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    \"\"\"Dropout слой\"\"\"\n",
    "    \n",
    "    def __init__(self, drop_prob=0.5):\n",
    "        self.drop_prob = drop_prob\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            self.mask = np.random.binomial(1, 1 - self.drop_prob, X.shape) / (1 - self.drop_prob)\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "\n",
    "# Визуализация Dropout\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Исходная матрица\n",
    "X = np.random.randn(10, 10)\n",
    "axes[0].imshow(X, cmap='viridis', aspect='auto')\n",
    "axes[0].set_title('Исходные активации')\n",
    "axes[0].set_xlabel('Нейроны')\n",
    "axes[0].set_ylabel('Примеры')\n",
    "\n",
    "# С Dropout 0.5\n",
    "dropout = Dropout(drop_prob=0.5)\n",
    "X_drop = dropout.forward(X, training=True)\n",
    "axes[1].imshow(X_drop, cmap='viridis', aspect='auto')\n",
    "axes[1].set_title('С Dropout (p=0.5)')\n",
    "axes[1].set_xlabel('Нейроны')\n",
    "\n",
    "# Маска\n",
    "axes[2].imshow(dropout.mask, cmap='gray', aspect='auto')\n",
    "axes[2].set_title('Маска Dropout')\n",
    "axes[2].set_xlabel('Нейроны')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### 6.3. Early Stopping\n",
    "\n",
    "Остановка обучения при ухудшении качества на валидационной выборке.\n",
    "\n",
    "### 6.4. Data Augmentation\n",
    "\n",
    "Искусственное увеличение размера датасета (повороты, сдвиги, шум, и т.д.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 7. Реализация глубокой нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork:\n",
    "    \"\"\"Глубокая нейронная сеть с продвинутыми техниками\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, activation='relu', dropout_prob=0.0, use_batch_norm=False):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes) - 1\n",
    "        self.activation = activation\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        \n",
    "        # Инициализация весов (He инициализация для ReLU)\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.num_layers):\n",
    "            w = WeightInitializer.he((layer_sizes[i], layer_sizes[i+1]))\n",
    "            b = np.zeros((1, layer_sizes[i+1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "        \n",
    "        # Batch Normalization слои\n",
    "        if use_batch_norm:\n",
    "            self.bn_layers = [BatchNormalization(size) for size in layer_sizes[1:]]\n",
    "        \n",
    "        # Dropout слои\n",
    "        if dropout_prob > 0:\n",
    "            self.dropout_layers = [Dropout(dropout_prob) for _ in range(self.num_layers - 1)]\n",
    "        \n",
    "        # История обучения\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    def _activate(self, z):\n",
    "        if self.activation == 'relu':\n",
    "            return Activations.relu(z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return Activations.sigmoid(z)\n",
    "        elif self.activation == 'tanh':\n",
    "            return Activations.tanh(z)\n",
    "        else:\n",
    "            return z\n",
    "    \n",
    "    def _activate_derivative(self, z):\n",
    "        if self.activation == 'relu':\n",
    "            return Activations.relu_derivative(z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return Activations.sigmoid_derivative(z)\n",
    "        elif self.activation == 'tanh':\n",
    "            return Activations.tanh_derivative(z)\n",
    "        else:\n",
    "            return np.ones_like(z)\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"Прямое распространение\"\"\"\n",
    "        self.activations = [X]\n",
    "        self.zs = []\n",
    "        \n",
    "        A = X\n",
    "        for i in range(self.num_layers):\n",
    "            Z = np.dot(A, self.weights[i]) + self.biases[i]\n",
    "            self.zs.append(Z)\n",
    "            \n",
    "            # Batch Normalization\n",
    "            if self.use_batch_norm and i < self.num_layers - 1:\n",
    "                Z = self.bn_layers[i].forward(Z, training)\n",
    "            \n",
    "            # Активация\n",
    "            if i == self.num_layers - 1:  # Выходной слой (sigmoid для бинарной классификации)\n",
    "                A = Activations.sigmoid(Z)\n",
    "            else:\n",
    "                A = self._activate(Z)\n",
    "                \n",
    "                # Dropout\n",
    "                if self.dropout_prob > 0 and training:\n",
    "                    A = self.dropout_layers[i].forward(A, training)\n",
    "            \n",
    "            self.activations.append(A)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, X, y, learning_rate, lambda_reg=0.01):\n",
    "        \"\"\"Обратное распространение с L2 регуляризацией\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Градиент выходного слоя\n",
    "        dA = self.activations[-1] - y.reshape(-1, 1)\n",
    "        \n",
    "        # Обратное распространение через слои\n",
    "        for i in reversed(range(self.num_layers)):\n",
    "            dZ = dA * Activations.sigmoid_derivative(self.zs[i]) if i == self.num_layers - 1 else dA * self._activate_derivative(self.zs[i])\n",
    "            \n",
    "            # Градиенты весов и смещений\n",
    "            dW = np.dot(self.activations[i].T, dZ) / m + (lambda_reg / m) * self.weights[i]\n",
    "            db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "            \n",
    "            # Обновление параметров\n",
    "            self.weights[i] -= learning_rate * dW\n",
    "            self.biases[i] -= learning_rate * db\n",
    "            \n",
    "            # Градиент для предыдущего слоя\n",
    "            if i > 0:\n",
    "                dA = np.dot(dZ, self.weights[i].T)\n",
    "                \n",
    "                # Dropout градиент\n",
    "                if self.dropout_prob > 0:\n",
    "                    dA = self.dropout_layers[i-1].backward(dA)\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, epochs=1000, \n",
    "              learning_rate=0.01, lambda_reg=0.01, batch_size=32, verbose=True):\n",
    "        \"\"\"Обучение сети с mini-batch gradient descent\"\"\"\n",
    "        \n",
    "        n_samples = X_train.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Перемешивание данных\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_train[indices]\n",
    "            \n",
    "            # Mini-batch обучение\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                # Прямое и обратное распространение\n",
    "                self.forward(X_batch, training=True)\n",
    "                self.backward(X_batch, y_batch, learning_rate, lambda_reg)\n",
    "            \n",
    "            # Вычисление loss\n",
    "            train_pred = self.forward(X_train, training=False)\n",
    "            train_loss = self._compute_loss(y_train, train_pred, lambda_reg)\n",
    "            self.train_losses.append(train_loss)\n",
    "            \n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_pred = self.forward(X_val, training=False)\n",
    "                val_loss = self._compute_loss(y_val, val_pred, lambda_reg)\n",
    "                self.val_losses.append(val_loss)\n",
    "            \n",
    "            if verbose and epoch % 100 == 0:\n",
    "                if X_val is not None:\n",
    "                    print(f'Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}')\n",
    "                else:\n",
    "                    print(f'Epoch {epoch}: Train Loss = {train_loss:.4f}')\n",
    "    \n",
    "    def _compute_loss(self, y, y_pred, lambda_reg):\n",
    "        \"\"\"Вычисление loss с L2 регуляризацией\"\"\"\n",
    "        m = y.shape[0]\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        # Binary cross-entropy\n",
    "        bce_loss = -np.mean(y * np.log(y_pred + 1e-8) + (1 - y) * np.log(1 - y_pred + 1e-8))\n",
    "        \n",
    "        # L2 регуляризация\n",
    "        l2_loss = (lambda_reg / (2 * m)) * sum(np.sum(w ** 2) for w in self.weights)\n",
    "        \n",
    "        return bce_loss + l2_loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Предсказание классов\"\"\"\n",
    "        y_pred = self.forward(X, training=False)\n",
    "        return (y_pred > 0.5).astype(int).flatten()\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Предсказание вероятностей\"\"\"\n",
    "        return self.forward(X, training=False).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 8. Сравнение техник на практике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация данных\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Разделение на train/val/test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
    "\n",
    "# Нормализация\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эксперимент 1: Базовая сеть (без регуляризации)\n",
    "print(\"=\" * 50)\n",
    "print(\"Эксперимент 1: Базовая сеть\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "dnn_basic = DeepNeuralNetwork(\n",
    "    layer_sizes=[20, 64, 32, 16, 1],\n",
    "    activation='relu',\n",
    "    dropout_prob=0.0,\n",
    "    use_batch_norm=False\n",
    ")\n",
    "\n",
    "dnn_basic.train(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    epochs=500,\n",
    "    learning_rate=0.01,\n",
    "    lambda_reg=0.0,\n",
    "    batch_size=32,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "train_acc = np.mean(dnn_basic.predict(X_train) == y_train)\n",
    "val_acc = np.mean(dnn_basic.predict(X_val) == y_val)\n",
    "test_acc = np.mean(dnn_basic.predict(X_test) == y_test)\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Val Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эксперимент 2: С Dropout\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Эксперимент 2: С Dropout\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "dnn_dropout = DeepNeuralNetwork(\n",
    "    layer_sizes=[20, 64, 32, 16, 1],\n",
    "    activation='relu',\n",
    "    dropout_prob=0.3,\n",
    "    use_batch_norm=False\n",
    ")\n",
    "\n",
    "dnn_dropout.train(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    epochs=500,\n",
    "    learning_rate=0.01,\n",
    "    lambda_reg=0.0,\n",
    "    batch_size=32,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "train_acc = np.mean(dnn_dropout.predict(X_train) == y_train)\n",
    "val_acc = np.mean(dnn_dropout.predict(X_val) == y_val)\n",
    "test_acc = np.mean(dnn_dropout.predict(X_test) == y_test)\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Val Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эксперимент 3: С L2 регуляризацией\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Эксперимент 3: С L2 регуляризацией\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "dnn_l2 = DeepNeuralNetwork(\n",
    "    layer_sizes=[20, 64, 32, 16, 1],\n",
    "    activation='relu',\n",
    "    dropout_prob=0.0,\n",
    "    use_batch_norm=False\n",
    ")\n",
    "\n",
    "dnn_l2.train(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    epochs=500,\n",
    "    learning_rate=0.01,\n",
    "    lambda_reg=0.01,\n",
    "    batch_size=32,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "train_acc = np.mean(dnn_l2.predict(X_train) == y_train)\n",
    "val_acc = np.mean(dnn_l2.predict(X_val) == y_val)\n",
    "test_acc = np.mean(dnn_l2.predict(X_test) == y_test)\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Val Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эксперимент 4: Все вместе (Dropout + L2 + BatchNorm)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Эксперимент 4: Dropout + L2 + BatchNorm\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "dnn_full = DeepNeuralNetwork(\n",
    "    layer_sizes=[20, 64, 32, 16, 1],\n",
    "    activation='relu',\n",
    "    dropout_prob=0.3,\n",
    "    use_batch_norm=True\n",
    ")\n",
    "\n",
    "dnn_full.train(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    epochs=500,\n",
    "    learning_rate=0.01,\n",
    "    lambda_reg=0.01,\n",
    "    batch_size=32,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "train_acc = np.mean(dnn_full.predict(X_train) == y_train)\n",
    "val_acc = np.mean(dnn_full.predict(X_val) == y_val)\n",
    "test_acc = np.mean(dnn_full.predict(X_test) == y_test)\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Val Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация кривых обучения\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "models = [\n",
    "    (dnn_basic, 'Базовая сеть'),\n",
    "    (dnn_dropout, 'С Dropout'),\n",
    "    (dnn_l2, 'С L2'),\n",
    "    (dnn_full, 'Dropout + L2 + BN')\n",
    "]\n",
    "\n",
    "for idx, (model, title) in enumerate(models):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    ax.plot(model.train_losses, label='Train Loss', linewidth=2)\n",
    "    ax.plot(model.val_losses, label='Val Loss', linewidth=2)\n",
    "    ax.set_xlabel('Эпоха')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## 9. Residual Networks (ResNet)\n",
    "\n",
    "### Проблема очень глубоких сетей\n",
    "\n",
    "Даже с ReLU и правильной инициализацией, очень глубокие сети (50+ слоев) трудно обучать.\n",
    "\n",
    "### Идея ResNet\n",
    "\n",
    "Вместо обучения функции $H(x)$, обучаем остаточную функцию:\n",
    "$$F(x) = H(x) - x$$\n",
    "\n",
    "Тогда:\n",
    "$$H(x) = F(x) + x$$\n",
    "\n",
    "Это называется **skip connection** или **residual connection**.\n",
    "\n",
    "### Преимущества:\n",
    "- Градиенты легко распространяются через skip connections\n",
    "- Позволяет обучать сети с сотнями слоев\n",
    "- Если слой не нужен, сеть может обучить F(x) ≈ 0\n",
    "\n",
    "### Архитектура ResNet блока:\n",
    "\n",
    "```\n",
    "x → [Conv → BN → ReLU → Conv → BN] → (+) → ReLU → output\n",
    "    ↓                                   ↑\n",
    "    └──────────────────────────────────┘\n",
    "              (skip connection)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock:\n",
    "    \"\"\"Residual блок для fully-connected сетей\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, dropout_prob=0.0):\n",
    "        # Два слоя в residual блоке\n",
    "        self.W1 = WeightInitializer.he((input_size, hidden_size))\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = WeightInitializer.he((hidden_size, input_size))\n",
    "        self.b2 = np.zeros((1, input_size))\n",
    "        \n",
    "        self.bn1 = BatchNormalization(hidden_size)\n",
    "        self.bn2 = BatchNormalization(input_size)\n",
    "        \n",
    "        if dropout_prob > 0:\n",
    "            self.dropout = Dropout(dropout_prob)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"Прямое распространение с skip connection\"\"\"\n",
    "        # Первый слой\n",
    "        Z1 = np.dot(X, self.W1) + self.b1\n",
    "        Z1 = self.bn1.forward(Z1, training)\n",
    "        A1 = Activations.relu(Z1)\n",
    "        \n",
    "        if self.dropout is not None:\n",
    "            A1 = self.dropout.forward(A1, training)\n",
    "        \n",
    "        # Второй слой\n",
    "        Z2 = np.dot(A1, self.W2) + self.b2\n",
    "        Z2 = self.bn2.forward(Z2, training)\n",
    "        \n",
    "        # Skip connection\n",
    "        out = Activations.relu(Z2 + X)  # Residual: F(x) + x\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet:\n",
    "    \"\"\"Residual Network\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_blocks, output_size, dropout_prob=0.0):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_blocks = num_blocks\n",
    "        \n",
    "        # Входной слой (проецирует в hidden_size)\n",
    "        self.W_input = WeightInitializer.he((input_size, hidden_size))\n",
    "        self.b_input = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Residual блоки\n",
    "        self.blocks = [ResidualBlock(hidden_size, hidden_size, dropout_prob) \n",
    "                       for _ in range(num_blocks)]\n",
    "        \n",
    "        # Выходной слой\n",
    "        self.W_output = WeightInitializer.he((hidden_size, output_size))\n",
    "        self.b_output = np.zeros((1, output_size))\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"Прямое распространение\"\"\"\n",
    "        # Входной слой\n",
    "        A = np.dot(X, self.W_input) + self.b_input\n",
    "        A = Activations.relu(A)\n",
    "        \n",
    "        # Residual блоки\n",
    "        for block in self.blocks:\n",
    "            A = block.forward(A, training)\n",
    "        \n",
    "        # Выходной слой\n",
    "        Z = np.dot(A, self.W_output) + self.b_output\n",
    "        out = Activations.sigmoid(Z)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Предсказание классов\"\"\"\n",
    "        y_pred = self.forward(X, training=False)\n",
    "        return (y_pred > 0.5).astype(int).flatten()\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Предсказание вероятностей\"\"\"\n",
    "        return self.forward(X, training=False).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Визуализация архитектуры ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация разницы между обычной сетью и ResNet\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Обычная сеть\n",
    "ax = axes[0]\n",
    "ax.text(0.5, 0.9, 'Input', ha='center', va='center', fontsize=12, \n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "for i in range(5):\n",
    "    y = 0.75 - i * 0.15\n",
    "    ax.arrow(0.5, y + 0.05, 0, -0.08, head_width=0.05, head_length=0.02, fc='black', ec='black')\n",
    "    ax.text(0.5, y, f'Layer {i+1}', ha='center', va='center', fontsize=11,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightcoral'))\n",
    "ax.arrow(0.5, 0.05, 0, -0.08, head_width=0.05, head_length=0.02, fc='black', ec='black')\n",
    "ax.text(0.5, -0.05, 'Output', ha='center', va='center', fontsize=12,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-0.15, 1)\n",
    "ax.axis('off')\n",
    "ax.set_title('Обычная глубокая сеть', fontsize=14, fontweight='bold')\n",
    "\n",
    "# ResNet\n",
    "ax = axes[1]\n",
    "ax.text(0.5, 0.9, 'Input', ha='center', va='center', fontsize=12,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "for i in range(5):\n",
    "    y = 0.75 - i * 0.15\n",
    "    # Основной путь\n",
    "    ax.arrow(0.5, y + 0.05, 0, -0.08, head_width=0.05, head_length=0.02, fc='black', ec='black')\n",
    "    ax.text(0.5, y, f'Block {i+1}', ha='center', va='center', fontsize=11,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightcoral'))\n",
    "    # Skip connection\n",
    "    if i < 4:\n",
    "        ax.annotate('', xy=(0.5, y - 0.05), xytext=(0.5, y + 0.05),\n",
    "                   arrowprops=dict(arrowstyle='->', lw=2, color='blue',\n",
    "                                 connectionstyle='arc3,rad=0.3'))\n",
    "ax.arrow(0.5, 0.05, 0, -0.08, head_width=0.05, head_length=0.02, fc='black', ec='black')\n",
    "ax.text(0.5, -0.05, 'Output', ha='center', va='center', fontsize=12,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "ax.text(0.75, 0.4, 'Skip\\nConnections', ha='center', va='center', \n",
    "        fontsize=10, color='blue', fontweight='bold')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-0.15, 1)\n",
    "ax.axis('off')\n",
    "ax.set_title('Residual Network (ResNet)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## 10. Ключевые выводы\n",
    "\n",
    "### Проблемы глубоких сетей:\n",
    "\n",
    "| Проблема | Причины | Решения |\n",
    "|----------|---------|----------|\n",
    "| **Затухающий градиент** | Sigmoid/Tanh активации, много слоев | ReLU, правильная инициализация, BatchNorm, ResNet |\n",
    "| **Взрывающийся градиент** | Плохая инициализация, высокий LR | Gradient clipping, Xavier/He init, BatchNorm |\n",
    "| **Переобучение** | Сложная модель, мало данных | Dropout, L2, Early stopping, Data augmentation |\n",
    "| **Медленное обучение** | Плохая инициализация | BatchNorm, Adam optimizer |\n",
    "\n",
    "### Лучшие практики:\n",
    "\n",
    "1. **Функции активации:**\n",
    "   - Скрытые слои: ReLU или его варианты\n",
    "   - Выходной слой: Sigmoid (бинарная), Softmax (многоклассовая)\n",
    "\n",
    "2. **Инициализация весов:**\n",
    "   - Xavier для Sigmoid/Tanh\n",
    "   - He для ReLU\n",
    "\n",
    "3. **Регуляризация:**\n",
    "   - Начните с Dropout (0.3-0.5)\n",
    "   - Добавьте L2 если нужно\n",
    "   - Используйте BatchNorm\n",
    "\n",
    "4. **Архитектура:**\n",
    "   - Для очень глубоких сетей (50+ слоев) используйте skip connections (ResNet)\n",
    "   - Начинайте с простой архитектуры и усложняйте\n",
    "\n",
    "5. **Обучение:**\n",
    "   - Mini-batch gradient descent\n",
    "   - Оптимизатор Adam\n",
    "   - Learning rate scheduling\n",
    "   - Early stopping\n",
    "\n",
    "### ResNet революция:\n",
    "\n",
    "- Позволил обучать сети с 100+ слоями\n",
    "- Skip connections решают проблему затухающего градиента\n",
    "- Стал основой для многих современных архитектур\n",
    "- Идея residual learning применима к разным типам сетей\n",
    "\n",
    "### Практические рекомендации:\n",
    "\n",
    "1. Всегда нормализуйте входные данные\n",
    "2. Используйте валидационную выборку для мониторинга\n",
    "3. Визуализируйте кривые обучения\n",
    "4. Экспериментируйте с гиперпараметрами\n",
    "5. Начинайте с простых моделей\n",
    "6. Используйте готовые библиотеки (PyTorch, TensorFlow) для продакшна"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## 11. Задания для самостоятельной работы\n",
    "\n",
    "1. **Градиенты:**\n",
    "   - Реализуйте визуализацию градиентов по слоям во время обучения\n",
    "   - Сравните затухание градиентов для разных функций активации\n",
    "\n",
    "2. **Регуляризация:**\n",
    "   - Реализуйте L1 регуляризацию\n",
    "   - Попробуйте Elastic Net (L1 + L2)\n",
    "   - Реализуйте Early Stopping\n",
    "\n",
    "3. **Оптимизаторы:**\n",
    "   - Реализуйте Adam optimizer\n",
    "   - Сравните SGD, Momentum, RMSprop, Adam\n",
    "\n",
    "4. **Архитектура:**\n",
    "   - Реализуйте ResNet с несколькими residual блоками\n",
    "   - Обучите на MNIST или CIFAR-10\n",
    "   - Сравните с обычной глубокой сетью той же глубины\n",
    "\n",
    "5. **Анализ:**\n",
    "   - Визуализируйте веса сети\n",
    "   - Постройте confusion matrix\n",
    "   - Анализ ошибок модели\n",
    "\n",
    "6. **Продвинутое:**\n",
    "   - Реализуйте DenseNet (dense connections)\n",
    "   - Попробуйте cyclical learning rates\n",
    "   - Реализуйте mixup augmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
