{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кросс-валидация (Cross-Validation)\n",
    "\n",
    "## Введение\n",
    "\n",
    "Кросс-валидация — это метод оценки качества модели машинного обучения на ограниченных данных. Основная идея: разделить данные на несколько частей (фолдов), обучить модель на одних частях и протестировать на других, повторяя процесс для всех комбинаций.\n",
    "\n",
    "### Применение в биологии:\n",
    "- Оценка моделей классификации заболеваний\n",
    "- Предсказание эффективности лекарств\n",
    "- Анализ экспрессии генов\n",
    "- Валидация биомаркеров\n",
    "\n",
    "### Зачем нужна кросс-валидация?\n",
    "- Более надежная оценка качества модели\n",
    "- Защита от переобучения\n",
    "- Эффективное использование ограниченных данных\n",
    "- Выбор гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    KFold, StratifiedKFold, LeaveOneOut, ShuffleSplit,\n",
    "    cross_val_score, cross_validate, learning_curve,\n",
    "    GridSearchCV, RandomizedSearchCV\n",
    ")\n",
    "from sklearn.datasets import make_classification, load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Проблема: почему простое разбиение Train/Test недостаточно?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Генерируем данные: классификация типов клеток\n",
    "X, y = make_classification(\n",
    "    n_samples=100,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Демонстрируем вариабельность при разных разбиениях\n",
    "n_splits = 10\n",
    "accuracies = []\n",
    "\n",
    "for i in range(n_splits):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=i\n",
    "    )\n",
    "    \n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "print(f\"Accuracies для разных разбиений: {[f'{acc:.3f}' for acc in accuracies]}\")\n",
    "print(f\"\\nСреднее: {np.mean(accuracies):.3f}\")\n",
    "print(f\"Стандартное отклонение: {np.std(accuracies):.3f}\")\n",
    "print(f\"Разброс: {np.max(accuracies) - np.min(accuracies):.3f}\")\n",
    "\n",
    "# Визуализация\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(n_splits), accuracies, alpha=0.7, color='steelblue')\n",
    "plt.axhline(np.mean(accuracies), color='red', linestyle='--', linewidth=2, label=f'Среднее: {np.mean(accuracies):.3f}')\n",
    "plt.xlabel('Номер разбиения')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Вариабельность accuracy при разных Train/Test разбиениях')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n⚠️ Проблема: результат сильно зависит от случайного разбиения!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-Fold Cross-Validation\n",
    "\n",
    "Самый популярный метод кросс-валидации:\n",
    "1. Разделить данные на K частей (фолдов)\n",
    "2. Для каждого фолда:\n",
    "   - Использовать его как тестовую выборку\n",
    "   - Остальные K-1 фолдов — как обучающую\n",
    "3. Усреднить результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация K-Fold разбиения\n",
    "def visualize_cv_split(cv, X, y, title):\n",
    "    \"\"\"Визуализация разбиения данных при кросс-валидации\"\"\"\n",
    "    fig, axes = plt.subplots(len(list(cv.split(X, y))), 1, figsize=(12, len(list(cv.split(X, y))) * 0.8))\n",
    "    \n",
    "    if len(list(cv.split(X, y))) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        # Создаем массив для визуализации\n",
    "        indices = np.arange(len(X))\n",
    "        colors = np.array(['white'] * len(X))\n",
    "        colors[train_idx] = 'blue'\n",
    "        colors[test_idx] = 'red'\n",
    "        \n",
    "        # Рисуем\n",
    "        axes[idx].scatter(indices, [idx] * len(indices), c=colors, marker='s', s=50, edgecolors='black', linewidths=0.5)\n",
    "        axes[idx].set_yticks([idx])\n",
    "        axes[idx].set_yticklabels([f'Fold {idx + 1}'])\n",
    "        axes[idx].set_xlim(-1, len(X))\n",
    "        axes[idx].set_ylim(idx - 0.5, idx + 0.5)\n",
    "        \n",
    "        # Аннотация\n",
    "        axes[idx].text(len(X) + 5, idx, f'Train: {len(train_idx)}  Test: {len(test_idx)}', \n",
    "                      verticalalignment='center', fontsize=9)\n",
    "    \n",
    "    axes[-1].set_xlabel('Индекс образца')\n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Легенда\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='blue', edgecolor='black', label='Обучающая выборка'),\n",
    "        Patch(facecolor='red', edgecolor='black', label='Тестовая выборка')\n",
    "    ]\n",
    "    axes[0].legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.2, 1.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Создаем небольшой датасет для визуализации\n",
    "X_small = np.arange(50).reshape(-1, 1)\n",
    "y_small = np.random.randint(0, 2, 50)\n",
    "\n",
    "# 5-Fold CV\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "visualize_cv_split(kfold, X_small, y_small, '5-Fold Cross-Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применяем K-Fold CV\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# 5-fold CV\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"5-Fold Cross-Validation:\")\n",
    "print(f\"Scores по фолдам: {[f'{score:.3f}' for score in cv_scores]}\")\n",
    "print(f\"\\nСредний accuracy: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "\n",
    "# Сравниваем с одним разбиением\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "single_split_score = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nОдно разбиение train/test: {single_split_score:.3f}\")\n",
    "print(f\"\\n✅ Кросс-валидация дает более надежную оценку!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Типы кросс-валидации\n",
    "\n",
    "### 3.1 Stratified K-Fold\n",
    "Сохраняет пропорции классов в каждом фолде (важно для несбалансированных данных)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем несбалансированный датасет\n",
    "X_imb, y_imb = make_classification(\n",
    "    n_samples=100,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    weights=[0.8, 0.2],  # 80% класс 0, 20% класс 1\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Распределение классов: {np.bincount(y_imb)}\")\n",
    "print(f\"Процентное соотношение: {np.bincount(y_imb) / len(y_imb) * 100}\")\n",
    "\n",
    "# Сравниваем обычный KFold и Stratified KFold\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Обычный K-Fold:\")\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(X_imb, y_imb), 1):\n",
    "    print(f\"Fold {fold}: Test set class distribution: {np.bincount(y_imb[test_idx])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Stratified K-Fold:\")\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, test_idx) in enumerate(skfold.split(X_imb, y_imb), 1):\n",
    "    print(f\"Fold {fold}: Test set class distribution: {np.bincount(y_imb[test_idx])}\")\n",
    "\n",
    "print(\"\\n✅ Stratified K-Fold сохраняет пропорции классов!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравниваем качество оценки\n",
    "cv_regular = cross_val_score(model, X_imb, y_imb, cv=KFold(n_splits=5, shuffle=True, random_state=42))\n",
    "cv_stratified = cross_val_score(model, X_imb, y_imb, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].bar(range(5), cv_regular, alpha=0.7, color='steelblue', label='Regular K-Fold')\n",
    "axes[0].axhline(cv_regular.mean(), color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Fold')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title(f'Regular K-Fold\\nMean: {cv_regular.mean():.3f} ± {cv_regular.std():.3f}')\n",
    "axes[0].set_ylim([0.7, 1.0])\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[1].bar(range(5), cv_stratified, alpha=0.7, color='seagreen', label='Stratified K-Fold')\n",
    "axes[1].axhline(cv_stratified.mean(), color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Fold')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title(f'Stratified K-Fold\\nMean: {cv_stratified.mean():.3f} ± {cv_stratified.std():.3f}')\n",
    "axes[1].set_ylim([0.7, 1.0])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Leave-One-Out Cross-Validation (LOOCV)\n",
    "K = n (количество образцов). Каждый образец по очереди становится тестовым."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOCV на небольшом датасете\n",
    "X_small_data, y_small_data = make_classification(n_samples=30, n_features=5, n_informative=3, random_state=42)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "cv_loo = cross_val_score(model, X_small_data, y_small_data, cv=loo)\n",
    "\n",
    "print(f\"LOOCV результаты:\")\n",
    "print(f\"Количество фолдов: {loo.get_n_splits(X_small_data)}\")\n",
    "print(f\"Средний accuracy: {cv_loo.mean():.3f}\")\n",
    "print(f\"\\n⚠️ LOOCV очень затратный по времени для больших датасетов!\")\n",
    "print(f\"   Для датасета из 1000 образцов потребуется 1000 обучений модели.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Shuffle Split\n",
    "Случайные разбиения на train/test с заданными пропорциями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle Split\n",
    "shuffle_split = ShuffleSplit(n_splits=10, test_size=0.3, random_state=42)\n",
    "cv_shuffle = cross_val_score(model, X, y, cv=shuffle_split)\n",
    "\n",
    "print(f\"Shuffle Split (10 итераций, 30% test):\")\n",
    "print(f\"Средний accuracy: {cv_shuffle.mean():.3f} ± {cv_shuffle.std():.3f}\")\n",
    "\n",
    "# Визуализация\n",
    "visualize_cv_split(ShuffleSplit(n_splits=5, test_size=0.3, random_state=42), \n",
    "                   X_small, y_small, 'Shuffle Split (5 splits, 30% test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Детальный анализ с cross_validate\n",
    "\n",
    "Функция `cross_validate` позволяет получить несколько метрик одновременно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Множественные метрики\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1'\n",
    "}\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    model, X, y, \n",
    "    cv=5, \n",
    "    scoring=scoring,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Создаем DataFrame для удобства\n",
    "results_df = pd.DataFrame({\n",
    "    'Fold': range(1, 6),\n",
    "    'Train Accuracy': cv_results['train_accuracy'],\n",
    "    'Test Accuracy': cv_results['test_accuracy'],\n",
    "    'Test Precision': cv_results['test_precision'],\n",
    "    'Test Recall': cv_results['test_recall'],\n",
    "    'Test F1': cv_results['test_f1']\n",
    "})\n",
    "\n",
    "print(\"Детальные результаты кросс-валидации:\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nСредние значения:\")\n",
    "print(results_df.mean(numeric_only=True).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация метрик\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# График 1: Все метрики по фолдам\n",
    "metrics = ['Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1']\n",
    "x = np.arange(5)\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[0].bar(x + i*width, results_df[metric], width, label=metric.replace('Test ', ''), alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel('Fold')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Метрики по фолдам')\n",
    "axes[0].set_xticks(x + width * 1.5)\n",
    "axes[0].set_xticklabels([f'Fold {i+1}' for i in range(5)])\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# График 2: Train vs Test Accuracy\n",
    "axes[1].plot(range(1, 6), results_df['Train Accuracy'], 'o-', linewidth=2, markersize=8, label='Train Accuracy')\n",
    "axes[1].plot(range(1, 6), results_df['Test Accuracy'], 's-', linewidth=2, markersize=8, label='Test Accuracy')\n",
    "axes[1].fill_between(range(1, 6), results_df['Train Accuracy'], results_df['Test Accuracy'], alpha=0.2)\n",
    "axes[1].set_xlabel('Fold')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Сравнение Train и Test Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Learning Curves — диагностика модели\n",
    "\n",
    "Learning curves показывают, как меняется качество модели в зависимости от размера обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем больший датасет\n",
    "X_large, y_large = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Строим learning curves для разных моделей\n",
    "models_to_compare = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (name, model) in enumerate(models_to_compare.items()):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        model, X_large, y_large,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    axes[idx].plot(train_sizes, train_mean, 'o-', linewidth=2, label='Train score')\n",
    "    axes[idx].fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2)\n",
    "    \n",
    "    axes[idx].plot(train_sizes, test_mean, 's-', linewidth=2, label='CV score')\n",
    "    axes[idx].fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.2)\n",
    "    \n",
    "    axes[idx].set_xlabel('Размер обучающей выборки')\n",
    "    axes[idx].set_ylabel('Accuracy')\n",
    "    axes[idx].set_title(f'{name}\\nLearning Curve')\n",
    "    axes[idx].legend(loc='lower right')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_ylim([0.5, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Подбор гиперпараметров с Grid Search\n",
    "\n",
    "Кросс-валидация — ключевой инструмент для подбора гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем реальный биологический датасет\n",
    "data = load_breast_cancer()\n",
    "X_cancer = data.data\n",
    "y_cancer = data.target\n",
    "\n",
    "print(\"Датасет: Wisconsin Breast Cancer\")\n",
    "print(f\"Количество образцов: {len(X_cancer)}\")\n",
    "print(f\"Количество признаков: {X_cancer.shape[1]}\")\n",
    "print(f\"Классы: {data.target_names}\")\n",
    "print(f\"Распределение классов: {np.bincount(y_cancer)}\")\n",
    "\n",
    "# Нормализация данных\n",
    "scaler = StandardScaler()\n",
    "X_cancer_scaled = scaler.fit_transform(X_cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search для Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Запуск Grid Search...\")\n",
    "print(f\"Всего комбинаций параметров: {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf'])}\")\n",
    "\n",
    "grid_search.fit(X_cancer_scaled, y_cancer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Результаты Grid Search:\")\n",
    "print(f\"Лучшие параметры: {grid_search.best_params_}\")\n",
    "print(f\"Лучший CV score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация результатов Grid Search\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "top_10 = results.nsmallest(10, 'rank_test_score')[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]\n",
    "\n",
    "print(\"\\nТоп-10 комбинаций параметров:\")\n",
    "for idx, row in top_10.iterrows():\n",
    "    print(f\"Rank {int(row['rank_test_score'])}: Score = {row['mean_test_score']:.4f} (±{row['std_test_score']:.4f})\")\n",
    "    print(f\"  Параметры: {row['params']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: влияние двух параметров\n",
    "# Фиксируем некоторые параметры и смотрим на влияние n_estimators и max_depth\n",
    "pivot_data = results[results['param_min_samples_split'] == 2][results['param_min_samples_leaf'] == 1]\n",
    "\n",
    "if len(pivot_data) > 0:\n",
    "    pivot_table = pivot_data.pivot_table(\n",
    "        values='mean_test_score',\n",
    "        index='param_max_depth',\n",
    "        columns='param_n_estimators'\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='YlGnBu', cbar_kws={'label': 'Mean CV Score'})\n",
    "    plt.title('Grid Search: Влияние n_estimators и max_depth\\n(min_samples_split=2, min_samples_leaf=1)')\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('max_depth')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Randomized Search — более эффективная альтернатива\n",
    "\n",
    "Для больших пространств параметров Randomized Search часто эффективнее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Распределения параметров\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': uniform(0.5, 0.5)  # от 0.5 до 1.0\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf,\n",
    "    param_distributions,\n",
    "    n_iter=50,  # количество случайных комбинаций\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Запуск Randomized Search (50 итераций)...\")\n",
    "random_search.fit(X_cancer_scaled, y_cancer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Результаты Randomized Search:\")\n",
    "print(f\"Лучшие параметры: {random_search.best_params_}\")\n",
    "print(f\"Лучший CV score: {random_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\nСравнение Grid Search vs Randomized Search:\")\n",
    "print(f\"Grid Search - лучший score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Randomized Search - лучший score: {random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Nested Cross-Validation\n",
    "\n",
    "Для несмещенной оценки качества модели с подбором гиперпараметров используется вложенная кросс-валидация:\n",
    "- Внешний цикл — для оценки качества\n",
    "- Внутренний цикл — для подбора гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested CV\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Упрощенная сетка параметров для скорости\n",
    "param_grid_simple = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [5, 10, None]\n",
    "}\n",
    "\n",
    "nested_scores = []\n",
    "\n",
    "print(\"Nested Cross-Validation:\")\n",
    "for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X_cancer_scaled, y_cancer), 1):\n",
    "    X_train, X_test = X_cancer_scaled[train_idx], X_cancer_scaled[test_idx]\n",
    "    y_train, y_test = y_cancer[train_idx], y_cancer[test_idx]\n",
    "    \n",
    "    # Внутренний цикл - подбор гиперпараметров\n",
    "    clf = GridSearchCV(rf, param_grid_simple, cv=inner_cv, scoring='accuracy')\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Оценка на тестовой выборке внешнего фолда\n",
    "    score = clf.score(X_test, y_test)\n",
    "    nested_scores.append(score)\n",
    "    \n",
    "    print(f\"Fold {fold}: Test Score = {score:.4f}, Best Params = {clf.best_params_}\")\n",
    "\n",
    "print(f\"\\nНесмещенная оценка модели: {np.mean(nested_scores):.4f} ± {np.std(nested_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Практические рекомендации\n",
    "\n",
    "### Выбор количества фолдов:\n",
    "- **K=5**: быстро, хорошо для больших датасетов\n",
    "- **K=10**: золотой стандарт, баланс между bias и variance\n",
    "- **LOOCV**: максимальное использование данных, но медленно\n",
    "\n",
    "### Когда использовать Stratified CV:\n",
    "- Несбалансированные классы\n",
    "- Малые датасеты\n",
    "- Классификация\n",
    "\n",
    "### Grid Search vs Randomized Search:\n",
    "- **Grid Search**: для небольших пространств параметров\n",
    "- **Randomized Search**: для больших пространств, когда нужна скорость\n",
    "\n",
    "### Nested CV:\n",
    "- Всегда используйте для финальной оценки модели с подбором гиперпараметров\n",
    "- Дает несмещенную оценку качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Задания для самостоятельной работы\n",
    "\n",
    "1. Реализуйте Time Series Cross-Validation для временных рядов\n",
    "2. Сравните различные стратегии кросс-валидации на реальных биологических данных\n",
    "3. Реализуйте собственный cross-validator для специфических задач (например, группированная кросс-валидация)\n",
    "4. Исследуйте влияние размера валидационной выборки на стабильность оценок"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
