{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный бустинг (Gradient Boosting)\n",
    "\n",
    "## Введение\n",
    "\n",
    "Gradient Boosting — это мощный ансамблевый алгоритм, который последовательно строит деревья, каждое из которых исправляет ошибки предыдущих. В отличие от Random Forest (bagging), который строит деревья параллельно и независимо, Gradient Boosting использует **boosting** — последовательное обучение.\n",
    "\n",
    "### Применение в биологии:\n",
    "- Предсказание активности соединений (drug discovery)\n",
    "- Классификация белковых последовательностей\n",
    "- Предсказание клинических исходов\n",
    "- Анализ геномных данных\n",
    "- Предсказание токсичности веществ\n",
    "\n",
    "### Ключевые различия подходов:\n",
    "\n",
    "| Аспект | Random Forest (Bagging) | Gradient Boosting |\n",
    "|--------|------------------------|-------------------|\n",
    "| **Стратегия** | Параллельные независимые деревья | Последовательные зависимые деревья |\n",
    "| **Цель** | Снизить variance | Снизить bias |\n",
    "| **Усреднение** | Простое усреднение/голосование | Взвешенная сумма |\n",
    "| **Деревья** | Глубокие (low bias) | Мелкие (high bias) |\n",
    "| **Overfitting** | Устойчив | Склонен (нужна регуляризация) |\n",
    "\n",
    "### Основная идея Gradient Boosting:\n",
    "\n",
    "1. Начинаем с простой модели (константа или мелкое дерево)\n",
    "2. Вычисляем ошибки (остатки) текущей модели\n",
    "3. Обучаем новое дерево предсказывать эти ошибки\n",
    "4. Добавляем новое дерево к ансамблю с коэффициентом learning_rate\n",
    "5. Повторяем шаги 2-4\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)$$\n",
    "\n",
    "где $\\eta$ — learning rate, $h_m(x)$ — новое дерево"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T10:04:05.767805Z",
     "start_time": "2026-01-24T10:04:03.837860Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.datasets import make_moons, make_classification\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmatplotlib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplt\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mensemble\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m GradientBoostingRegressor, GradientBoostingClassifier\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mensemble\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m RandomForestRegressor, RandomForestClassifier\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtree\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DecisionTreeRegressor\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/ml-biology-WzZaiGi0-py3.13/lib/python3.13/site-packages/sklearn/ensemble/__init__.py:6\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[33;03m\"\"\"Ensemble-based methods for classification, regression and anomaly detection.\"\"\"\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# Authors: The scikit-learn developers\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# SPDX-License-Identifier: BSD-3-Clause\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mensemble\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_bagging\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m BaggingClassifier, BaggingRegressor\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mensemble\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_base\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m BaseEnsemble\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mensemble\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_forest\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      9\u001B[39m     ExtraTreesClassifier,\n\u001B[32m     10\u001B[39m     ExtraTreesRegressor,\n\u001B[32m   (...)\u001B[39m\u001B[32m     13\u001B[39m     RandomTreesEmbedding,\n\u001B[32m     14\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/ml-biology-WzZaiGi0-py3.13/lib/python3.13/site-packages/sklearn/ensemble/_bagging.py:18\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mensemble\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_base\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m BaseEnsemble, _partition_estimators\n\u001B[32m     17\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmetrics\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m accuracy_score, r2_score\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtree\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DecisionTreeClassifier, DecisionTreeRegressor\n\u001B[32m     19\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Bunch, _safe_indexing, check_random_state, column_or_1d\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_mask\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m indices_to_mask\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/ml-biology-WzZaiGi0-py3.13/lib/python3.13/site-packages/sklearn/tree/__init__.py:6\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[33;03m\"\"\"Decision tree based models for classification and regression.\"\"\"\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# Authors: The scikit-learn developers\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# SPDX-License-Identifier: BSD-3-Clause\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtree\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_classes\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      7\u001B[39m     BaseDecisionTree,\n\u001B[32m      8\u001B[39m     DecisionTreeClassifier,\n\u001B[32m      9\u001B[39m     DecisionTreeRegressor,\n\u001B[32m     10\u001B[39m     ExtraTreeClassifier,\n\u001B[32m     11\u001B[39m     ExtraTreeRegressor,\n\u001B[32m     12\u001B[39m )\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtree\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_export\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m export_graphviz, export_text, plot_tree\n\u001B[32m     15\u001B[39m __all__ = [\n\u001B[32m     16\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mBaseDecisionTree\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     17\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mDecisionTreeClassifier\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     23\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mplot_tree\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     24\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/ml-biology-WzZaiGi0-py3.13/lib/python3.13/site-packages/sklearn/tree/_classes.py:27\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mscipy\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msparse\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m issparse\n\u001B[32m     18\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbase\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     19\u001B[39m     BaseEstimator,\n\u001B[32m     20\u001B[39m     ClassifierMixin,\n\u001B[32m   (...)\u001B[39m\u001B[32m     25\u001B[39m     is_classifier,\n\u001B[32m     26\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtree\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _criterion, _splitter, _tree\n\u001B[32m     28\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtree\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_criterion\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Criterion\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtree\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_splitter\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Splitter\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/ml-biology-WzZaiGi0-py3.13/lib/python3.13/site-packages/sklearn/tree/_criterion.pyx:1\u001B[39m, in \u001B[36minit sklearn.tree._criterion\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/ml-biology-WzZaiGi0-py3.13/lib/python3.13/site-packages/sklearn/tree/_splitter.pyx:1\u001B[39m, in \u001B[36minit sklearn.tree._splitter\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:463\u001B[39m, in \u001B[36m_lock_unlock_module\u001B[39m\u001B[34m(name)\u001B[39m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Визуализация процесса бустинга\n",
    "\n",
    "Покажем, как постепенно улучшаются предсказания при добавлении новых деревьев."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Генерируем данные\n",
    "def true_function(x):\n",
    "    return np.sin(x) * x + 0.5 * x\n",
    "\n",
    "n_samples = 100\n",
    "X_train = np.sort(np.random.uniform(0, 10, n_samples))\n",
    "y_train = true_function(X_train) + np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "X_test = np.linspace(0, 10, 200)\n",
    "y_test_true = true_function(X_test)\n",
    "\n",
    "X_train_2d = X_train.reshape(-1, 1)\n",
    "X_test_2d = X_test.reshape(-1, 1)\n",
    "\n",
    "print(f\"Размер выборки: {n_samples}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Визуализация эволюции бустинга\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 9))\n",
    "axes = axes.ravel()\n",
    "\n",
    "n_estimators_stages = [1, 3, 10, 30, 100, 200]\n",
    "\n",
    "for idx, n_est in enumerate(n_estimators_stages):\n",
    "    # Обучаем Gradient Boosting\n",
    "    gb = GradientBoostingRegressor(n_estimators=n_est, max_depth=3, \n",
    "                                   learning_rate=0.1, random_state=42)\n",
    "    gb.fit(X_train_2d, y_train)\n",
    "    \n",
    "    y_pred = gb.predict(X_test_2d)\n",
    "    train_mse = mean_squared_error(y_train, gb.predict(X_train_2d))\n",
    "    \n",
    "    # График\n",
    "    axes[idx].scatter(X_train, y_train, alpha=0.4, s=20, label='Данные')\n",
    "    axes[idx].plot(X_test, y_test_true, 'g--', lw=2, label='Истинная функция')\n",
    "    axes[idx].plot(X_test, y_pred, 'r-', lw=2, label=f'Бустинг ({n_est} деревьев)')\n",
    "    axes[idx].set_xlabel('X')\n",
    "    axes[idx].set_ylabel('y')\n",
    "    axes[idx].set_title(f'n_estimators = {n_est}\\nTrain MSE = {train_mse:.3f}', fontweight='bold')\n",
    "    axes[idx].legend(fontsize=8)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ С каждым новым деревом модель лучше аппроксимирует истинную функцию!\")\n",
    "print(\"  Но слишком много деревьев может привести к overfitting\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bias-Variance Tradeoff для Gradient Boosting\n",
    "\n",
    "### Теория:\n",
    "\n",
    "Gradient Boosting работает иначе, чем Random Forest:\n",
    "\n",
    "- **Random Forest**: Начинаем с low bias (глубокие деревья) → снижаем variance через усреднение\n",
    "- **Gradient Boosting**: Начинаем с high bias (мелкие деревья) → снижаем bias последовательным добавлением деревьев\n",
    "\n",
    "### Ключевые гиперпараметры:\n",
    "\n",
    "1. **n_estimators** (количество деревьев):\n",
    "   - Больше → меньше bias\n",
    "   - Но МОЖЕТ привести к overfitting!\n",
    "   \n",
    "2. **learning_rate** (скорость обучения):\n",
    "   - Меньше → нужно больше деревьев, но лучше обобщение\n",
    "   - Типичные значения: 0.01 - 0.3\n",
    "   \n",
    "3. **max_depth** (глубина деревьев):\n",
    "   - Обычно мелкие деревья (3-8)\n",
    "   - Глубже → больше риск overfitting\n",
    "\n",
    "### Важный трейдофф:\n",
    "\n",
    "$$\\text{learning\\_rate} \\times \\text{n\\_estimators} \\approx \\text{const}$$\n",
    "\n",
    "Маленький learning_rate требует больше деревьев, но дает лучшую обобщающую способность.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эксперимент: Влияние количества деревьев и learning rate"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Функция для вычисления bias и variance\n",
    "def compute_bias_variance_gb(n_estimators, learning_rate, max_depth=3, n_iterations=30):\n",
    "    \"\"\"Вычисляет bias и variance для Gradient Boosting\"\"\"\n",
    "    predictions = np.zeros((n_iterations, len(X_test)))\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Генерируем новую выборку\n",
    "        X_sample = np.sort(np.random.uniform(0, 10, n_samples))\n",
    "        y_sample = true_function(X_sample) + np.random.normal(0, 0.5, n_samples)\n",
    "        \n",
    "        # Обучаем Gradient Boosting\n",
    "        gb = GradientBoostingRegressor(n_estimators=n_estimators, \n",
    "                                      learning_rate=learning_rate,\n",
    "                                      max_depth=max_depth, \n",
    "                                      random_state=i)\n",
    "        gb.fit(X_sample.reshape(-1, 1), y_sample)\n",
    "        \n",
    "        predictions[i] = gb.predict(X_test_2d)\n",
    "    \n",
    "    mean_prediction = np.mean(predictions, axis=0)\n",
    "    bias_squared = np.mean((mean_prediction - y_test_true)**2)\n",
    "    variance = np.mean(np.var(predictions, axis=0))\n",
    "    \n",
    "    return bias_squared, variance\n",
    "\n",
    "# Эксперимент 1: влияние n_estimators (при фиксированном learning_rate)\n",
    "n_trees_range = [1, 5, 10, 20, 50, 100, 200, 300]\n",
    "biases_trees_gb = []\n",
    "variances_trees_gb = []\n",
    "\n",
    "print(\"Вычисление bias и variance для разного количества деревьев...\")\n",
    "for n_trees in n_trees_range:\n",
    "    bias_sq, var = compute_bias_variance_gb(n_trees, learning_rate=0.1)\n",
    "    biases_trees_gb.append(bias_sq)\n",
    "    variances_trees_gb.append(var)\n",
    "    print(f\"n_estimators={n_trees:3d}: Bias² = {bias_sq:.3f}, Variance = {var:.3f}\")\n",
    "\n",
    "print(\"\\nГотово!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# График зависимости от количества деревьев\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(n_trees_range, biases_trees_gb, 'b-o', lw=2, markersize=7, label='Bias²')\n",
    "ax.plot(n_trees_range, variances_trees_gb, 'r-s', lw=2, markersize=7, label='Variance')\n",
    "total_error_gb = np.array(biases_trees_gb) + np.array(variances_trees_gb)\n",
    "ax.plot(n_trees_range, total_error_gb, 'g-^', lw=2.5, markersize=7, label='Total Error')\n",
    "\n",
    "optimal_n_trees = n_trees_range[np.argmin(total_error_gb)]\n",
    "ax.axvline(optimal_n_trees, color='purple', linestyle='--', lw=2, alpha=0.7,\n",
    "          label=f'Оптимум ≈ {optimal_n_trees} деревьев')\n",
    "\n",
    "ax.set_xlabel('Количество деревьев (n_estimators)', fontsize=12)\n",
    "ax.set_ylabel('Ошибка', fontsize=12)\n",
    "ax.set_title('Gradient Boosting: Bias-Variance Tradeoff\\n(learning_rate=0.1, max_depth=3)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "# Аннотации\n",
    "ax.annotate('Bias уменьшается', xy=(50, biases_trees_gb[4]), xytext=(20, biases_trees_gb[4]+1),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue', lw=1.5),\n",
    "            fontsize=10, color='blue', fontweight='bold')\n",
    "\n",
    "ax.annotate('Variance растет!\\n(overfitting)', xy=(200, variances_trees_gb[-2]), xytext=(80, variances_trees_gb[-2]+1.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=1.5),\n",
    "            fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ В отличие от Random Forest, слишком много деревьев МОЖЕТ привести к overfitting!\")\n",
    "print(f\"✓ Оптимальное количество: ~{optimal_n_trees} деревьев при learning_rate=0.1\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эксперимент: Влияние learning rate"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Сравниваем разные learning rates\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.3]\n",
    "n_trees_test = range(10, 201, 20)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, lr in enumerate(learning_rates):\n",
    "    biases_lr = []\n",
    "    variances_lr = []\n",
    "    \n",
    "    print(f\"\\nОбработка learning_rate = {lr}...\")\n",
    "    for n_trees in n_trees_test:\n",
    "        bias_sq, var = compute_bias_variance_gb(n_trees, learning_rate=lr, n_iterations=20)\n",
    "        biases_lr.append(bias_sq)\n",
    "        variances_lr.append(var)\n",
    "    \n",
    "    total_error_lr = np.array(biases_lr) + np.array(variances_lr)\n",
    "    \n",
    "    # График\n",
    "    axes[idx].plot(n_trees_test, biases_lr, 'b-o', lw=2, markersize=5, label='Bias²')\n",
    "    axes[idx].plot(n_trees_test, variances_lr, 'r-s', lw=2, markersize=5, label='Variance')\n",
    "    axes[idx].plot(n_trees_test, total_error_lr, 'g-^', lw=2.5, markersize=5, label='Total Error')\n",
    "    \n",
    "    optimal_idx = np.argmin(total_error_lr)\n",
    "    axes[idx].axvline(list(n_trees_test)[optimal_idx], color='purple', \n",
    "                     linestyle='--', lw=2, alpha=0.5)\n",
    "    \n",
    "    axes[idx].set_xlabel('n_estimators')\n",
    "    axes[idx].set_ylabel('Ошибка')\n",
    "    axes[idx].set_title(f'learning_rate = {lr}\\nОптимум: {list(n_trees_test)[optimal_idx]} деревьев', \n",
    "                       fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Меньший learning_rate требует больше деревьев, но дает лучший результат!\")\n",
    "print(\"✓ Компромисс: lr=0.05-0.1 с early stopping\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Сравнение Gradient Boosting и Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Генерируем данные для классификации\n",
    "X_class, y_class = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_class, y_class, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Обучаем модели\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_clf.fit(X_train_c, y_train_c)\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, max_depth=3, \n",
    "                                   learning_rate=0.1, random_state=42)\n",
    "gb_clf.fit(X_train_c, y_train_c)\n",
    "\n",
    "# Оценка\n",
    "rf_train_acc = accuracy_score(y_train_c, rf_clf.predict(X_train_c))\n",
    "rf_test_acc = accuracy_score(y_test_c, rf_clf.predict(X_test_c))\n",
    "\n",
    "gb_train_acc = accuracy_score(y_train_c, gb_clf.predict(X_train_c))\n",
    "gb_test_acc = accuracy_score(y_test_c, gb_clf.predict(X_test_c))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"СРАВНЕНИЕ: RANDOM FOREST vs GRADIENT BOOSTING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Random Forest:\")\n",
    "print(f\"  Train Accuracy: {rf_train_acc:.2%}\")\n",
    "print(f\"  Test Accuracy:  {rf_test_acc:.2%}\")\n",
    "print(f\"  Overfitting gap: {(rf_train_acc - rf_test_acc):.2%}\")\n",
    "print(f\"\\nGradient Boosting:\")\n",
    "print(f\"  Train Accuracy: {gb_train_acc:.2%}\")\n",
    "print(f\"  Test Accuracy:  {gb_test_acc:.2%}\")\n",
    "print(f\"  Overfitting gap: {(gb_train_acc - gb_test_acc):.2%}\")\n",
    "print(f\"\\n{'✓ Gradient Boosting обычно точнее, но требует тщательной настройки!' if gb_test_acc > rf_test_acc else '✓ Random Forest более устойчив к overfitting!'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация границ решений"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_decision_boundary(model, X, y, ax, title):\n",
    "    \"\"\"Визуализирует границу решения\"\"\"\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black', s=40, alpha=0.7)\n",
    "    ax.set_title(title, fontweight='bold', fontsize=11)\n",
    "    ax.set_xlabel('Признак 1')\n",
    "    ax.set_ylabel('Признак 2')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "plot_decision_boundary(rf_clf, X_test_c, y_test_c, axes[0],\n",
    "                      f'Random Forest\\nTest Acc = {rf_test_acc:.2%}')\n",
    "plot_decision_boundary(gb_clf, X_test_c, y_test_c, axes[1],\n",
    "                      f'Gradient Boosting\\nTest Acc = {gb_test_acc:.2%}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Регуляризация в Gradient Boosting\n",
    "\n",
    "Для предотвращения overfitting используются различные техники:\n",
    "\n",
    "1. **Learning rate**: меньше → медленнее обучение → лучше обобщение\n",
    "2. **Subsample**: использовать только часть данных для каждого дерева (stochastic gradient boosting)\n",
    "3. **max_depth**: ограничивать глубину деревьев\n",
    "4. **min_samples_split / min_samples_leaf**: требовать минимум объектов в узлах\n",
    "5. **Early stopping**: остановка обучения при ухудшении на валидации"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Демонстрация влияния subsample\n",
    "X_train_reg, X_val_reg, y_train_reg, y_val_reg = train_test_split(\n",
    "    X_train_2d, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "subsamples = [0.5, 0.7, 1.0]\n",
    "n_estimators_range = range(1, 201, 5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for subsample in subsamples:\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    \n",
    "    for n_est in n_estimators_range:\n",
    "        gb = GradientBoostingRegressor(n_estimators=n_est, max_depth=3, \n",
    "                                      learning_rate=0.1, subsample=subsample,\n",
    "                                      random_state=42)\n",
    "        gb.fit(X_train_reg, y_train_reg)\n",
    "        \n",
    "        train_errors.append(mean_squared_error(y_train_reg, gb.predict(X_train_reg)))\n",
    "        val_errors.append(mean_squared_error(y_val_reg, gb.predict(X_val_reg)))\n",
    "    \n",
    "    ax.plot(n_estimators_range, val_errors, lw=2, label=f'subsample={subsample}')\n",
    "\n",
    "ax.set_xlabel('Количество деревьев (n_estimators)', fontsize=12)\n",
    "ax.set_ylabel('Validation MSE', fontsize=12)\n",
    "ax.set_title('Влияние subsample на переобучение', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ subsample < 1.0 (stochastic gradient boosting) помогает избежать overfitting!\")\n",
    "print(\"  Рекомендуемые значения: 0.5-0.8\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Gradient Boosting с early stopping\n",
    "gb_early = GradientBoostingRegressor(n_estimators=1000, max_depth=3, \n",
    "                                    learning_rate=0.05, subsample=0.8,\n",
    "                                    validation_fraction=0.2, n_iter_no_change=20,\n",
    "                                    random_state=42)\n",
    "gb_early.fit(X_train_2d, y_train)\n",
    "\n",
    "print(f\"\\nEarly Stopping:\")\n",
    "print(f\"  Максимум итераций: 1000\")\n",
    "print(f\"  Фактически использовано: {gb_early.n_estimators_} деревьев\")\n",
    "print(f\"  Остановка при отсутствии улучшения за 20 итераций\")\n",
    "print(f\"\\n✓ Early stopping автоматически находит оптимальное количество деревьев!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Важность признаков"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Генерируем данные с множеством признаков\n",
    "X_features, y_features = make_classification(\n",
    "    n_samples=1000, n_features=15, n_informative=8, n_redundant=4,\n",
    "    n_repeated=0, n_classes=2, random_state=42\n",
    ")\n",
    "\n",
    "feature_names = [f'Признак {i+1}' for i in range(15)]\n",
    "\n",
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(\n",
    "    X_features, y_features, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Обучаем обе модели\n",
    "rf_feat = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_feat.fit(X_train_f, y_train_f)\n",
    "\n",
    "gb_feat = GradientBoostingClassifier(n_estimators=100, max_depth=3, \n",
    "                                    learning_rate=0.1, random_state=42)\n",
    "gb_feat.fit(X_train_f, y_train_f)\n",
    "\n",
    "# Сравнение важности признаков\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Random Forest\n",
    "rf_importances = rf_feat.feature_importances_\n",
    "rf_indices = np.argsort(rf_importances)[::-1]\n",
    "axes[0].bar(range(15), rf_importances[rf_indices], color='steelblue', alpha=0.8)\n",
    "axes[0].set_xticks(range(15))\n",
    "axes[0].set_xticklabels([feature_names[i] for i in rf_indices], rotation=45, ha='right')\n",
    "axes[0].set_ylabel('Важность')\n",
    "axes[0].set_title('Feature Importance: Random Forest', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_importances = gb_feat.feature_importances_\n",
    "gb_indices = np.argsort(gb_importances)[::-1]\n",
    "axes[1].bar(range(15), gb_importances[gb_indices], color='darkgreen', alpha=0.8)\n",
    "axes[1].set_xticks(range(15))\n",
    "axes[1].set_xticklabels([feature_names[i] for i in gb_indices], rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Важность')\n",
    "axes[1].set_title('Feature Importance: Gradient Boosting', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nСравнение моделей:\")\n",
    "print(f\"Random Forest     - Test Accuracy: {accuracy_score(y_test_f, rf_feat.predict(X_test_f)):.2%}\")\n",
    "print(f\"Gradient Boosting - Test Accuracy: {accuracy_score(y_test_f, gb_feat.predict(X_test_f)):.2%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Современные реализации: XGBoost и LightGBM\n",
    "\n",
    "Sklearn's GradientBoosting хорош для понимания, но на практике используют более эффективные реализации:\n",
    "\n",
    "### XGBoost (Extreme Gradient Boosting):\n",
    "- Оптимизированная реализация\n",
    "- Регуляризация L1/L2\n",
    "- Параллельное построение деревьев\n",
    "- Обработка пропущенных значений\n",
    "\n",
    "### LightGBM (Light Gradient Boosting Machine):\n",
    "- Еще быстрее чем XGBoost\n",
    "- Leaf-wise рост деревьев (вместо level-wise)\n",
    "- Эффективна на больших данных\n",
    "\n",
    "### CatBoost:\n",
    "- Автоматическая обработка категориальных признаков\n",
    "- Устойчивость к overfitting\n",
    "- Хорошие результаты \"из коробки\"\n",
    "\n",
    "```python\n",
    "# Пример использования (требуется установка)\n",
    "# pip install xgboost lightgbm catboost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "lgb = LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "cat = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=3, verbose=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Резюме\n",
    "\n",
    "### Bias-Variance Tradeoff: Итоговое сравнение\n",
    "\n",
    "| Алгоритм | Начальное состояние | Стратегия снижения ошибки | Склонность к overfitting |\n",
    "|----------|-------------------|--------------------------|-------------------------|\n",
    "| **Одиночное дерево** | High variance | Ограничение глубины | Очень высокая ↑↑↑ |\n",
    "| **Random Forest** | High variance (глубокие деревья) | Снижение variance через bagging | Низкая ↓ |\n",
    "| **Gradient Boosting** | High bias (мелкие деревья) | Снижение bias через boosting | Средняя → |\n",
    "\n",
    "### Random Forest vs Gradient Boosting:\n",
    "\n",
    "| Аспект | Random Forest | Gradient Boosting |\n",
    "|--------|--------------|------------------|\n",
    "| **Принцип** | Bagging (параллельно) | Boosting (последовательно) |\n",
    "| **Деревья** | Глубокие, независимые | Мелкие, зависимые |\n",
    "| **Что снижает** | Variance | Bias |\n",
    "| **Overfitting** | Устойчив ✓✓ | Склонен ✗ |\n",
    "| **Скорость** | Быстрее (параллельно) | Медленнее |\n",
    "| **Настройка** | Проще | Требует внимания |\n",
    "| **Точность** | Хорошая ✓ | Отличная ✓✓ |\n",
    "| **n_estimators** | Больше → лучше | Больше → может overfitting |\n",
    "\n",
    "### Ключевые гиперпараметры Gradient Boosting:\n",
    "\n",
    "1. **n_estimators**: 100-1000 (с early stopping)\n",
    "2. **learning_rate**: 0.01-0.3 (меньше = лучше, но медленнее)\n",
    "3. **max_depth**: 3-8 (мелкие деревья!)\n",
    "4. **subsample**: 0.5-0.8 (stochastic GB)\n",
    "5. **min_samples_split**: 2-20\n",
    "\n",
    "### Практические рекомендации:\n",
    "\n",
    "**Используйте Random Forest когда:**\n",
    "- ✓ Нужна устойчивость к overfitting\n",
    "- ✓ Мало времени на настройку\n",
    "- ✓ Важна параллелизация\n",
    "- ✓ Нужны out-of-bag оценки\n",
    "\n",
    "**Используйте Gradient Boosting когда:**\n",
    "- ✓ Нужна максимальная точность\n",
    "- ✓ Есть время на подбор гиперпараметров\n",
    "- ✓ Можете использовать валидационную выборку\n",
    "- ✓ Готовы применить XGBoost/LightGBM\n",
    "\n",
    "### Применение в биологии:\n",
    "\n",
    "- **Drug discovery**: предсказание активности соединений (XGBoost/LightGBM)\n",
    "- **Геномика**: предсказание функций генов, вариантов\n",
    "- **Клинические исходы**: предсказание ответа на терапию\n",
    "- **Структурная биология**: предсказание структуры белков\n",
    "- **Системная биология**: анализ многомерных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Задания для самостоятельной работы\n",
    "\n",
    "1. **Подбор гиперпараметров**: Используйте GridSearchCV или RandomizedSearchCV для оптимизации гиперпараметров Gradient Boosting\n",
    "\n",
    "2. **Сравнение с XGBoost**: Установите XGBoost и сравните его с sklearn's GradientBoosting на том же датасете\n",
    "\n",
    "3. **Staged predictions**: Используйте метод `staged_predict` для отслеживания эволюции предсказаний по мере добавления деревьев\n",
    "\n",
    "4. **Partial Dependence**: Постройте partial dependence plots для анализа влияния отдельных признаков\n",
    "\n",
    "5. **Реальные биологические данные**: Примените все три алгоритма (Decision Tree, Random Forest, Gradient Boosting) к датасету по классификации рака и сравните результаты\n",
    "\n",
    "6. **Learning rate schedule**: Экспериментируйте с уменьшением learning rate по мере обучения"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
