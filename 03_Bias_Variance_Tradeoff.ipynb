{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff\n",
    "\n",
    "## Введение\n",
    "\n",
    "Bias-Variance Tradeoff — это фундаментальная концепция в машинном обучении, объясняющая компромисс между двумя источниками ошибок модели:\n",
    "\n",
    "- **Bias (смещение)** — систематическая ошибка модели, неспособность уловить истинную зависимость\n",
    "- **Variance (разброс)** — чувствительность модели к вариациям в обучающих данных\n",
    "\n",
    "### Применение в биологии:\n",
    "- Предсказание структуры белков\n",
    "- Анализ экспрессии генов\n",
    "- Классификация типов опухолей\n",
    "- Моделирование взаимодействий лекарств"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Математические основы\n",
    "\n",
    "Ожидаемая ошибка модели разлагается на три компоненты:\n",
    "\n",
    "$$\\text{Expected Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n",
    "\n",
    "где:\n",
    "- **Bias²** — $(\\mathbb{E}[\\hat{f}(x)] - f(x))^2$ — систематическая ошибка\n",
    "- **Variance** — $\\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^2]$ — вариабельность предсказаний\n",
    "- **Irreducible Error** — шум в данных, который невозможно устранить"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Визуализация концепции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем истинную функцию (нелинейная зависимость)\n",
    "def true_function(x):\n",
    "    \"\"\"Истинная зависимость: концентрация метаболита от времени\"\"\"\n",
    "    return np.sin(1.5 * x) + 0.5 * x\n",
    "\n",
    "# Генерируем данные\n",
    "n_samples = 50\n",
    "X_train = np.linspace(0, 5, n_samples)\n",
    "y_true = true_function(X_train)\n",
    "noise = np.random.normal(0, 0.3, n_samples)\n",
    "y_train = y_true + noise\n",
    "\n",
    "# Тестовая выборка (без шума)\n",
    "X_test = np.linspace(0, 5, 200)\n",
    "y_test_true = true_function(X_test)\n",
    "\n",
    "print(f\"Размер обучающей выборки: {n_samples}\")\n",
    "print(f\"Стандартное отклонение шума: 0.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем модели с разной сложностью\n",
    "degrees = [1, 3, 15]  # степени полинома\n",
    "models = {}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, degree in enumerate(degrees):\n",
    "    # Полиномиальные признаки\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly.fit_transform(X_train.reshape(-1, 1))\n",
    "    X_test_poly = poly.transform(X_test.reshape(-1, 1))\n",
    "    \n",
    "    # Обучаем модель\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    y_pred = model.predict(X_test_poly)\n",
    "    \n",
    "    # Вычисляем ошибку\n",
    "    mse = mean_squared_error(y_test_true, y_pred)\n",
    "    \n",
    "    # Визуализация\n",
    "    axes[idx].scatter(X_train, y_train, alpha=0.6, s=50, label='Обучающие данные')\n",
    "    axes[idx].plot(X_test, y_test_true, 'g-', linewidth=2, label='Истинная функция')\n",
    "    axes[idx].plot(X_test, y_pred, 'r--', linewidth=2, label=f'Модель (степень {degree})')\n",
    "    axes[idx].set_xlabel('Время (часы)')\n",
    "    axes[idx].set_ylabel('Концентрация метаболита')\n",
    "    axes[idx].set_title(f'Степень {degree}\\nMSE: {mse:.3f}')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    if degree == 1:\n",
    "        axes[idx].text(0.5, -1, 'HIGH BIAS\\nLOW VARIANCE', \n",
    "                      bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5),\n",
    "                      fontsize=10, ha='center')\n",
    "    elif degree == 15:\n",
    "        axes[idx].text(0.5, -1, 'LOW BIAS\\nHIGH VARIANCE', \n",
    "                      bbox=dict(boxstyle='round', facecolor='orange', alpha=0.5),\n",
    "                      fontsize=10, ha='center')\n",
    "    else:\n",
    "        axes[idx].text(0.5, -1, 'BALANCED', \n",
    "                      bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5),\n",
    "                      fontsize=10, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Эмпирическая оценка Bias и Variance\n",
    "\n",
    "Создадим множество обучающих выборок и оценим bias и variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bias_variance(degree, n_datasets=100):\n",
    "    \"\"\"Вычисление bias и variance для модели заданной сложности\"\"\"\n",
    "    \n",
    "    # Тестовая точка\n",
    "    X_test_point = np.array([[2.5]])  # середина диапазона\n",
    "    y_true_point = true_function(X_test_point[0, 0])\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # Генерируем множество датасетов\n",
    "    for _ in range(n_datasets):\n",
    "        # Новая выборка\n",
    "        X_sample = np.linspace(0, 5, 30)\n",
    "        y_sample = true_function(X_sample) + np.random.normal(0, 0.3, 30)\n",
    "        \n",
    "        # Обучаем модель\n",
    "        poly = PolynomialFeatures(degree=degree)\n",
    "        X_sample_poly = poly.fit_transform(X_sample.reshape(-1, 1))\n",
    "        X_test_poly = poly.transform(X_test_point)\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X_sample_poly, y_sample)\n",
    "        \n",
    "        # Предсказание\n",
    "        y_pred = model.predict(X_test_poly)[0]\n",
    "        predictions.append(y_pred)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Вычисляем bias и variance\n",
    "    bias_squared = (np.mean(predictions) - y_true_point) ** 2\n",
    "    variance = np.var(predictions)\n",
    "    \n",
    "    return bias_squared, variance\n",
    "\n",
    "# Вычисляем для разных степеней полинома\n",
    "degrees_range = range(1, 16)\n",
    "biases = []\n",
    "variances = []\n",
    "total_errors = []\n",
    "\n",
    "for degree in degrees_range:\n",
    "    bias_sq, var = compute_bias_variance(degree)\n",
    "    biases.append(bias_sq)\n",
    "    variances.append(var)\n",
    "    total_errors.append(bias_sq + var)\n",
    "    \n",
    "print(\"Вычисление завершено!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация Bias-Variance Tradeoff\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(degrees_range, biases, 'b-o', linewidth=2, markersize=6, label='Bias²')\n",
    "plt.plot(degrees_range, variances, 'r-s', linewidth=2, markersize=6, label='Variance')\n",
    "plt.plot(degrees_range, total_errors, 'g-^', linewidth=2, markersize=6, label='Total Error (Bias² + Variance)')\n",
    "\n",
    "# Находим оптимальную степень\n",
    "optimal_degree = degrees_range[np.argmin(total_errors)]\n",
    "plt.axvline(optimal_degree, color='purple', linestyle='--', linewidth=2, \n",
    "            label=f'Оптимальная сложность (степень {optimal_degree})')\n",
    "\n",
    "plt.xlabel('Сложность модели (степень полинома)')\n",
    "plt.ylabel('Ошибка')\n",
    "plt.title('Bias-Variance Tradeoff')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Аннотации\n",
    "plt.annotate('Underfitting\\n(High Bias)', xy=(2, 0.1), xytext=(3, 0.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue', lw=1.5),\n",
    "            fontsize=11, color='blue')\n",
    "plt.annotate('Overfitting\\n(High Variance)', xy=(14, 1), xytext=(12, 2),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=1.5),\n",
    "            fontsize=11, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nОптимальная степень полинома: {optimal_degree}\")\n",
    "print(f\"Минимальная ошибка: {min(total_errors):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Влияние размера выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curve(degree, sample_sizes):\n",
    "    \"\"\"Кривые обучения для разных размеров выборки\"\"\"\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    \n",
    "    X_test_full = np.linspace(0, 5, 200).reshape(-1, 1)\n",
    "    y_test_full = true_function(X_test_full.ravel())\n",
    "    \n",
    "    for n in sample_sizes:\n",
    "        # Генерируем выборку\n",
    "        X = np.linspace(0, 5, n).reshape(-1, 1)\n",
    "        y = true_function(X.ravel()) + np.random.normal(0, 0.3, n)\n",
    "        \n",
    "        # Обучаем модель\n",
    "        poly = PolynomialFeatures(degree=degree)\n",
    "        X_poly = poly.fit_transform(X)\n",
    "        X_test_poly = poly.transform(X_test_full)\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X_poly, y)\n",
    "        \n",
    "        # Ошибки\n",
    "        train_pred = model.predict(X_poly)\n",
    "        test_pred = model.predict(X_test_poly)\n",
    "        \n",
    "        train_errors.append(mean_squared_error(y, train_pred))\n",
    "        test_errors.append(mean_squared_error(y_test_full, test_pred))\n",
    "    \n",
    "    return train_errors, test_errors\n",
    "\n",
    "# Тестируем для разных сложностей модели\n",
    "sample_sizes = [10, 20, 30, 50, 75, 100, 150, 200]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, degree in enumerate([1, 5, 15]):\n",
    "    train_errors, test_errors = learning_curve(degree, sample_sizes)\n",
    "    \n",
    "    axes[idx].plot(sample_sizes, train_errors, 'b-o', linewidth=2, label='Ошибка на обучении')\n",
    "    axes[idx].plot(sample_sizes, test_errors, 'r-s', linewidth=2, label='Ошибка на тесте')\n",
    "    axes[idx].set_xlabel('Размер обучающей выборки')\n",
    "    axes[idx].set_ylabel('MSE')\n",
    "    axes[idx].set_title(f'Learning Curve (степень {degree})')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_ylim([0, max(max(test_errors), 2)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Регуляризация как инструмент контроля Bias-Variance\n",
    "\n",
    "Регуляризация помогает найти баланс между bias и variance, штрафуя сложность модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем данные\n",
    "n_samples = 50\n",
    "X = np.linspace(0, 5, n_samples).reshape(-1, 1)\n",
    "y = true_function(X.ravel()) + np.random.normal(0, 0.3, n_samples)\n",
    "\n",
    "X_test_reg = np.linspace(0, 5, 200).reshape(-1, 1)\n",
    "y_test_reg = true_function(X_test_reg.ravel())\n",
    "\n",
    "# Высокая степень полинома с разной регуляризацией\n",
    "degree = 15\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_test_poly = poly.transform(X_test_reg)\n",
    "\n",
    "# Разные значения регуляризации\n",
    "alphas = [0, 0.01, 1, 100]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, alpha in enumerate(alphas):\n",
    "    if alpha == 0:\n",
    "        model = LinearRegression()\n",
    "        title = 'Без регуляризации'\n",
    "    else:\n",
    "        model = Ridge(alpha=alpha)\n",
    "        title = f'Ridge регуляризация (α={alpha})'\n",
    "    \n",
    "    model.fit(X_poly, y)\n",
    "    y_pred = model.predict(X_test_poly)\n",
    "    mse = mean_squared_error(y_test_reg, y_pred)\n",
    "    \n",
    "    axes[idx].scatter(X, y, alpha=0.6, s=50, label='Данные')\n",
    "    axes[idx].plot(X_test_reg, y_test_reg, 'g-', linewidth=2, label='Истинная функция')\n",
    "    axes[idx].plot(X_test_reg, y_pred, 'r--', linewidth=2, label='Предсказания')\n",
    "    axes[idx].set_xlabel('X')\n",
    "    axes[idx].set_ylabel('y')\n",
    "    axes[idx].set_title(f'{title}\\nMSE: {mse:.3f}')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_ylim([-3, 4])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Биологический пример: Предсказание активности лекарства\n",
    "\n",
    "Предсказываем эффективность лекарства на основе молекулярных дескрипторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Генерируем данные: молекулярные дескрипторы -> активность лекарства\n",
    "X_drug, y_drug = make_regression(\n",
    "    n_samples=200,\n",
    "    n_features=20,\n",
    "    n_informative=10,\n",
    "    noise=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_drug, X_test_drug, y_train_drug, y_test_drug = train_test_split(\n",
    "    X_drug, y_drug, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Размер обучающей выборки: {len(X_train_drug)}\")\n",
    "print(f\"Количество признаков: {X_train_drug.shape[1]}\")\n",
    "print(f\"Размер тестовой выборки: {len(X_test_drug)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравниваем модели с разной сложностью\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "models = {\n",
    "    'Linear Regression\\n(Low Bias, High Variance)': LinearRegression(),\n",
    "    'Ridge (α=1)\\n(Balanced)': Ridge(alpha=1),\n",
    "    'Ridge (α=100)\\n(High Bias, Low Variance)': Ridge(alpha=100),\n",
    "    'Decision Tree (depth=10)\\n(Low Bias, High Variance)': DecisionTreeRegressor(max_depth=10, random_state=42),\n",
    "    'Decision Tree (depth=3)\\n(High Bias, Low Variance)': DecisionTreeRegressor(max_depth=3, random_state=42),\n",
    "    'Random Forest\\n(Balanced)': RandomForestRegressor(n_estimators=50, max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_drug, y_train_drug, cv=5, \n",
    "                                 scoring='neg_mean_squared_error')\n",
    "    cv_mse = -cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    # Обучение и тест\n",
    "    model.fit(X_train_drug, y_train_drug)\n",
    "    train_mse = mean_squared_error(y_train_drug, model.predict(X_train_drug))\n",
    "    test_mse = mean_squared_error(y_test_drug, model.predict(X_test_drug))\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train MSE': train_mse,\n",
    "        'CV MSE': cv_mse,\n",
    "        'Test MSE': test_mse,\n",
    "        'Overfit': test_mse - train_mse\n",
    "    })\n",
    "\n",
    "# Визуализация результатов\n",
    "import pandas as pd\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\nСравнение моделей:\")\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация сравнения\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# График 1: Train vs Test MSE\n",
    "x_pos = np.arange(len(df_results))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x_pos - width/2, df_results['Train MSE'], width, label='Train MSE', alpha=0.8)\n",
    "axes[0].bar(x_pos + width/2, df_results['Test MSE'], width, label='Test MSE', alpha=0.8)\n",
    "axes[0].set_xlabel('Модель')\n",
    "axes[0].set_ylabel('MSE')\n",
    "axes[0].set_title('Сравнение ошибок на обучении и тесте')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels([name.split('\\n')[0] for name in df_results['Model']], \n",
    "                        rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# График 2: Степень переобучения\n",
    "colors = ['green' if x < 500 else 'orange' if x < 1000 else 'red' for x in df_results['Overfit']]\n",
    "axes[1].bar(x_pos, df_results['Overfit'], color=colors, alpha=0.7)\n",
    "axes[1].axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "axes[1].set_xlabel('Модель')\n",
    "axes[1].set_ylabel('Overfit (Test MSE - Train MSE)')\n",
    "axes[1].set_title('Степень переобучения')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels([name.split('\\n')[0] for name in df_results['Model']], \n",
    "                        rotation=45, ha='right')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Практические рекомендации\n",
    "\n",
    "### Признаки High Bias (Underfitting):\n",
    "- Высокая ошибка на обучающей выборке\n",
    "- Высокая ошибка на тестовой выборке\n",
    "- Малая разница между ошибками\n",
    "\n",
    "**Решения:**\n",
    "- Увеличить сложность модели\n",
    "- Добавить больше признаков\n",
    "- Уменьшить регуляризацию\n",
    "\n",
    "### Признаки High Variance (Overfitting):\n",
    "- Низкая ошибка на обучающей выборке\n",
    "- Высокая ошибка на тестовой выборке\n",
    "- Большая разница между ошибками\n",
    "\n",
    "**Решения:**\n",
    "- Собрать больше данных\n",
    "- Уменьшить сложность модели\n",
    "- Использовать регуляризацию\n",
    "- Применить кросс-валидацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Визуализация концепции: мишень"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Классическая визуализация Bias-Variance через мишень\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "scenarios = [\n",
    "    {'title': 'Low Bias, Low Variance\\n(Идеальная модель)', 'bias': 0, 'variance': 0.5, 'pos': (0, 0)},\n",
    "    {'title': 'Low Bias, High Variance\\n(Overfitting)', 'bias': 0, 'variance': 3, 'pos': (0, 1)},\n",
    "    {'title': 'High Bias, Low Variance\\n(Underfitting)', 'bias': 4, 'variance': 0.5, 'pos': (1, 0)},\n",
    "    {'title': 'High Bias, High Variance\\n(Худший случай)', 'bias': 4, 'variance': 3, 'pos': (1, 1)}\n",
    "]\n",
    "\n",
    "for scenario in scenarios:\n",
    "    ax = axes[scenario['pos']]\n",
    "    \n",
    "    # Рисуем мишень\n",
    "    circles = [plt.Circle((0, 0), r, fill=False, color='black', linewidth=2) \n",
    "               for r in [2, 4, 6, 8]]\n",
    "    for circle in circles:\n",
    "        ax.add_patch(circle)\n",
    "    \n",
    "    # Центр мишени\n",
    "    ax.plot(0, 0, 'r*', markersize=20, label='Истинное значение')\n",
    "    \n",
    "    # Генерируем выстрелы (предсказания модели)\n",
    "    n_shots = 20\n",
    "    shots_x = np.random.normal(scenario['bias'], scenario['variance'], n_shots)\n",
    "    shots_y = np.random.normal(scenario['bias'], scenario['variance'], n_shots)\n",
    "    \n",
    "    ax.scatter(shots_x, shots_y, c='blue', s=100, alpha=0.6, label='Предсказания')\n",
    "    \n",
    "    # Среднее предсказаний\n",
    "    ax.plot(np.mean(shots_x), np.mean(shots_y), 'go', markersize=15, \n",
    "            label='Среднее предсказание', markeredgecolor='black', markeredgewidth=2)\n",
    "    \n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_ylim(-10, 10)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(scenario['title'], fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "    ax.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Задания для самостоятельной работы\n",
    "\n",
    "1. Исследуйте влияние разных типов регуляризации (L1, L2, ElasticNet) на bias-variance tradeoff\n",
    "2. Реализуйте bootstrap для оценки variance модели\n",
    "3. Примените концепции к реальным биологическим данным (например, из UCI ML Repository)\n",
    "4. Сравните bias-variance tradeoff для различных алгоритмов (SVM, KNN, нейронные сети)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
