{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Случайный лес (Random Forest)\n",
    "\n",
    "## Введение\n",
    "\n",
    "Random Forest — это ансамблевый алгоритм, который строит множество решающих деревьев и объединяет их предсказания. Это один из самых популярных и эффективных алгоритмов машинного обучения.\n",
    "\n",
    "### Применение в биологии:\n",
    "- Классификация типов опухолей по экспрессии генов\n",
    "- Предсказание взаимодействий белок-белок\n",
    "- Анализ важности генов для заболеваний\n",
    "- Предсказание структуры белков\n",
    "- Диагностика заболеваний по множеству биомаркеров\n",
    "\n",
    "### Ключевая идея:\n",
    "**Решающие деревья имеют высокий variance → Random Forest снижает variance через усреднение множества деревьев!**\n",
    "\n",
    "### Основные принципы:\n",
    "1. **Bagging (Bootstrap Aggregating)**: Каждое дерево обучается на случайной подвыборке данных (с возвращением)\n",
    "2. **Random Subspace Method**: Для каждого разбиения используется случайное подмножество признаков\n",
    "3. **Усреднение предсказаний**: Финальное предсказание = среднее/голосование всех деревьев\n",
    "\n",
    "$$\\text{Variance}_{\\text{RF}} \\approx \\frac{\\text{Variance}_{\\text{tree}}}{n_{\\text{trees}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.datasets import make_moons, make_classification\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Сравнение одиночного дерева и Random Forest\n",
    "\n",
    "### Демонстрация снижения variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем данные\n",
    "def true_function(x):\n",
    "    return np.sin(x) * x + 0.5 * x\n",
    "\n",
    "n_samples = 100\n",
    "X_train = np.sort(np.random.uniform(0, 10, n_samples))\n",
    "y_train = true_function(X_train) + np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "X_test = np.linspace(0, 10, 200)\n",
    "y_test_true = true_function(X_test)\n",
    "\n",
    "X_train_2d = X_train.reshape(-1, 1)\n",
    "X_test_2d = X_test.reshape(-1, 1)\n",
    "\n",
    "print(f\"Размер выборки: {n_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем модели\n",
    "single_tree = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "single_tree.fit(X_train_2d, y_train)\n",
    "\n",
    "random_forest = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest.fit(X_train_2d, y_train)\n",
    "\n",
    "# Предсказания\n",
    "y_pred_tree = single_tree.predict(X_test_2d)\n",
    "y_pred_rf = random_forest.predict(X_test_2d)\n",
    "\n",
    "# MSE на тренировочной выборке\n",
    "train_mse_tree = mean_squared_error(y_train, single_tree.predict(X_train_2d))\n",
    "train_mse_rf = mean_squared_error(y_train, random_forest.predict(X_train_2d))\n",
    "\n",
    "print(f\"\\nОдиночное дерево - Train MSE: {train_mse_tree:.3f}\")\n",
    "print(f\"Random Forest    - Train MSE: {train_mse_rf:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Одиночное дерево\n",
    "axes[0].scatter(X_train, y_train, alpha=0.4, s=30, label='Данные')\n",
    "axes[0].plot(X_test, y_test_true, 'g--', lw=2, label='Истинная функция')\n",
    "axes[0].plot(X_test, y_pred_tree, 'r-', lw=2, label='Предсказание (1 дерево)')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Одиночное дерево (max_depth=10)\\nВысокий VARIANCE', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Random Forest\n",
    "axes[1].scatter(X_train, y_train, alpha=0.4, s=30, label='Данные')\n",
    "axes[1].plot(X_test, y_test_true, 'g--', lw=2, label='Истинная функция')\n",
    "axes[1].plot(X_test, y_pred_rf, 'b-', lw=2, label='Предсказание (100 деревьев)')\n",
    "axes[1].set_xlabel('X')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('Random Forest (n_estimators=100)\\nСниженный VARIANCE', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Random Forest дает более гладкие предсказания за счет усреднения!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bias-Variance Tradeoff для Random Forest\n",
    "\n",
    "### Теория:\n",
    "\n",
    "Для Random Forest важны два основных гиперпараметра:\n",
    "\n",
    "1. **n_estimators** (количество деревьев):\n",
    "   - Больше деревьев → меньше variance\n",
    "   - Bias не меняется\n",
    "   - **Почти никогда не приводит к overfitting!**\n",
    "\n",
    "2. **max_depth** (глубина деревьев):\n",
    "   - Больше глубина → меньше bias, больше variance\n",
    "   - Аналогично одиночному дереву, но variance ниже\n",
    "\n",
    "### Математика:\n",
    "\n",
    "Для независимых деревьев:\n",
    "$$\\text{Var}(\\text{среднее}) = \\frac{\\sigma^2}{n}$$\n",
    "\n",
    "Для коррелированных деревьев:\n",
    "$$\\text{Var}(\\text{RF}) = \\rho\\sigma^2 + \\frac{1-\\rho}{n}\\sigma^2$$\n",
    "\n",
    "где $\\rho$ — корреляция между деревьями, $n$ — количество деревьев\n",
    "\n",
    "**Random subspace method снижает корреляцию между деревьями!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эксперимент: Влияние количества деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для вычисления bias и variance\n",
    "def compute_bias_variance_rf(n_estimators, max_depth=None, n_iterations=50):\n",
    "    \"\"\"Вычисляет bias и variance для Random Forest\"\"\"\n",
    "    predictions = np.zeros((n_iterations, len(X_test)))\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Генерируем новую выборку\n",
    "        X_sample = np.sort(np.random.uniform(0, 10, n_samples))\n",
    "        y_sample = true_function(X_sample) + np.random.normal(0, 0.5, n_samples)\n",
    "        \n",
    "        # Обучаем Random Forest\n",
    "        rf = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, \n",
    "                                   random_state=i, n_jobs=-1)\n",
    "        rf.fit(X_sample.reshape(-1, 1), y_sample)\n",
    "        \n",
    "        # Предсказания\n",
    "        predictions[i] = rf.predict(X_test_2d)\n",
    "    \n",
    "    # Среднее предсказание\n",
    "    mean_prediction = np.mean(predictions, axis=0)\n",
    "    \n",
    "    # Bias^2 и Variance\n",
    "    bias_squared = np.mean((mean_prediction - y_test_true)**2)\n",
    "    variance = np.mean(np.var(predictions, axis=0))\n",
    "    \n",
    "    return bias_squared, variance\n",
    "\n",
    "# Эксперимент: влияние n_estimators\n",
    "n_trees_range = [1, 5, 10, 20, 50, 100, 200]\n",
    "biases_trees = []\n",
    "variances_trees = []\n",
    "\n",
    "print(\"Вычисление bias и variance для разного количества деревьев...\")\n",
    "for n_trees in n_trees_range:\n",
    "    bias_sq, var = compute_bias_variance_rf(n_trees, max_depth=10, n_iterations=30)\n",
    "    biases_trees.append(bias_sq)\n",
    "    variances_trees.append(var)\n",
    "    print(f\"n_estimators={n_trees:3d}: Bias² = {bias_sq:.3f}, Variance = {var:.3f}\")\n",
    "\n",
    "print(\"\\nГотово!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# График зависимости от количества деревьев\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(n_trees_range, biases_trees, 'b-o', lw=2, markersize=8, label='Bias²')\n",
    "ax.plot(n_trees_range, variances_trees, 'r-s', lw=2, markersize=8, label='Variance')\n",
    "total_error = np.array(biases_trees) + np.array(variances_trees)\n",
    "ax.plot(n_trees_range, total_error, 'g-^', lw=2.5, markersize=8, label='Total Error')\n",
    "\n",
    "ax.set_xlabel('Количество деревьев (n_estimators)', fontsize=12)\n",
    "ax.set_ylabel('Ошибка', fontsize=12)\n",
    "ax.set_title('Влияние количества деревьев на Bias-Variance\\n(max_depth=10)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "# Аннотация\n",
    "ax.annotate('Bias НЕ меняется!', xy=(100, biases_trees[-1]), xytext=(30, biases_trees[-1]+0.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue', lw=1.5),\n",
    "            fontsize=11, color='blue', fontweight='bold')\n",
    "\n",
    "ax.annotate('Variance уменьшается', xy=(100, variances_trees[-1]), xytext=(20, variances_trees[-1]-1),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=1.5),\n",
    "            fontsize=11, color='red', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Увеличение количества деревьев снижает variance, не влияя на bias!\")\n",
    "print(\"✓ После ~50-100 деревьев улучшение замедляется\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эксперимент: Влияние глубины деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эксперимент: влияние max_depth\n",
    "depths_range = range(1, 16)\n",
    "biases_depth = []\n",
    "variances_depth = []\n",
    "\n",
    "print(\"Вычисление bias и variance для разной глубины деревьев...\")\n",
    "for depth in depths_range:\n",
    "    bias_sq, var = compute_bias_variance_rf(n_estimators=100, max_depth=depth, n_iterations=30)\n",
    "    biases_depth.append(bias_sq)\n",
    "    variances_depth.append(var)\n",
    "    print(f\"max_depth={depth:2d}: Bias² = {bias_sq:.3f}, Variance = {var:.3f}\")\n",
    "\n",
    "print(\"\\nГотово!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# График зависимости от глубины\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(depths_range, biases_depth, 'b-o', lw=2, markersize=6, label='Bias²')\n",
    "ax.plot(depths_range, variances_depth, 'r-s', lw=2, markersize=6, label='Variance')\n",
    "total_error_depth = np.array(biases_depth) + np.array(variances_depth)\n",
    "ax.plot(depths_range, total_error_depth, 'g-^', lw=2.5, markersize=6, label='Total Error')\n",
    "\n",
    "optimal_depth_rf = list(depths_range)[np.argmin(total_error_depth)]\n",
    "ax.axvline(optimal_depth_rf, color='purple', linestyle='--', lw=2, alpha=0.7,\n",
    "          label=f'Оптимальная глубина ≈ {optimal_depth_rf}')\n",
    "\n",
    "ax.set_xlabel('Максимальная глубина деревьев (max_depth)', fontsize=12)\n",
    "ax.set_ylabel('Ошибка', fontsize=12)\n",
    "ax.set_title('Влияние глубины деревьев на Bias-Variance\\n(n_estimators=100)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Оптимальная глубина для Random Forest: {optimal_depth_rf}\")\n",
    "print(f\"  Variance Random Forest ниже, чем у одиночного дерева той же глубины!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Классификация: Диагностика заболевания\n",
    "\n",
    "Используем Random Forest для классификации пациентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем данные\n",
    "X_class, y_class = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_class, y_class, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Тренировочная выборка: {len(X_train_c)} пациентов\")\n",
    "print(f\"Тестовая выборка: {len(X_test_c)} пациентов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравнение одиночного дерева и Random Forest\n",
    "tree_clf = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "tree_clf.fit(X_train_c, y_train_c)\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_clf.fit(X_train_c, y_train_c)\n",
    "\n",
    "# Оценка\n",
    "tree_train_acc = accuracy_score(y_train_c, tree_clf.predict(X_train_c))\n",
    "tree_test_acc = accuracy_score(y_test_c, tree_clf.predict(X_test_c))\n",
    "\n",
    "rf_train_acc = accuracy_score(y_train_c, rf_clf.predict(X_train_c))\n",
    "rf_test_acc = accuracy_score(y_test_c, rf_clf.predict(X_test_c))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"СРАВНЕНИЕ МОДЕЛЕЙ\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Одиночное дерево:\")\n",
    "print(f\"  Train Accuracy: {tree_train_acc:.2%}\")\n",
    "print(f\"  Test Accuracy:  {tree_test_acc:.2%}\")\n",
    "print(f\"  Overfitting gap: {(tree_train_acc - tree_test_acc):.2%}\")\n",
    "print(f\"\\nRandom Forest:\")\n",
    "print(f\"  Train Accuracy: {rf_train_acc:.2%}\")\n",
    "print(f\"  Test Accuracy:  {rf_test_acc:.2%}\")\n",
    "print(f\"  Overfitting gap: {(rf_train_acc - rf_test_acc):.2%}\")\n",
    "print(f\"\\n✓ Random Forest имеет меньший overfitting gap!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация границ решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, ax, title):\n",
    "    \"\"\"Визуализирует границу решения\"\"\"\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black', s=40, alpha=0.7)\n",
    "    ax.set_title(title, fontweight='bold', fontsize=11)\n",
    "    ax.set_xlabel('Биомаркер 1')\n",
    "    ax.set_ylabel('Биомаркер 2')\n",
    "\n",
    "# Визуализация\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "plot_decision_boundary(tree_clf, X_test_c, y_test_c, axes[0],\n",
    "                      f'Одиночное дерево\\nTest Acc = {tree_test_acc:.2%}')\n",
    "plot_decision_boundary(rf_clf, X_test_c, y_test_c, axes[1],\n",
    "                      f'Random Forest (100 деревьев)\\nTest Acc = {rf_test_acc:.2%}')\n",
    "\n",
    "axes[0].text(0.5, 0.02, 'Изломанная граница\\n(высокий variance)', \n",
    "            transform=axes[0].transAxes, ha='center', fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "axes[1].text(0.5, 0.02, 'Гладкая граница\\n(сниженный variance)', \n",
    "            transform=axes[1].transAxes, ha='center', fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Важность признаков (Feature Importance)\n",
    "\n",
    "Одно из важных преимуществ Random Forest — возможность оценить важность каждого признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем данные с большим количеством признаков\n",
    "X_features, y_features = make_classification(\n",
    "    n_samples=500, n_features=10, n_informative=5, n_redundant=3, \n",
    "    n_repeated=0, n_classes=2, random_state=42\n",
    ")\n",
    "\n",
    "feature_names = [f'Ген {i+1}' for i in range(10)]\n",
    "\n",
    "# Обучаем Random Forest\n",
    "rf_importance = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_importance.fit(X_features, y_features)\n",
    "\n",
    "# Важность признаков\n",
    "importances = rf_importance.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Визуализация\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(10), importances[indices], color='steelblue', alpha=0.8)\n",
    "plt.xticks(range(10), [feature_names[i] for i in indices], rotation=45)\n",
    "plt.xlabel('Признаки (гены)', fontsize=12)\n",
    "plt.ylabel('Важность', fontsize=12)\n",
    "plt.title('Важность признаков для классификации\\n(Feature Importance)', \n",
    "         fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nТоп-5 самых важных признаков:\")\n",
    "for i in range(5):\n",
    "    print(f\"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.3f}\")\n",
    "\n",
    "print(\"\\n✓ Random Forest позволяет выявить важные биомаркеры!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Out-of-Bag (OOB) Error\n",
    "\n",
    "Random Forest использует bootstrap-выборки. Для каждого дерева ~37% данных остаются неиспользованными (out-of-bag). Их можно использовать для оценки ошибки без отдельной валидационной выборки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest с OOB оценкой\n",
    "rf_oob = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "rf_oob.fit(X_train_c, y_train_c)\n",
    "\n",
    "oob_score = rf_oob.oob_score_\n",
    "test_score = accuracy_score(y_test_c, rf_oob.predict(X_test_c))\n",
    "\n",
    "print(\"\\nOut-of-Bag (OOB) оценка:\")\n",
    "print(f\"  OOB Accuracy:  {oob_score:.2%}\")\n",
    "print(f\"  Test Accuracy: {test_score:.2%}\")\n",
    "print(f\"\\n✓ OOB score близок к тестовой точности!\")\n",
    "print(\"  Это позволяет оценить качество модели без отдельной валидации\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Кривые обучения (Learning Curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравниваем кривые обучения для дерева и Random Forest\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Одиночное дерево\n",
    "train_sizes_tree, train_scores_tree, val_scores_tree = learning_curve(\n",
    "    DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    X_train_c, y_train_c, train_sizes=train_sizes, cv=5, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Random Forest\n",
    "train_sizes_rf, train_scores_rf, val_scores_rf = learning_curve(\n",
    "    RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
    "    X_train_c, y_train_c, train_sizes=train_sizes, cv=5, n_jobs=-1\n",
    ")\n",
    "\n",
    "# График\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Одиночное дерево\n",
    "axes[0].plot(train_sizes_tree, train_scores_tree.mean(axis=1), 'b-o', lw=2, label='Train')\n",
    "axes[0].plot(train_sizes_tree, val_scores_tree.mean(axis=1), 'r-s', lw=2, label='Validation')\n",
    "axes[0].fill_between(train_sizes_tree, \n",
    "                     val_scores_tree.mean(axis=1) - val_scores_tree.std(axis=1),\n",
    "                     val_scores_tree.mean(axis=1) + val_scores_tree.std(axis=1),\n",
    "                     alpha=0.2, color='r')\n",
    "axes[0].set_xlabel('Размер выборки')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Learning Curve: Одиночное дерево\\n(большой gap = высокий variance)', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0.6, 1.0])\n",
    "\n",
    "# Random Forest\n",
    "axes[1].plot(train_sizes_rf, train_scores_rf.mean(axis=1), 'b-o', lw=2, label='Train')\n",
    "axes[1].plot(train_sizes_rf, val_scores_rf.mean(axis=1), 'r-s', lw=2, label='Validation')\n",
    "axes[1].fill_between(train_sizes_rf, \n",
    "                     val_scores_rf.mean(axis=1) - val_scores_rf.std(axis=1),\n",
    "                     val_scores_rf.mean(axis=1) + val_scores_rf.std(axis=1),\n",
    "                     alpha=0.2, color='r')\n",
    "axes[1].set_xlabel('Размер выборки')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Learning Curve: Random Forest\\n(меньший gap = сниженный variance)', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0.6, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Random Forest показывает меньший разрыв между train и validation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Подбор гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Влияние основных гиперпараметров\n",
    "n_trees_test = [10, 30, 50, 100, 200]\n",
    "max_depths_test = [5, 10, 15, None]\n",
    "\n",
    "results = []\n",
    "\n",
    "for n_trees in n_trees_test:\n",
    "    for max_depth in max_depths_test:\n",
    "        rf = RandomForestClassifier(n_estimators=n_trees, max_depth=max_depth, random_state=42)\n",
    "        scores = cross_val_score(rf, X_train_c, y_train_c, cv=5)\n",
    "        results.append({\n",
    "            'n_estimators': n_trees,\n",
    "            'max_depth': str(max_depth) if max_depth else '∞',\n",
    "            'cv_score': scores.mean(),\n",
    "            'cv_std': scores.std()\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_pivot = df_results.pivot(index='n_estimators', columns='max_depth', values='cv_score')\n",
    "\n",
    "print(\"\\nТочность (CV) для разных гиперпараметров:\\n\")\n",
    "print(df_pivot.to_string())\n",
    "\n",
    "# Визуализация\n",
    "plt.figure(figsize=(10, 6))\n",
    "for depth in max_depths_test:\n",
    "    depth_str = str(depth) if depth else '∞'\n",
    "    data = df_results[df_results['max_depth'] == depth_str]\n",
    "    plt.plot(data['n_estimators'], data['cv_score'], 'o-', lw=2, markersize=8, label=f'max_depth={depth_str}')\n",
    "\n",
    "plt.xlabel('Количество деревьев (n_estimators)', fontsize=12)\n",
    "plt.ylabel('CV Accuracy', fontsize=12)\n",
    "plt.title('Влияние гиперпараметров на качество Random Forest', fontsize=13, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_result = df_results.loc[df_results['cv_score'].idxmax()]\n",
    "print(f\"\\n✓ Лучшая комбинация:\")\n",
    "print(f\"  n_estimators = {best_result['n_estimators']:.0f}\")\n",
    "print(f\"  max_depth = {best_result['max_depth']}\")\n",
    "print(f\"  CV Accuracy = {best_result['cv_score']:.2%} ± {best_result['cv_std']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Резюме\n",
    "\n",
    "### Bias-Variance Tradeoff для Random Forest:\n",
    "\n",
    "| Параметр | Bias | Variance | Рекомендации |\n",
    "|----------|------|----------|-------------|\n",
    "| **n_estimators** ↑ | Не меняется | ↓ Уменьшается | Больше = лучше (до насыщения) |\n",
    "| **max_depth** ↑ | ↓ Уменьшается | ↑ Увеличивается | Подбирать по CV |\n",
    "| **max_features** ↓ | ↑ Увеличивается | ↓ Уменьшается | sqrt(n) для классификации, n/3 для регрессии |\n",
    "| **min_samples_split** ↑ | ↑ Увеличивается | ↓ Уменьшается | 2-20 |\n",
    "\n",
    "### Ключевые выводы:\n",
    "\n",
    "1. **Random Forest снижает variance** по сравнению с одиночным деревом за счет усреднения\n",
    "2. **Увеличение n_estimators почти никогда не приводит к overfitting**\n",
    "3. **OOB error** позволяет оценить качество без отдельной валидации\n",
    "4. **Feature importance** помогает в интерпретации и отборе признаков\n",
    "5. **Random Forest устойчивее к шуму** и выбросам, чем одиночное дерево\n",
    "\n",
    "### Сравнение с одиночным деревом:\n",
    "\n",
    "| Аспект | Одиночное дерево | Random Forest |\n",
    "|--------|-----------------|---------------|\n",
    "| **Bias** | Зависит от глубины | Аналогично |\n",
    "| **Variance** | Высокий ↑↑ | Низкий ↓ |\n",
    "| **Интерпретируемость** | Высокая ✓✓ | Низкая ✗ |\n",
    "| **Точность** | Средняя | Высокая ✓✓ |\n",
    "| **Устойчивость** | Низкая | Высокая ✓ |\n",
    "| **Скорость** | Быстро ✓✓ | Медленнее ✗ |\n",
    "\n",
    "### Применение в биологии:\n",
    "\n",
    "- **Анализ экспрессии генов**: выявление важных генов для заболеваний\n",
    "- **Предсказание структуры белков**: классификация вторичных структур\n",
    "- **Диагностика**: классификация заболеваний по множеству биомаркеров\n",
    "- **Взаимодействия**: предсказание белок-белковых или белок-лигандных взаимодействий\n",
    "\n",
    "### Когда использовать Random Forest:\n",
    "\n",
    "✓ Табличные данные с множеством признаков\n",
    "✓ Нужна высокая точность\n",
    "✓ Важна оценка важности признаков\n",
    "✓ Данные с шумом и выбросами\n",
    "✓ Нелинейные зависимости\n",
    "\n",
    "✗ Нужна полная интерпретируемость (используйте одиночное мелкое дерево)\n",
    "✗ Критична скорость предсказания в реальном времени\n",
    "✗ Очень большие датасеты (используйте XGBoost, LightGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Задания для самостоятельной работы\n",
    "\n",
    "1. **Влияние max_features**: Исследуйте, как параметр `max_features` влияет на корреляцию между деревьями и итоговую точность\n",
    "\n",
    "2. **Экстремально случайные деревья (Extra Trees)**: Сравните `RandomForestClassifier` и `ExtraTreesClassifier` на том же датасете\n",
    "\n",
    "3. **Реальные данные**: Загрузите датасет по экспрессии генов (например, breast cancer из sklearn) и постройте модель для классификации\n",
    "\n",
    "4. **Partial Dependence Plots**: Используйте `sklearn.inspection.partial_dependence` для анализа влияния отдельных признаков\n",
    "\n",
    "5. **Сравнение с градиентным бустингом**: Подготовьтесь к следующему ноутбуку — изучите различия между bagging и boosting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
