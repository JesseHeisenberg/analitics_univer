{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решающие деревья (Decision Trees)\n",
    "\n",
    "## Введение\n",
    "\n",
    "Решающее дерево — это алгоритм машинного обучения, который принимает решения путем разбиения пространства признаков на регионы. Это один из самых интерпретируемых алгоритмов ML.\n",
    "\n",
    "### Применение в биологии:\n",
    "- Диагностика заболеваний по симптомам и анализам\n",
    "- Классификация типов опухолей по экспрессии генов\n",
    "- Предсказание эффективности лекарств\n",
    "- Анализ факторов риска заболеваний\n",
    "\n",
    "### Ключевые концепции:\n",
    "- **Высокий bias, низкий variance**: Мелкое дерево (underfitting)\n",
    "- **Низкий bias, высокий variance**: Глубокое дерево (overfitting)\n",
    "- **Bias-Variance Tradeoff**: Балансировка через глубину дерева"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Как работает решающее дерево\n",
    "\n",
    "### Принцип работы:\n",
    "\n",
    "1. **Выбор лучшего разбиения**: На каждом узле выбирается признак и порог, которые лучше всего разделяют данные\n",
    "2. **Критерий разбиения**:\n",
    "   - Для регрессии: минимизация MSE (Mean Squared Error)\n",
    "   - Для классификации: минимизация Gini impurity или энтропии\n",
    "3. **Рекурсивное разбиение**: Процесс повторяется для каждого поддерева\n",
    "4. **Остановка**: По критериям (максимальная глубина, минимум объектов в листе и т.д.)\n",
    "\n",
    "### Критерий разбиения для регрессии:\n",
    "\n",
    "В узле $m$ ищем признак $j$ и порог $t$, которые минимизируют:\n",
    "\n",
    "$$\\text{MSE}_{left}(j, t) + \\text{MSE}_{right}(j, t)$$\n",
    "\n",
    "где:\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\bar{y})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Пример: Регрессия (предсказание концентрации препарата)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем данные: нелинейная зависимость концентрации от времени\n",
    "def true_function(x):\n",
    "    \"\"\"Истинная нелинейная функция\"\"\"\n",
    "    return np.sin(x) * x + 0.5 * x\n",
    "\n",
    "# Тренировочные данные\n",
    "n_samples = 100\n",
    "X_train = np.sort(np.random.uniform(0, 10, n_samples))\n",
    "y_train = true_function(X_train) + np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "# Тестовые данные\n",
    "X_test = np.linspace(0, 10, 200)\n",
    "y_test_true = true_function(X_test)\n",
    "\n",
    "X_train_2d = X_train.reshape(-1, 1)\n",
    "X_test_2d = X_test.reshape(-1, 1)\n",
    "\n",
    "print(f\"Размер тренировочной выборки: {n_samples}\")\n",
    "print(f\"Диапазон времени: [0, 10] часов\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация влияния глубины дерева"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем деревья разной глубины\n",
    "depths = [1, 2, 5, 20]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, depth in enumerate(depths):\n",
    "    # Обучаем дерево\n",
    "    tree = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train_2d, y_train)\n",
    "    \n",
    "    # Предсказания\n",
    "    y_pred = tree.predict(X_test_2d)\n",
    "    \n",
    "    # Ошибка на тренировочной выборке\n",
    "    train_mse = mean_squared_error(y_train, tree.predict(X_train_2d))\n",
    "    \n",
    "    # График\n",
    "    axes[idx].scatter(X_train, y_train, alpha=0.4, s=20, label='Тренировочные данные')\n",
    "    axes[idx].plot(X_test, y_test_true, 'g--', lw=2, label='Истинная функция')\n",
    "    axes[idx].plot(X_test, y_pred, 'r-', lw=2, label=f'Предсказание (depth={depth})')\n",
    "    axes[idx].set_xlabel('Время (часы)')\n",
    "    axes[idx].set_ylabel('Концентрация')\n",
    "    axes[idx].set_title(f'Глубина = {depth}, Train MSE = {train_mse:.3f}', fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Добавляем комментарий\n",
    "    if depth == 1:\n",
    "        axes[idx].text(0.5, 0.95, 'Высокий BIAS\\n(underfitting)', \n",
    "                      transform=axes[idx].transAxes, fontsize=10, \n",
    "                      verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    elif depth == 20:\n",
    "        axes[idx].text(0.5, 0.95, 'Высокий VARIANCE\\n(overfitting)', \n",
    "                      transform=axes[idx].transAxes, fontsize=10, \n",
    "                      verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bias-Variance Tradeoff\n",
    "\n",
    "### Теория:\n",
    "\n",
    "Ожидаемая ошибка модели разлагается на три компоненты:\n",
    "\n",
    "$$\\text{Expected Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n",
    "\n",
    "- **Bias (смещение)**: Ошибка из-за упрощающих предположений модели\n",
    "  - Высокий bias → underfitting\n",
    "  - Модель слишком простая\n",
    "  \n",
    "- **Variance (разброс)**: Ошибка из-за чувствительности к флуктуациям в данных\n",
    "  - Высокий variance → overfitting\n",
    "  - Модель слишком сложная\n",
    "  \n",
    "- **Irreducible Error**: Шум в данных (не зависит от модели)\n",
    "\n",
    "### Для решающих деревьев:\n",
    "\n",
    "| Параметр | Bias | Variance |\n",
    "|----------|------|----------|\n",
    "| Маленькая глубина | ↑ Высокий | ↓ Низкий |\n",
    "| Большая глубина | ↓ Низкий | ↑ Высокий |\n",
    "| Много объектов в листе | ↑ Высокий | ↓ Низкий |\n",
    "| Мало объектов в листе | ↓ Низкий | ↑ Высокий |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Демонстрация Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для вычисления bias и variance\n",
    "def compute_bias_variance(max_depth, n_iterations=100):\n",
    "    \"\"\"\n",
    "    Вычисляет bias и variance для дерева заданной глубины\n",
    "    путем обучения на разных подвыборках данных\n",
    "    \"\"\"\n",
    "    predictions = np.zeros((n_iterations, len(X_test)))\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Генерируем новую выборку\n",
    "        X_sample = np.sort(np.random.uniform(0, 10, n_samples))\n",
    "        y_sample = true_function(X_sample) + np.random.normal(0, 0.5, n_samples)\n",
    "        \n",
    "        # Обучаем дерево\n",
    "        tree = DecisionTreeRegressor(max_depth=max_depth, random_state=i)\n",
    "        tree.fit(X_sample.reshape(-1, 1), y_sample)\n",
    "        \n",
    "        # Предсказания\n",
    "        predictions[i] = tree.predict(X_test_2d)\n",
    "    \n",
    "    # Среднее предсказание по всем моделям\n",
    "    mean_prediction = np.mean(predictions, axis=0)\n",
    "    \n",
    "    # Bias^2: (среднее предсказание - истинное значение)^2\n",
    "    bias_squared = np.mean((mean_prediction - y_test_true)**2)\n",
    "    \n",
    "    # Variance: разброс предсказаний разных моделей\n",
    "    variance = np.mean(np.var(predictions, axis=0))\n",
    "    \n",
    "    return bias_squared, variance\n",
    "\n",
    "# Вычисляем bias и variance для разных глубин\n",
    "depths_range = range(1, 16)\n",
    "biases = []\n",
    "variances = []\n",
    "\n",
    "print(\"Вычисление bias и variance для разных глубин...\")\n",
    "for depth in depths_range:\n",
    "    bias_sq, var = compute_bias_variance(depth, n_iterations=50)\n",
    "    biases.append(bias_sq)\n",
    "    variances.append(var)\n",
    "    print(f\"Глубина {depth:2d}: Bias² = {bias_sq:.3f}, Variance = {var:.3f}\")\n",
    "\n",
    "print(\"\\nГотово!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### График Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(depths_range, biases, 'b-o', lw=2, markersize=6, label='Bias²')\n",
    "ax.plot(depths_range, variances, 'r-s', lw=2, markersize=6, label='Variance')\n",
    "total_error = np.array(biases) + np.array(variances)\n",
    "ax.plot(depths_range, total_error, 'g-^', lw=2.5, markersize=6, label='Total Error (Bias² + Variance)')\n",
    "\n",
    "# Находим оптимальную глубину\n",
    "optimal_depth = list(depths_range)[np.argmin(total_error)]\n",
    "ax.axvline(optimal_depth, color='purple', linestyle='--', lw=2, alpha=0.7, \n",
    "           label=f'Оптимальная глубина ≈ {optimal_depth}')\n",
    "\n",
    "ax.set_xlabel('Максимальная глубина дерева', fontsize=12)\n",
    "ax.set_ylabel('Ошибка', fontsize=12)\n",
    "ax.set_title('Bias-Variance Tradeoff для решающих деревьев', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Добавляем аннотации\n",
    "ax.annotate('Underfitting\\n(Высокий Bias)', xy=(2, biases[1]), xytext=(3, biases[1]+1),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue', lw=1.5),\n",
    "            fontsize=10, color='blue', fontweight='bold')\n",
    "\n",
    "ax.annotate('Overfitting\\n(Высокий Variance)', xy=(14, variances[-2]), xytext=(11, variances[-2]+1.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=1.5),\n",
    "            fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Оптимальная глубина дерева: {optimal_depth}\")\n",
    "print(f\"  Минимальная ошибка: {min(total_error):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Пример классификации: Диагностика заболевания\n",
    "\n",
    "Классифицируем пациентов на больных/здоровых по двум биомаркерам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Генерируем данные (нелинейно разделимые классы)\n",
    "X_class, y_class = make_moons(n_samples=300, noise=0.25, random_state=42)\n",
    "\n",
    "# Разделяем на train/test\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_class, y_class, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Тренировочная выборка: {len(X_train_c)} пациентов\")\n",
    "print(f\"Тестовая выборка: {len(X_test_c)} пациентов\")\n",
    "print(f\"Классы: 0 (здоров), 1 (болен)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация границ решений для разных глубин"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, ax, title):\n",
    "    \"\"\"Визуализирует границу решения классификатора\"\"\"\n",
    "    h = 0.02  # шаг сетки\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black', s=40)\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "    ax.set_xlabel('Биомаркер 1')\n",
    "    ax.set_ylabel('Биомаркер 2')\n",
    "\n",
    "# Создаем классификаторы разной сложности\n",
    "depths_class = [2, 4, 8, None]  # None = без ограничения глубины\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, depth in enumerate(depths_class):\n",
    "    # Обучаем дерево\n",
    "    tree_clf = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree_clf.fit(X_train_c, y_train_c)\n",
    "    \n",
    "    # Точность\n",
    "    train_acc = accuracy_score(y_train_c, tree_clf.predict(X_train_c))\n",
    "    test_acc = accuracy_score(y_test_c, tree_clf.predict(X_test_c))\n",
    "    \n",
    "    # Визуализация\n",
    "    depth_str = str(depth) if depth is not None else '∞'\n",
    "    title = f'Глубина = {depth_str}\\nTrain Acc = {train_acc:.2%}, Test Acc = {test_acc:.2%}'\n",
    "    plot_decision_boundary(tree_clf, X_train_c, y_train_c, axes[idx], title)\n",
    "    \n",
    "    # Добавляем комментарий\n",
    "    if depth == 2:\n",
    "        axes[idx].text(0.5, 0.02, 'Высокий BIAS', transform=axes[idx].transAxes, \n",
    "                      fontsize=10, ha='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "    elif depth is None:\n",
    "        axes[idx].text(0.5, 0.02, 'Высокий VARIANCE', transform=axes[idx].transAxes, \n",
    "                      fontsize=10, ha='center', bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Кривые обучения (Learning Curves)\n",
    "\n",
    "Кривые обучения показывают, как ошибка на тренировочной и валидационной выборках зависит от размера данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(max_depth):\n",
    "    \"\"\"Строит кривые обучения для дерева заданной глубины\"\"\"\n",
    "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    \n",
    "    for size in train_sizes:\n",
    "        n_samples_subset = int(size * len(X_train_c))\n",
    "        \n",
    "        # Берем подвыборку\n",
    "        X_subset = X_train_c[:n_samples_subset]\n",
    "        y_subset = y_train_c[:n_samples_subset]\n",
    "        \n",
    "        # Обучаем модель\n",
    "        tree = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "        tree.fit(X_subset, y_subset)\n",
    "        \n",
    "        # Ошибки\n",
    "        train_error = 1 - accuracy_score(y_subset, tree.predict(X_subset))\n",
    "        val_error = 1 - accuracy_score(y_test_c, tree.predict(X_test_c))\n",
    "        \n",
    "        train_errors.append(train_error)\n",
    "        val_errors.append(val_error)\n",
    "    \n",
    "    return train_sizes * len(X_train_c), train_errors, val_errors\n",
    "\n",
    "# Строим кривые для разных глубин\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, depth in enumerate([2, 5, None]):\n",
    "    sizes, train_err, val_err = plot_learning_curves(depth)\n",
    "    \n",
    "    axes[idx].plot(sizes, train_err, 'b-o', lw=2, label='Train Error')\n",
    "    axes[idx].plot(sizes, val_err, 'r-s', lw=2, label='Validation Error')\n",
    "    \n",
    "    depth_str = str(depth) if depth is not None else '∞'\n",
    "    axes[idx].set_title(f'Глубина = {depth_str}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Размер тренировочной выборки')\n",
    "    axes[idx].set_ylabel('Ошибка')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_ylim([0, 0.5])\n",
    "    \n",
    "    # Добавляем комментарии\n",
    "    if depth == 2:\n",
    "        axes[idx].text(0.5, 0.95, 'Высокий bias\\n(обе ошибки высокие)', \n",
    "                      transform=axes[idx].transAxes, fontsize=9, va='top', ha='center',\n",
    "                      bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "    elif depth is None:\n",
    "        axes[idx].text(0.5, 0.95, 'Высокий variance\\n(большой разрыв)', \n",
    "                      transform=axes[idx].transAxes, fontsize=9, va='top', ha='center',\n",
    "                      bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Визуализация структуры дерева"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем небольшое дерево для визуализации\n",
    "tree_visual = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_visual.fit(X_train_c, y_train_c)\n",
    "\n",
    "# Визуализация структуры\n",
    "plt.figure(figsize=(16, 8))\n",
    "plot_tree(tree_visual, \n",
    "          feature_names=['Биомаркер 1', 'Биомаркер 2'],\n",
    "          class_names=['Здоров', 'Болен'],\n",
    "          filled=True, \n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title('Структура решающего дерева (max_depth=3)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nИнтерпретация узлов:\")\n",
    "print(\"  - Верхняя строка: условие разбиения (feature <= threshold)\")\n",
    "print(\"  - gini: мера неопределенности (0 = чистый узел, 0.5 = максимальная неопределенность)\")\n",
    "print(\"  - samples: количество объектов в узле\")\n",
    "print(\"  - value: [количество класса 0, количество класса 1]\")\n",
    "print(\"  - class: предсказанный класс (большинство в узле)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Практические рекомендации\n",
    "\n",
    "### Преимущества решающих деревьев:\n",
    "✓ Интерпретируемость (можно визуализировать и объяснить)\n",
    "✓ Не требуют нормализации данных\n",
    "✓ Работают с категориальными и числовыми признаками\n",
    "✓ Автоматический отбор признаков\n",
    "✓ Нелинейные зависимости\n",
    "\n",
    "### Недостатки:\n",
    "✗ Склонность к переобучению (высокий variance)\n",
    "✗ Неустойчивость (малые изменения в данных → большие изменения в дереве)\n",
    "✗ Сложность обобщения на новые данные\n",
    "\n",
    "### Решение проблем:\n",
    "1. **Ограничение сложности**: max_depth, min_samples_split, min_samples_leaf\n",
    "2. **Pruning (обрезка)**: ccp_alpha (cost complexity pruning)\n",
    "3. **Ансамбли**: Random Forest, Gradient Boosting (следующие ноутбуки!)\n",
    "\n",
    "### Как выбрать гиперпараметры:\n",
    "- Используйте **кросс-валидацию** для оценки обобщающей способности\n",
    "- **max_depth**: обычно 3-10 для интерпретируемости, до 20+ для точности\n",
    "- **min_samples_split**: 2-20 (больше → меньше overfitting)\n",
    "- **min_samples_leaf**: 1-10 (больше → более гладкие границы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Сравнение с кросс-валидацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка для разных глубин с помощью кросс-валидации\n",
    "depths_cv = range(1, 21)\n",
    "cv_scores_mean = []\n",
    "cv_scores_std = []\n",
    "\n",
    "for depth in depths_cv:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    scores = cross_val_score(tree, X_train_c, y_train_c, cv=5, scoring='accuracy')\n",
    "    cv_scores_mean.append(scores.mean())\n",
    "    cv_scores_std.append(scores.std())\n",
    "\n",
    "# График\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(depths_cv, cv_scores_mean, 'b-o', lw=2, label='Средняя точность (CV)')\n",
    "plt.fill_between(depths_cv, \n",
    "                 np.array(cv_scores_mean) - np.array(cv_scores_std),\n",
    "                 np.array(cv_scores_mean) + np.array(cv_scores_std),\n",
    "                 alpha=0.2, color='b', label='±1 std')\n",
    "\n",
    "optimal_depth_cv = list(depths_cv)[np.argmax(cv_scores_mean)]\n",
    "plt.axvline(optimal_depth_cv, color='red', linestyle='--', lw=2, \n",
    "           label=f'Оптимальная глубина = {optimal_depth_cv}')\n",
    "\n",
    "plt.xlabel('Максимальная глубина дерева', fontsize=12)\n",
    "plt.ylabel('Accuracy (5-fold CV)', fontsize=12)\n",
    "plt.title('Выбор оптимальной глубины с помощью кросс-валидации', fontsize=13, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Оптимальная глубина (CV): {optimal_depth_cv}\")\n",
    "print(f\"  Точность: {max(cv_scores_mean):.2%} ± {cv_scores_std[optimal_depth_cv-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Резюме\n",
    "\n",
    "### Bias-Variance Tradeoff для решающих деревьев:\n",
    "\n",
    "| Модель | Bias | Variance | Применение |\n",
    "|--------|------|----------|------------|\n",
    "| **Мелкое дерево** (depth=1-3) | Высокий ↑ | Низкий ↓ | Быстрая интерпретация, простые правила |\n",
    "| **Средняя глубина** (depth=5-8) | Средний → | Средний → | **Оптимальный баланс** для большинства задач |\n",
    "| **Глубокое дерево** (depth>15 или ∞) | Низкий ↓ | Высокий ↑ | Переобучение, нужны ансамбли |\n",
    "\n",
    "### Ключевые выводы:\n",
    "\n",
    "1. **Решающие деревья имеют высокий variance** → склонность к переобучению\n",
    "2. **Контроль сложности** через max_depth, min_samples_* критичен для обобщения\n",
    "3. **Одиночные деревья нестабильны** → малые изменения в данных дают разные деревья\n",
    "4. **Решение: ансамбли деревьев** (Random Forest, Gradient Boosting) → следующие ноутбуки!\n",
    "\n",
    "### Применение в биологии:\n",
    "\n",
    "- Решающие деревья отлично подходят для **диагностических правил**\n",
    "- Можно получить **интерпретируемые биомедицинские правила**\n",
    "- Для **высокой точности** лучше использовать ансамбли (Random Forest, Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Задания для самостоятельной работы\n",
    "\n",
    "1. **Влияние других гиперпараметров**: Исследуйте влияние `min_samples_split`, `min_samples_leaf` и `max_features` на bias-variance tradeoff\n",
    "\n",
    "2. **Pruning**: Примените cost complexity pruning (`ccp_alpha`) и сравните с ограничением глубины\n",
    "\n",
    "3. **Реальные данные**: Используйте датасет по экспрессии генов (например, из sklearn.datasets) и постройте дерево для классификации типов рака\n",
    "\n",
    "4. **Feature importance**: Постройте график важности признаков (`feature_importances_`) для интерпретации модели\n",
    "\n",
    "5. **Сравнение критериев**: Сравните работу деревьев с критериями 'gini' и 'entropy' для классификации"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
